<html>
  <head>
    <title>
      Overheard at Code4Lib 2016
    </title>

  <style>
blockquote{
  display:block;
  background: #fff;
  padding: 15px 20px 15px 45px;
  margin: auto;
  width: 75%;
  position: relative;


  /*Font*/
  font-family: Georgia, serif;
  font-size: 2em;
  line-height: 1.2;
  color: #666;
  text-align: justify;

  /*Borders - (Optional)*/
  border-left: 15px solid #d35400;
  border-right: 2px solid #d35400;

  /*Box Shadow - (Optional)*/
  -moz-box-shadow: 2px 2px 15px #ccc;
  -webkit-box-shadow: 2px 2px 15px #ccc;
  box-shadow: 2px 2px 15px #ccc;


  transition: opacity 1s ease;
}

blockquote::before{
  content: "\201C"; /*Unicode for Left Double Quote*/

  /*Font*/
  font-family: Georgia, serif;
  font-size: 60px;
  font-weight: bold;
  color: #999;

  /*Positioning*/
  position: absolute;
  left: 10px;
  top:5px;
}

blockquote::after{
  /*Reset to make sure*/
  content: "";
}

blockquote a{
  text-decoration: none;
  background: #666;
  cursor: pointer;
  padding: 0 3px;
  color: #c76c0c;
}

blockquote a:hover{
 color: #666;
}

blockquote em{
  font-style: italic;
}
#header, #button{
  text-align: center;
  font-family: sans-serif;
  font-size: 1.75em;
  padding: 30px;
  color: #34495e;
    font-weight: bold;


}


// reset
html {
  box-sizing: border-box;
}
*, *:before, *:after {
  box-sizing: inherit;
}



.container {
  display: flex;
  flex-direction: column;
  width: 100%;
  padding: 20px;
}

.button-container {
    display: flex;
    flex-wrap: wrap;
    justify-content: center;
    margin: 60px auto;
}
button {
  border: 0 none;
  border-radius: 24px;

  padding: 12px 18px;

  cursor: pointer;

  display: flex;
  justify-content: center;
  align-items: center;

  flex: 0 0 160px;

  text-align: center;
  line-height: 1.3;
  font-size: 14px;
  color: #fff;
  text-transform: none;

  background: #d35400;
  color: #fff;
  outline: none;


  font-weight: 500;

  transition: all 60ms ease-in-out;

}

button:hover {
    opacity: .95;
    transition: all 60ms ease;
}
button:active {
    opacity: .75;
    transform: scale(.97);
    transition: all 60ms ease;
}

#foot{
  font-family: sans-serif;
  text-align: center;
  color: #999;

}
#foot a{
  color: #999;
}

</style>
</head>
<body>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js"></script>
  <script src="./rita.min.js"></script>
  <script>
    window.onload = function() {
    $('#content').text("Loading Data Please Wait...");
    $('#content').css("opacity",0.25)
    setTimeout(function(){
      var rm = new RiMarkov(3);
    rm.loadText("Hello. Hello. We're about to get started. Welcome everyone to the 2016 Code4Lib here in Philadelphia. [Clapping]. This is the 11th annual Code4Lib and I must say I'm a little bit disappointed in all of you that I have not seen any spinal tap references anywhere because this one goes to 11. It's great to have you here we're happy you have made it and didn't have some wig snowstorm that prevented it. So again welcome. Over the past year the mobile community in Philadelphia really kind of clicked together there's about a half a dozen institutions that are part of organizing this event and it's been a labor of love and we are just really happy that everyone came out to Philadelphia. And I found it a real pleasure most of the time organizing a Code4Lib conference and the biggest part of that was the turn out how many people committed to making this happen. There was over 60 volunteers that either worked locally at this conference or worked on endless stream of committees that we have but so many people really worked very hard to make this week happen and this conference happen. People committed themselves. And that's always been one of my favorite things about this community is that how much effort people give from the community how much people are committed to make it work. The other thing I find great is the way that we really work hard to reach out beyond our own kind of, our own limits to try and bring in new people into the group and we do that through a kind of hospitality and whether that be the friendliless we show to new coders and people that are some of the initiatives. I was proud we were able to offer child care, that we were able to do a transcription service. [Clapping] a transcription service and that we were able to offer ten diversity scholarships and the only reason that happened is the community was behind it and thought those were really important initiatives we should see through. So it really made me proud to be part of Code4Lib while that was happening and see it up close and see it happen and see it through. So I really want to thank the community for making those things happen and being the kind of community we are. So thank you Code4Lib. [Clapping] so another aspect another way which make this community friendly is to, is by making people feel comfortable and part of what we do is we have a code of conduct. When you signed up for your registration you agreed to abide by the code of conduct and even if you got your ticket or your registration on the fly from somebody else and the back channels you are still bound by that code of conduct and the code of conduct is a community created document that is a living document that is still being tweaked but and it comes down to we need to treat each other with with respect and that harassment in any form absolutely not tolerated at Code4Lib and will not be tolerated. That can be a very vague statement which is why we have a document that explains it. If you are unclear about harassment or any kind of conduct, the code of conduct is there to guide you. Along those same lines we've introduced a new role within the community this year called duty officers so each day at each day of the conference and at the evening events the reception tonight and the play and share tomorrow there will be duty officers who are on duty, and their role is to, not to police people they are not violating the code of conduct their role is to support people. If someone feels like they've just been harassed, then we want every attendee to know people are here who know how to handle that. It can be a very difficult and sensitive time so we have duty officers here specifically to provide that aid during the conference. So that service is there. We have two duty officers today who are sitting at the back table back by the back doors. Becky queues and Megan queuesy I can't, they are standing up. There's also a Code4Lib.org slash duty slash officers that has a picture of our officers if you need to find someone to talk so. So that service is there and we're really glad to have it we know it makes some people more comfortable and that's what we're here to do and is make people feel welcome. One more thing. Also I just want to reiterate we do have a photography policy that is indicated by the lanyards that people are wearing. Green with the code #4 lib logo means take a picture. Yellow means ask before and red means don't ask and don't take a picture and violation of that is a violation of the code of conduct so it is enforced. So it's been mentioned that Code4Lib is a community, a community with many many times to get your voice out. Arguably too many. We have a IRV, twist account and slack channel we encourage you to use any and all of these things. They're all monitored all the time. If you have questions if you're following at home. If there's a time for Q and A you can get your voice out in the conference itself. Code4Lib is a community voiced with several channels what it is not is an entity and it is the only thing that makes this conference possible is if a host institution steps up to handle the liability and to write the checks to make this thing really happen and it really can't be under stated that this conference would not happen if if it weren't for the two institutions up on the screen, Princeton and temple university made this happen. [Clapping] they're taking on all the liability and it's just so -- we can't thank them enough. The least we can do is thank them and unfortunately that's all we can do. So I thank them again. And in addition to the host institutions there are also a lot of sponsors and to do the initiatives we've done and keep the costs low. Throughout the conference we will be thanking all of our sponsors and we'll start with our platinum sponsors so to black white to EBSCO and pen libraries and OCSC we appreciate it thank you very much and we're a big part of why we're able to be here today. This year we also did a new kind of sponsorship where you could do a particular thing, a tar getted sponsorship and we have a transcription sponsor temple university libraries sponsored the transportation service that is running today. So throughout the conference all of the speakers will have their talks transcribed live. You can go to a URL it is on the back of your schedule, the schedule that was in your folders there's a URL that lists the URL for the transcription service so you can watch the text of the talks in real time. We will use that later to add the text to the videos. And the URL for that transcription service is on the website at 2016 dot Code4Lib.org slash attend. Thank you temple I think it's great we have transcription and it will be incredibly useful. Quick word with the program. Those of you who have been here before may have noticed it looks a little bit different. I was on the program committee this year with about a dozen or people and we were all in general agreement that we wanted to try some new things so as we were kicking around some ideas you know we started talking about the talk submission process and each year we'd get about 70 to 75 proposals and then we'd vote and keep more or less the top 23 at which point then we turn away 50 talks and that's just a tragedy. And you know we understand there's other opportunities to get your word out and get your story out, lightning talks or the Code4Lib journal or regional events but that's not this. This is really important and we wanted to try to stop turning away so many people from being a part of this. So we played with some of the times a little bit we shortened some talks to make room for other talks that normally would not have been a part of the program. Also, we're introducing a panel this year, a little bit of an experiment for those of you that come every year, the sessions are theme mat cally similar and they're mapped directly to the trends that are happening within our space and we thought it would be interesting to take some talks that are sort of on the same page but not really and put them together and see what kind of conversation comes out of it. And the committee that put all this together is really excited we hope everyone enjoys it and yes it's all about experimentation and we hope you enjoy what we've come up with. Just a quick note anyone who is speaking, please see me at the break to get your slides on the computer there are a lot of you floating around out there so please see me. All right. One quick announcement there's quite a few people standing in the back so people that are in seats try and scoot in and not -- move to the center so people can easily get in. We're in tight this year so you're going to have to help each other out. Don't put bags on seats. There's four or five seats on the far right and about four or five on the far left. If you want seats there's some over there. I have a couple of empty seats. Somebody is smart. If you want a seat you all back there there's some over here where these hands are raised. And one last thank you we want to do it is a very special one one of our local planning committee could not be here today. Anna Hedley from chemical heritage foundation. She was an absolutely essential part of the planning committee and she was always the coolest cat in the room, she was like so relaxed all the time and just to give you you an example how relaxed she was she was at chemical heritage foundation yesterday working on the WIFI making sure the preconferences happened all the while she was timing her contractions. Anna, I'm really happen to say -- [clapping] -- I am really happen to say that Anna has continued the tradition of a Code4Lib planner having a baby just before the conference this is like the third or fourth so I'm glad she timed that just right for code for 4 and we want to thank her and congratulate her and tell her we miss her and wish she could be here but we understand. For Anna, one more. [Clapping]. Let's get this started. And with that, I would like to introduce our opening keynote Kate Krauss. Kate is the director of communications in public policy at the Torah project. That's right it was in my pocket. Here we go. So the Torah project if you're not aware is a, it's a software that helps you protect your privacy on-line it helps you depend yourself against network surveillance and traffic analysis and so that is Kate's role, she's got a much longer history than that. Kate has been an influential political straj gist since the late 1980's. An early member of act out she led and organized state-wide coalition that succeeded in trimming the California aid drug program. And restructuring California AIDS if you understand priorities. One of the she is a key strategist behind the global AIDS treatment movement in the late 1990's. Working with healthed grap and THC of South Africa. In 2002 she founded AIDS project relevant to the glee balance community. She districts advocacy com pain focused on a cure for AIDS as a major public goal. She works with international health societies to identify and overcome obstacles for this critical research. She works closely with Chinese AIDS activists and a coalition of advocacy groups interested in AIDS in China. Her advocacy was influential in securing 90 million in aide for China HIV AIDS program and instrumental in the release of 3 dozen Chinese activists detained for their work since 2002. She is organized successful international kwaps for the release of Juan I don't know high and communications coordinator for the AIDS campaign at physicians for human rights. Katie has placed stories. She has worked on clinical trial, vaccine, structure of medical research, pediatric AIDS, trade and IP issues, Africa health, Africa's health worker shore at that place financing for global funds and the pep reorganization. She's also a local Philadelphiian and very active in the local filly community and a lot of people have influenced and touched by her work so I'm really happy to have Kate come up and give our opening keynote. Welcome Kate Krauss [clapping]. Kate: Hi everybody I am so happy to be here. I am really honored to be chosen to give this talk and libraries have been incredibly important in my life and my family's life. If there's one thing that I know we have in common as a family the Krauss's are library people we all remember the day we got our first library card and so since that time actually I've continued to work really closely with librarians as you'll satisfy later. I have done a lot of different activism it's true but the talk today that I want to give really focuses on my life in west Philadelphia. I live in a row house in west Philadelphia. I love my community. There are many community groups, the area of the neighborhood is very full of grassroots organizations and community action and wonderful institutions. But not everything is perfect there. And I want to talk today about the world that libraries and technology can have in what many people see as a golden age of online surveillance and what we can do to help and protect our community. When I accepted your invitation to do this talk I thought about the communities that I know who were most under pressure in my area anyway in the United States, and most under surveillance. I asked activists in them what they really wanted out of libraries and I interviewed a bunch of people from a number of communities. I ended up focusing on two groups, black lives matter and the trans gender movement but first I want to tell you a story something that happened to me in 2004 in west Philadelphia. I was coming home from work and I -- I'm still having problems with this, it's not coming up. Not staying in presenter mode. It just keeping jiggling back to this other mode, I don't know why. All right. So I do live in a row house in west Philadelphia and in April 2014 I came home late from work, I was in my bedroom and I started to hear helicopters circling around my house which if you live in west Philadelphia that is not the first time that has ever happened but they weren't stopping and they were increasing more than one helicopter for a really long time and I couldn't figure out what was going on. I peeked out the front door didn't see anything and I finally went on-line because I was starting to get frightened and I could hear speakers but nobody was visiting me or knocking on my door and I had gotten an e-mail from one of my neighbors telling me that a 19-year-old college student had been shot very close to my house and he was, he had a part-time job, he was delivering a sandwich and he was two men with guns came at him and he tried to get away in his car and it didn't work and she shot at him and they actually shot him three times including once in the face and they were unfortunately Philadelphia police officers. And when this happened I have to tell you that I was horrified and upset and it galvanized our whole neighborhood and really the city. But we didn't really know what then do you do. The person has already been shot what do you do, what do you dot and we still don't necessarily have the answer to that. I just decided to do something as opposed to nothing and so I went to the hospital and I brought the family some flowers and talked to them because I wanted to know everybody in west Philadelphia was not trying to shoot and kill our child. Later our neighborhood got together and we raised money for this family. It's an inDeming milk problem in Philadelphia and so many cities and there hasn't been an answer or any real movement as to what to know what to do until black lives matter started. And I feel like that group has really been surfacing this issue which has gone on for decades but no one has known what to do as a pressing issue that needs to be solved. And yet they are under themselves under incredible surveillance. Here is just a couple comments about black lives matter. Typical black lives malter demonstration of high school students. They are being surveilled their social media accounts are being surveilled their locations are being tracked by a wide range of law enforcement. And they are political activists pushing for social change. And it is not unusual that this should happen, I'm looking for the slide that talks about it. There we go. You know, we know right now that there is surveillance of black lives matter. Just two years ago with occupy Wall Street there was a huge list of agencies that were infiltrating and surveilling occupy Wall Street that is the group that brought to service the issue of student debt which had been going on for decades and decades and no one had thought to do anything about. I talk about student debt my mom knows about it it's a big thing but before that people took it for granted they were going to have gigantic academic loans. So me that's one of the things occupy Wall Street worked on. They all readied through Foya documents we know about some but not all of the surveillance that that group endured. If we look back at the civil rights movement in the 50s and 60s we run into Cohen dill prowhich is the worst ever owe prigs of political activist in United States. If you aren't an expert on it already you can look it up it's awful awful awful. These groups are important and a way they've always been surveilled but right now they can be surveilled so much more acutely than ever before because of the tools that are available because of the budget, the 60 billion dollars budget of the department of homeland security which there aren't that many terrorists but they still have 60 billion dollars to spend so they're looking for terrorists. Maybe where there aren't any. And so you know when I think of this group, these girls, this is who we're talking about. You know we're talking about high school girls we're talking about the founders of black lives matter who are three African-American women in their 30s and they are being followed everywhere and tracked. I spoke to an organizer with this movement a few days ago actually and he was saying within the group they are aware of 19 people who are experiencing extreme surveillance right now. And there has been some media coverage of it. And this is a quote that always stays with me I did do this activism in South Africa around AIDS and when I think about the surveillance that black lives matter is enduring I think about this question, what are we going to do to try and provide some solidarity for them. Here is just a quick map of info catchers. Those are the things that they were using with occupy Wall Street and maybe using with black lives matter. Vans that have cell phone towers they drive along a demonstration and they can scan all the phones that people have and they know who they are and they know what they're doing on their phones. Whoever is within range so it's not just people in a demonstration. And these are the states that are using empty catchers. So as you can see we're in one of those states. So this is one of the communities that I feel could really -- is in a vulnerable situation and maybe could use some support in thinking about confronting surveillance. It's getting some support but it's a community that I think we might want to focus on. We meaning people that are interested in helping vulnerable communities who are facing surveillance. Another community that I've thought a lot about is the transgender community. They are having a big year in the media, there is the movie I think it's called the day nish girl. Transparent the TV show there's a whole series of articles in the New York Times and other many stream media but there is still a huge amount of sdrim nation against this group. And it's all about for this community control of online information, identifying information can mean whether or not you get a job, mean something bad happens in your family it can even be a life or death situation. And here are some statistics about the transgender community right now. Huge rates of people not being able to get jobs, huge rates of people being fired, huge rates of poverty because of that. And it's really a very vulnerable community and surveillance is a part of that. As we think about the role of surveillance in libraries though there is the point of which we might want to consider helping or supporting vulnerable communities with education or tools but there is also the point in which we ourselves may be collecting information that puts our users at risk. That's something else I've been thinking about. I'm about to focus on a company called Aon which does, let's people in to see archives and tracks who is looking at different archives and is the unfortunate recipient of my attention on this but it is just an example and people who use aeon are wonderful people but I want to point out there are political or social risks or prices to pay when we collect data that we don't need. So it's not about aeon it's just to point out where you think you're coding you might be doing a lot more than that and its good to sort of think that through. This is mayor copiss transgender activist and she makes the important point rules and body values. When we look at aeon they collect information about birthdays and home addressed and phone numbers there's a spot for your picture, I don't know if it's required that data could be accessed by people for generations of staff members I think. So you know if it's at pen there's that set of people and then there's the next set and the next set five years later or ten years later because a lot of institutions keep this data forever and there may be really important reasons to keep it for a long time but when you keep that data you are siding with your institution you're not necessarily siding with the researcher or the user because you're putting that person at risk and that data can be subpoenaed. Here is a screenshot of Ann's user interface I'm not sure if you can see it but it asks for lots of stuff. And I just want to make the point that the decisions that you do make about what to collect and what to retain what to distribute and who can see what and who can not see things because they are locked away is politically charged, those are political decisions and that they are also moral choices in some ways. The ALA which I'm not an expert on seems to have come out on the side of keeping the most minimum amount of records for the shortest amount of time that you can to protect patrons to protect libraries or researchers. But there's another component to this, so there's these communities at risk, there's a role that we as people in libraries may be taking in collecting information and retaining information and then there's that sense that even though you're a systems administrator you're not very interesting you're saying to yourself you don't really have to worry about people surveilling you, you're not a spy, you're not doing any real political activism you're just a guy, woman, working, maintaining a system. And I wanted to show you this film which we hope will actually work about a group of German systems administrators and that's what they felt and this is what happened. It's a film by Laura Poitrus that she made for intercept. If it doesn't go well I will just explain it. Let's see if I can get it rolling here. Question is whether there will be audio. [Film playing in different language than English]. Thank you to DUR spee gel and the intercept for letting us show this. There's an article that goes with it for intercept that you can read and these are documents from Edward snow den that were divulged and we don't know how many more there are and how many more programs there are dozens of programs like choke point the program that is described here. Many of them target central points like systems administrators and you may say to yourself oh, you know I'm the library person, I'm not -- our library is not doing spy stuff but you do have data to protect and you have the power in many cases to protect it. And I just wanted to make this probably highly inaccurate list of what I see as data that you may need to protect. I know lots of politicians with library cards and I know lots of activists who do research for talks like this one using the library and I myself discovered recently that the Philadelphia public library has a really exciting online movie thing that you can now access, you can watch movies from your house as a patron of the Philadelphia library but where does that information go and how is it protected? I don't know. So I'm very tempted to watch those movies and I am very interested to find out more about how that data is used to protect us. There are so many things that IT librarians can do to support their patrons and protect their data. Further down I have the ALA sort of guidelines for retaining data and but there are so many things that you can do. You can teach online privacy workshops that talk about the issues of freedom of expression and then teach maybe one tool, maybe you talk about the issues and you teach everyone in the room to use signal. I was sorely sorely tempted to ask this entire room to download signal a little while ago but I thought that would be a bad move as a keynoter to bring down the wireless, but I encourage you to use it I use it every day. Signal is a program, a free software for Android or iPhones that allows you to make encrypted phone calls or text with extreme inkrepgs great inkrepgs and it's inkrepgs that got someone in Brazil arrested the same people I have met and know and saw three days ago that they had developed it's by something called open list persistence. It's not -- it's just what we all use and I encourage you to use it. It's super easy to use. While I was on a trip I taught two 12 year olds to use it and now they're using it just not having inkripted phone calls when it's so easy to pick up phone data. Also I encourage you to ask yourselves and -- about what information or this is what I've learned, this is what I've learned in interviewing some of you about data collection. What are you keeping far away from people and what are you bringing close to them and what is easy for them to access and what is difficult for them to access and what are the social implications for that. What is the hot information that's hard to find that say a political activist might want? Is it purposely squirreled away? Would you move it if you got a phone call? If you got a phone call from law enforcement would you hand over data that you've collected from, say an aeon type program. If you have it you'll have a horrible choice to make. If you don't have it, you won't have to worry about it. I heard this expression don't collect what you can't protect which is a little snappy but seems to make sense. And so those are some things to think about. These are moral choices even if you've got your hetd phones on if you're code fog aeon if you're coding for a library that is protecting or not protecting user data you are making a political choice you are making a moral choice. Whether you want it or not, you have chosen the career that is the hot button civil rights and human rights issue of our time. You're in it. You're in it whether you wanted it or not, whether you cared or not, whether you picked it because you were not political you picked the wrong thing because this is where freedom is playing out. If you have privacy at home but you don't have private online you have no privacy, right. That is what you are working in every day, those issues. I think I'm using special corrections here wrong so my apologies. I got some really interested ideas from the people I spoke to. One of them is to put together lists of books or you know little tables of books or online groups of books for people who are interested in civil rights around African-American issues. And I love the idea of black feminist and intersectional books and sort of a sci-fi thing. What if we lived in a world where there was no racism. There is a sci-fi about that. One of those people I spoke to would like those to be available to people in a group. I had the idea of putting together a collection for people that don't know anything about the movement for black lives and are like what's going on what's going on. I know nothing. Because a lot of times if you ask an activist you're like you you have to learn on your own and it would be great if library provided resources for people to learn on their own. There are some really innovative things that libraries have already done and I'm learning more and more of them and probably in the questions you'll have even more which is exciting. One that happened I guess last year was the DC public libraries privacy month which was a national program so there may be libraries here that participated in it. They had a whole month of activities which is -- and their website is DC library.org 1984 and it's a little ironic they are right in the middle of Washington, D.C. surrounded by federal agent cease they life streamed reading of or wells 1984. They had film showing I highly recommend the Internet's own boy which is about Aaron Schwartz. They gave workshops on how to use privacy tools and it was really a big hit, they got grant funding to do it. Another really great innovation of the last 12 months is the revolutionary librarians who have been installing tour nodes into public library. This is Allison McKorean in a and u nis is a who have worked together to put the first library to exit node to public library in New Hampshire in a shall down in New Hampshire Lebanon New Hampshire and they had this installed the way that Torah works is users have on-line anonymity but volunteer computers provide the places where connections hop all-around the globe before they get to the final designation this is one of those volunteer computers. And they installed it and everything really went well until the department of homeland security knocked on the door of the local police department and said if you have a tour exit notes any traffic could go through that we would not be able to see it or investigate it, this is wrong and you must take it down. And the library panicked and they took it down. And then there was a national outcry of librarians activist librarians that this is a great opportunity for a library to demonstrate freedom of expression and their support for freedom of expression in a tangible practical way and the library board voted on it and the relay went back up so it's happening right nouf. The second library who has a tour node is going up in value lensy I can't Spain and there are a line of libraries that would like to join them. It's a way of saying we are for freedom of expression and doing something technical that backs it up. So that's something else people are doing. On a much lighter note asher wolf who is an online activist for digital rights and freedom invented the krip tow party a few years ago and that's where you literally have a party with a small group of people or I guess it could be a huge group of people and you have cupcakes or beer or whatever you want and then you teach each other privacy tools, how to use signal or how to use tour or PGP e-mail encryption and the great thing about that AIDS of A it's fun and friendly B it's the best way to learn these tools to have someone sitting next to you. Also there are privacy properties you can't be man in the middle if you're sitting right next to each other so it's actually great in that way as well. And it's taken off it's really really popular all over the world. Here is one of the sample posters from them. Here is one from the krip tow party lar lem which has luminaries in there. You don't need luminaries you just need friends. It has been really pop letter. Inkrepgs is an activist issue it works and is in support of freedom of expression and the right to privacy. I wanted to talk a little bit to say you are living in a city where you are aware of a vulnerable community you feel may have problems with online surveillance. And maybe doesn't have the tools you think or the tools to help themselves. And you're wondering about that, you don't know, they're not your friends you don't know them at all. How could you make friends with them and invite them in to find out if there are ways that you can support them. And this is very simple this is like a sort of basic community organizing model where you find an active vest group or advocacy group or some sort of ally group that knows them, you talk to the allied group, you invite the allies to lunch, you ask them to bring members of this community that they think would be interested so that members of the community they're not by themselves they have friends with them. You bring in a a librarian, you bring in a tech librarian and you bring in maybe a communications person or some other people from your library who are interested. And you basically listen to what this community is saying that they need, make a list of the things that they are saying that they need and then see if you can help them with things, with tools or events or skills or whatever that will really help them. By listening really carefully it's sort of a cliche and it's weird that I have to say it but there's a whole bunch of software developers that sometimes want to make a tool that no one needs for a problem that doesn't exist. It just happened two days ago I saw someone try and do did and the response is we actually solved that one problem thank you though. So it's just you know realizing sort of rook emistakes thing to know not to do. That's how to address a community you don't know and maybe make friends for the long-term and build your community and help the people that you can see are in a vulnerable situation. And that is basically all. I have one ex-soretative comment. If you are an IT librarian and I think a lot of people in this room are, you are in a battle to preserve people's online privacy and freedom whether you wanted it or not, it's too late. The cow is out of the barn. Library can be an embassy in that battle and it can be a moral, a public moral contest which we've seen over and over again over decades and decades of library work and helping to preserve American's rights to free expression. What you do counts even if you are coding all day not because coding doesn't matter but because it's technical as mayor rit copa says question the rules you are following. You can make such a difference in helping highly surveilled communities stay safe on-line and to borrow a phrase from back lives matter. Stay well and that's all. . We have a couple of minutes for a some quick questions if anyone has anything they want to add we have microphones set up in the aisles so you have anything you want to add do it quickly. Great talk thanks so much. I found this really moving and also very challenging. I'd like the quote you had from ALA American libraries association that points the historical activism among libraries to protect patron use data and I would be curious to hear your thoughts on Google analytics as we offer online resources and collect patron data with free tools. Thanks. One of the things that people talk to me about when I was interviewing them for the talk was the sometimes unthinking and I have definitely been that person very sympathetic to this, it's free, it works so beautifully, why can't we use it. The tour project doesn't actually use any analytics so there are a lot of things we don't know about our own millions of users but I think you have to make sometimes hard choices and I also wonder if by identifying the fact that this is a hard choice if the library community can find alternatives or like we have these weird ways of doing AD testing without analytics where we just do it all on the if irs day and we do something totally different on the second day then back to the first day. Then that's our AD you know. There may be innovations that we aren't sort of thinking through if we do rely on analytics or a model where your data is meant to go to the company so that it can show advertising. I don't think that's the worst thing in the world but boy it can be really hurtful to users. Any other questions? Yeah my question is about maybe putting this, making this something that's a choice like there are a lot of services that libraries can provide better to people if we have some data about them but is there attention -- there's obviously attention interest and I think one of those ways is to exclusively ask people for their permission to do this. Great idea. I think that's it that definitely makes sense but also people are so used opting in the Internet is it really opting in or not. Just having these conversations as you probably do every day I'm not sure is really essential and then brainstorming about what else could we have do, what else would ever work. Or maybe Google should be funding something else for libraries to protect library users. Google funds some awesome things like Google summer code maybe they would be willing to do something different for American libraries I don't know. Opting is certainly I would say should be mandatory. Anything else? Okay thank you again Kate Krauss [clapping] okay. So we're going to take a quick break 15 minutes then we will be back to do some give aways. We had a request come in through the chabl if everyone could please kindly silence your phones all the beeps, swooshs and for those of you in the next session if you could please meet me during the give away spot or time over here I have a couple of things to go over with you. But other than that, we'll see you all in 15 minutes. [15-minute break]. Hello is this thing on. Okay? If everyone could find their seats we had to skip the give aways we had some technical difficulties. We'll find room for that later. As people are shuffling in. Everyone is sitting next to an empty seat please raise your hand so people can find their spots. Try and keep them up for another minute when your hand starts to turn white you can put it back down for a couple of minutes. Okay. That's good I think we can start we've gt a lot to do. First up is can't wait for perfect implementing good enough digital preservation with Shira Peltzman and Alice Prael. Where are they? Hi guys can every one hear us okay. Sort of great. Hi this is our very first Code4Lib for both of us and we're really excited to be here [clapping] I'm Shira Peltzman and I'm the digital ar can Ivist and formal national stewardship at Carnegie Hall. I'm Alice Prael at the John Kennedy lay praer. We've got a ton of ground to cover so we wanted to quickly share the slides today so it's easier to follow-up and that's up in the center of the screen. Since a lot of what we're discussing today was inspired by our time as residents in the NDSR program we thought we would begin with the origin of the NDSR program and the digital preservation projects if which we were responsible. Speaking of digital preservation we realize that this can often feel like something of be a neb lass projects we thought it would be helpful to include a very brief description of the bedrock principals that will brings us to what we're really here to talk about today which is the fact that realistically the gold standard of digital preservation is always going to be just beyond reach. We will define good enough digital preservation and finish up by offering several concrete tools, strajees and practical approaches that will help you preserve digital preservation. The NDSL bridges the gap between professional by providing hands-on experience managing digital projects. There are currently three residency programs in New York DC and Boston hosting residents for 9 to is it months with the newest. Each resident is matched with the host institution where they're responsible for managing a project related to digital stewardship. The project at Carnegie Hall as JFK presidential library focus on digital strategies which included high level planning and policy work as well as the hands on work of implementing systems and solutions. Even though digital preservation may seem like a relatively new discipline the truth is that preservationists have long understood the technical and practical requirements for maintaining digital information over time. I'm going to take a moment to talk about the three underlying principals of digital preservation since they're the foundation upon which tool stand or policy has been built. Bit preservation, regardless of it's content or file type every single digital file is comprised of a series of 1 and 0 called bits over time these can flip from a 1 to 0 or advice remember is a which can have a devastating ability. Bit preservation wants it to remain constant or fixed. This is the most fundamental of digital preservation and including data security and back-up. The second principal of digital preservation is content accessibility this means the ability to ensure that a digital file can be foundry treefd delivered and successfully opened red or played back to you. Technology advances and file formats as well as the program's necessary to read them will change over time you may need to migrate your digital files to newer formats or use emulation strategies in order to maintain access to the files in the future. The final principle is on going management. This technology is the fact there is no starting or stopping point to digital preservation and it's an active continuous process that requires on going support funding and engagement on behalf of the repostories management. There are several international standards that have been developed to help institutions manage and plan their progress but the two mentioned are the reference model $open ar ki vial information OIS and audit and certification for digital repostories or tract. What institutes good enough digital preservation. In libraries and archives we hear a lot about best practices but sometimes best isn't feasible so that's when we rely on good enough. It can be defined by the available staff time and budget the needs of the collection and the priorities of the institution. In short digital good enough digital preservation slt most you can do with your current resources which is probably a little more than you're currently doing. This is the concerning point because no matter where you are there is usually more that you could be doing. Inherent in this definition is the notion what constitutes good enough digital preservation will differ from institution to institution and may change. You have to figure out what's good enough for your institution. Digital preservation is iterative by design and good enough is a moving target to remember these are things that will evolve over time so how do we follow-through on good enough digital preservation. The rest of this presentation will focus on the specific strategies and resources we have employed in our work. They fall into three categories project management advocacy and category we're calling doing work tools and costs. Starting with project management our first piece of advice is don't go it alone. You don't have to reinvent the wheel especially when it comes to policy. Lots of organizations have shared their policies for digital preservation for look to these for guidance. Scalable preservation environments has published has a collection of published preservation policies available on their page. Policies are category recessed by library archive or data center so you can see how similar institution to yours are directing digital preservation for their collections. These are also useful for creating a framework to understand what pieces to include and how you might organize your own policy. Library work flow exchange is also collecting work flows for all things library which includes digital processing and preservation. If you have a digital preservation policy or procedure documented consider sharing them with the community widely shared documentation helps us as a field better understand how digital preservation works in the wild. So before you can begin prevefshing the digital material you need to know what it is you're preserving. Understanding your holdings will cover what work flows you need to establish. If you're dealing with physical external media you immediate to specific tools without disrupting the original order. If you're accepting files from an external party you need something to validate those file types. Understanding those will help you identify risks and address obsolescence issues before they become cat trough particular. Inventories should include description and location of the physical media as well as the digital files and file formats and the size and ekts tent of thotz files. The level and detail will depend on your institution an institution with a few small digital collections will probably be able to create a much more detailed inventory than a large archive the goal of this inventory is to inventory all of your digital content so don't let perfect get in the way good enough. The Library of Congress dig talking preservation outreach and education program provides us with a sample inventory sheet which is what you're looking at here it's a really useful tool in figuring out what your inventory should look like from the start and this should be kept current so add to it as you continue to acquire digital content. Make sure it's accessible and create any useful and stainable format. Having an up-to-date inventory is a great tool for budgeting and advocacy. Arming yourself with a better understanding what you actually have will allow to set priorities for how much action is necessary. Something to keep in mind all digital assets are not created equal. Something to take into consideration are is there sufficient key manned from patrons did the content people within the mission of your institution is this content already being preserved elsewhere and can this did I think tal object be recreated if necessary. Digital material is often high pry or because if it's destroyed or damaged you can't replace it. Digitized assets for there is no an lolling copy are high priority for the same reason. They often become damaged or degraded. These high priority objects require more robust digital reks which means checks to remake sure the file remains the same over time. The more copies and geographically separate locations so if someone copy is damaged it be be replaced. It may include digitized assets or digital objects that are already being presearched elsewhere. If you do want to preserve these lower priority assets they may require fewer copies and less rebust format monitoring. Knowing what you have is certainly a crucial first step toward becoming a better digital steward sometimes looking at an inventory feel overmember whoeming. Which is a good idea in life is turning it into a series of much smaller achievable sub projects orthopedic tasks rather than taking it on in it's entirely. Start with something manageable by generating metadata about each of the files you're working with. Move onto another task that you have the resources or expertise to tackle in-house. The key is to make a little progress every day which is in keeping with the idea that good enough preservation means doing a little bit more than what you're currently doing. When you're deciding which piece to start with it can be helpful to pick the lowest hanging fruit first. If it seems inTim Tating focus on something do-able. You maybe start with creating fixity information for the files as hand. This is something you can do with free tools and watching how to's on YouTube that you should not underestimate power of. Starting small and building on those initial first steps. In order for materials remain accessible to users over a long period of time organizations must. What exactly is high level responsibility. Simply it entails committing resources across all levels of an organizations which is a flexion of the fact successful digital preservation requires participation from colleagues across such a wide range of roles including administration IT reference library I can't answer and so on. When it comes to digital the paradigm similarly will not work. You need to be able to collaborate across the institution and advocate up for continued resources. Having clab ray terse on board dlout your organization is essential. We recommend taking a page by power which stands for preserving digital objects with rekted resources. During the workshop participates called the 333 action plan which stands for 3 people 3 months 3 activities. First participants identify the institutional role they believe are important to involve in the creation of digital preservation program. Participants list three vitdz at their institution and make a plan to reach out to these potential clab ray terse. Finally participants list three activities they think can renal complifrn. Creating a team with set goals is a great way to start moving your program forward. Once you reach out to your colleagues what will you say. The reality of being an effective advocate for the material is you often find yourself why preserving digital information is important how it works and what roles your college will play and what what support you will need to make this work. Even if it isn't typically considered aus such being a communicator is good. Still in the essence of the concement you're trying to communicate can be a difficult task in and of itself and ta lettering your explanation to your intended audience can be even harder in particular knowing once used jar began and when to avoid is vital. To adjust this the digital power team has created editable documents to help professionals. Not only are these a fantastic resource but understanding the perspective of each flier can be extremely struck active. Something else we recommend is to up date your organizations mission statement so it includes digital preservation. This may seem simple but this can prove to be a hugely beneficial thing it impacts the widest number of a you sets using the fewest possible resource. Establishing writing as part of your organizations mission will achieve four things serve to raise awareness of the digital program within your organization and beyond it, it will help to provide a justification for all preservation efforts in the future, it will bring your organization in line with digital preservation standards that require this explicit level of commitment and finally it will provide grant making bodies of your institutional commitment. . And that last point is especially important because the you will goly truth of digital preservation is that it's expensive. There's initial investment to appraise content crate meth data you have to pay to storage and management which in many cases is forever so it can being pretty daunting to try and budget for forever which is why many organizations only make a plan to five to ten years this allows you to plan for preservation without being overwhelmed and given the rate of technological advancement you will need to resaeks anyway. So since preservation is going to be expensive you want to be sure that you're spending your money wisely. Meta archive created a list of 20 archives for cost police station to help assess potential questions like initial investment versus on going costs how copies are geographically distributed and define the line between in-house staff responsibilities and duties that should be handled by the digital preservation solution. These are great for an institution trying to decide what systems in storage make the best fit. So there are two central resources for assessing and discovering digital preservation tools. First is the community owned digital tool rentaling stray. It is a which can I that allows browsing. Each description has compliance with community standards and there is a space for users to share their experiences and provide updates about on going development. The second is the tool grid created by power group which grades tools against required digital press vase. The grid was created as a static document but recently upgraded to automatically pull information. As the name states the registry is community owned and operated so if you're using a tool that isn't listed or if you have a user experience to share consider contributing to the which can I. Tools like this succeed because of community involvement and we strongly edge courage all of you to participate. The levels were created by the national digital stewardship and address issues listed here. They are incredibly useful for how you're handling digital preservation. At the JFK library I color coded with green yellow and red to show we currently stand so I could advocate for better digital preservation. Since this is such a common tool for getting started we decided it would be worthwhile to map each level to the specific tools resources that can be used to complete the relevant requirements. Some tasks like creating files information and scanning in coming media can be managed by a single tool while other tasks like managing nfks security will require a number of duals and policies. It won't go every level in detail but essentially we've created a road map for digital preservation with tools from policy recommendations and other resources we've relied on. As an example here are the resources for managing file formats fits which follows formats by pulling together by drivewayed and drove into one environment the the format registry PRONON which can provide finding formats that are acceptable for your institution. And the and a checklist to aid organizations in managing content migration created by NDSA and finally the aspects of a digital preservation policy that address the file formats in using your collection. While NDSA levels has been incredibly useful tools how to enhance your preservation programs in certain areas there are definitely some blipd spots. The complete access of any access related benchmarks is a really crucial one. Providing access to the tool you're preserving is a foundation of digital preservation but it continues to be under valid to copyright issues to cost concerns. Shortly after my arrival at UCLA we used the NDSA levels as a guide but the lack of any access related guidelines was a sticking point since access is so central to our mission. I created an addition that's designed to help an institution measure and enhance their capacities to provide access to the materials in their care. Since there isn't a lot of time left I won't go those in great deal but each of the levels describes a couple of activities ability to redact pefrnl or sensitive finding. Collection sclipgs descriptions and guidelines about the metadata your institution gathers. Before we wrap up we want to leave you all with a few parting thoughts first and foremost we hope that the tools and strategies we've mentioned will help you in the long run. Our hope for this presentation was to be a jumping off point how well a digital preservation program your institution has. We want community input we're interested what you guys think about either the additions suggested to the NDSA levels or recommendations throughout this presentation. In the lifrng to our site that we've provided here you will be able to access an editable Google dock as well as our access related additions. We'd love to hear your feedback so please feel free to add to what we've got add comments and make suggestions. Thank you [clapping]. Thank you Shira and Alice. Now is the moment we've all been waiting for enabling access to old Wu tanning clan fan sites with nick rust and Ian Milligan. Hi everyone my name a Ian Milligan and this is in this case rust. We want to talk about facilitating interdisciplinary archive collaboration. My first note is why should we even care about web archives? In short how we preserve and disseminate information has changed dramatically. Since 1996 and the advance of web archive and the national libraries how we remember our histories has been dramatically altered, altered in it's speed and the scale in which we preserve it and in many other dimensions. We can see this in two dimensions first more data than ever before is being preserved and secondly this will be saved and delivered to us in very different ways. And the primary way that we work with web archives and most people work with web archives is this the work file. The big data of contemporary history the archive box that I work with. What is a work file it's an ISO 25002009 stand we've been able to build a community around the world working with these archives. What it means is we can take a website a universities website and you would see the different blocks there imagine the first one is a JPEG the second one that PDF the second one is word perfect file, you've got all these different files, and so a work file let's you jam all these different components together and intersperse them with metadata headers that describe the content you're preserving. What that really means for me has a historian is that imagine in the future a historian is studying a question of critical social or economic or cultural significance. Something like occupy Wall Street. This is the kind of archives that they're going to be using. But the problem is when it comes time to study something like occupy Wall Street, this is what I fear our archives will look like. It's sort of boxes or works if you will stretching into the assistance you can't bring this archive into perspective. These are so large we don't have finding aids or at least not finding aids as we thought of. And unlike other collections I can't reach out to an archivist who will have an grasp from occupy to geocities and beyond. So if my problem before when I write my first book on Kennedy 1960s don't buy that book, I can tell you the synopsis at the break my big problem before the historian was I always wished I had more information every day I wished these archives were bigger, more files, more data. And as the late great histor I don't know right away Rosen wig that problem he thought was being ecollapsed. My biggest problem is now abundance. Instead of wishing for more information I spend almost every day with nick wishing we had less information so when we look at something like geocities a service that started in 1994 lived until 2009 when the guys who killed it like they kill everything we see reallying the exponential growth of the sources that we're working with. That has been October 1995 geocities had 10,000 years August next year 100,000 and just a little a year after 1 million users eventually going to 7 million users creating those websites with 1, 2, 10 megabytes. We're dealing with historical serve. These wouldn't have been preserved if they were produced in a predigital age so the question I ask when I go reach out to nick and we do our work together is that I don't think you could write a history of the 1990's or beyond if you did not use web archives. And I think that's true for all historians I think if you're political historian and you want to study election, policy, how things are esolved you can understand what's going on with trump right now and less used web archives. If you're a military historian you're going to have access to the voices of rank and file sol years which are being preserved in boards and other parts of the web and for someone like myself a social and cultural historian it's even more evident we're seeing the voices of people who would otherwise not be part of the historical record. And the sad thing especially to those of us who lived in the 90s is that the 90s are now history and they will soon be history. It's painful. And the example I use is my first project I studied as a historian was the 1960s and if you take the year 1968 and you say when did historians consider 1968 to be a valid period of studs did I it was about 1986 when you could write a book about 1968. Late 1990's you could do your dissertation. 20 years between an event when historians start to study it we're at that 20 years now since the birth of the web the web is our history. Right now if you want to work with the web you have to use the way back machine and if you want to use the way back machine you have to know the URL of the document that you want to study you have to go here and here I want to look at geocities.com we see the bubble charts, few end of life crawl if you go to it now it's Yahoo small business pages and you can painstakingly go through page to page to page. We know that's not how historians are going to use web archives. The Internet archives it's about search engines we want to provide access for serious historical scholarship. I'm the input I ask the question I ask this magical black box the output comes understand the ranking mechanism that black box is writing my book I'm not writing my book and so to get rid of that black box we need to work interdisciplinary and that is where nick comes in. So I'm going to spend the next few minutes talking about a team in like some parts of the research project that we're working on. So the first thing like this is all comes under aide social sciences and research counsel. Thank you so much to them. We have five year grant eye I can't be is the PI I'm the coPI so on the infrastructure is travel to give these talks and getting data soughts we got a compute Canada grant it is a high performance kind of research computing government entity in Canada so we're able to spend a pretty huge clusters to start to do some of this research we didn't do on a large machine we have now. So the team you'll see WuTang lyrics so we have an interdisciplinary theme. First the one thing that brings us together is it what rules everything around us. We're able to work on this common thing. So Ian like I said is a history faculty member at the university of war water reduce he is the principal investigator on the grants. Jimmy listen who is a computer science faculty member we've talk about the work he's created in a little bit. He's created work base which Ian will talk about. We have Jeremy weed who is a thinks electric candidate he has a background in computer science and his Ph.D. works on work base as well and ally who is a comp sigh undergraduate. She is wonderful and then there's me which I wear I don't know a million different hats. Project management, archivest, developer, et cetera, kind of glue things together in terms of the team. And in terms of collaboration like I guess we're all on slack now like the main things that we use are slack and get hub. We go back zinc nis city and slack. We have a variety of get hub reviews we use get hub to write papers we try to write in public where we can depending where we're sitting and allow for open peer review and platforms we use a lot of so I'm going to flow through this because we've got 11 minutes to go. So one of the first platforms we started working with we got the notice the grant last May and one of the first thing we did is we had a data set we got from the University of Toronto Canadian political parties and there's this really great thing that the UK web archive wrote if you know Andy Jackson and the wonderful team there they made this. It is a play framework which is a scaler like NBC cool thing and xins apatch chi solar and we can explore them. We feed it into work based selection and solar for us and we can do things like see if this video will play. Can we do it? Yeah we can. Okay if you go to web archives you can kind of play around with this right now we will look at the trend searching this is one thing that comes out of the box with shine. What we have up here is three can dayedian political parties the conservative the lib brals and the NDP and we're searching on transit, public transit and you can stee a spike there where the liberals we're talking about talking about transit and then they stop talking about it because they lost the election and then kind of goes away and it can just jump to specific sites. You can see the same thing with the NDP as well. And then there's fas sitting on the whole corpus there's 14.6 million HTML pages you can go through and search and go through. What's next? The other thing is of course we make heavy use of regular command line tools that do all this, parallel sort, unique, WC those are our friends we use slack and get hub to shared knowledge and how to use them. And use JQ a lot for the Twitter data that we work with. And then finally we've got 8 minutes or so we're going to talk about really great data set we got from after kif and been working on it for the last three weeks so back to him. So this is just in time because as nick said we got the geocities about three weeks ago so a sprint to get it here. Geocities I mentioned it briefly before most of you are probably familiar 1994 to 2009 user got a free megabathe then 2 and 15 a a lot of it came in neighborhood if you were a kid you made a page in the enchanted forest if you were an academic in bued pest. First of all we couldn't actually use command line tools to work with the data set because it was about 4 Tara bytes of data so we had to use work base so work base is an open source platform for managing archives. It has a flexible data star you can run your way back machine and has scriptable analytics and data processing so we primarily in this talk we're talking about the second part how you can use work based to run great analysis on it. We have done whole talks on that slide that's basically the framework of work base and I'll give you some links in a second to find out more yourself. Work base is pretty cool. It's inherently scalable so you can run it on a ras Barry pie you actually can I'm not sure why but you can. You can run it on a laptop desktop beefy serving that nick has at York university where you have 3.2 tear a bytes on ram I take it on a long week end. The scripts for work basic run on my laptop that I can run on that class sister so it scales up and its 23 times faster when I run it on a 23 node cluster. It's an active and led by the Jimmy our clabter. We have work base.org leave issues, fry it out, complaints elts and dots work base.org which I'm trying to make it intelligible to people who don't know the basic code of writing work base. I want to do a quick walk through of how we've used work base on geocities hopefully by now you think web archives are cool you think collaboration is cool you think work base is cool. We get data from the Internet archive we write them cajole them say nice things we got 2 big Tara byte hard drives. We plug them into our server at York and there you have it 7 million user on geocities we can now access. We could run it in a few ways here is the basic shell we use to run basic inquiries I forgot my laser pointer, there we shell into to a server we load up the shell and you'll see we're pasting commands to get analysis so this command is just our test command tell us about ten domains and I try to tell other users and humanists you just change that line paste out the directory accordingly. To lower the bear yes we often prototype in spark notebook we're going to move out jupter but again here in browser visualization so to try our scripts on one or two works then we would go to the terminal window which can turn off many of our users but this is a prototype spark shell is our submitting mechanism. Just to show you scripts we did this is what one of our work based scripts this one generates URS list turns out it's big 186 million URL but to walk you through this basically that line there pointed at the directory say I want you to get the URL save it as text files farply human readable file that's our job. Lately we've been working on link grafts here is the code to extract the links for geostits we ran that we get this, this is about 404 giga bytes each with an origin site and designation site and the number of times that site is linked. These are linked farms that were popping up at the end of geocities. We were interested in the pre-1999 geocity sites and pre-1999 you all had an address unless you paid for the vanity URL which most people didn't. Your website would be enchanted forest grove 1234. The kids neighborhood there is a suburb called grove your site is is 1234 and we wanted to find a way to group those together your index page gets grouped together with your cats, dogsz and rab bits. This actually ran pretty quickly. Started includes terg we've posted our slides on Twitter and I'll walk you through this. We blafk started includes terg generated link grafts, there's meadow 1134 it linked 1083 times and becomes interesting to know why someone would do that. It is awards and we've taken a geee cities neighborhood and here we're finding what are the most important nodes based on page ranked ranking. The bigger the text. What nodes were perhaps the most significant what sites in this LGBT focused neighborhood were the most significant. We can generate nice little pictures like this. This here is the child focused enchanted forest. Zooming around in a laptop so it runs pretty smoothly and trying to find what sites are the most central and what holds it together. I'm curious was there a community in geocities and by finding the most significant one we were able to find this page which tells us a cool story about awards and also a cool story what happened this they shut down and the outcry about the end of geocities. Using this method that's two papers right there. And these are the historical uses that I mentioned and then to close it out nick to talk about our data sets. We blasted through this stuff it's hard to get it in. One of the other hats I wear in this project is the research data management and one of the research questions I'm asking this grant is a coPI is we've got these huge 4 Tara byte data systems how do we preserve and move forward. Creating how to create derivatives and bread crumbs to get back to what we need to do. Web after kifs we have a page with all the data sets for that and it goes into the scholars portal database we'll have links. A little bit of documentation for the data sets for this particular one how you should cite this data and why we are doing this. Right now in our data universe in the scholars portal one we are making Don talked Drumph tweets. We have the election 42 tweets that was the most recent Canadian federal election between us and library archives Canada we are confident we collected a lot of tweets I don't want to say all but I've collected the Paris attacks, Charlie HBDIO attacks and we have derivative data for this saet set. So the other thing some links we have a get hub organization for this project. We have a website at the university of Waterloo where all of our work is -- what? It's done. Get hub repos and our links and one Wutang slide. One of the goals of this talk was when I was a teenager I used to love making geocities site and specifically Wutang clan sites I haven't found my geocities site and I am derld determined for find it. Thank you and that's it [clapping]. Thank you nick and Ian up next is Julie Swierczek with gorilla -- digital preservation 101 how to keep it for centuries. Julie: So the content in this presentation was covered in the first one so this is a good time for you to write emails, hooray for redundancy and you can never have too much digital preservation. So once more with feeling. So this is the condensed edition of this talk the original was 45 minutes so hold onto your hats. So when I talk about digital preservation many people think I am talking about digital storage and when I say digital archives most people think I mean a back-up copy or even a zip file or a tar ball but that's not either. What is digital preservation. I should have called it the David at ten burrow edition of the talk so enjoy the animals. So it's rocket science seriously. In 1982 NASA and the other major space agencies realized they had a data problem so they worked together to develop data system standards for space flight. And they called themselves the consultative committee on space data systems and one of their publications is the reference model for an open archival inchings system which we lovingly refer to as OAIS and this is also an ISO standard because we need more abbreviations in our lives. So it looks something like this and this is a fun slide but you probably can't see it so here is a simplified version. Now one thing to keep in mind about the OAS model is that it's only a model. You implementation will depend on resources and what you are trying to achieve. So here is the basic idea. A producer submits content which is then ingested into the system as part of the ingest process descriptive and technical information about the content is captured. The content goes to archival storage and there is a data management layer that tracks the descriptive and technical information and other metadata. Notice the overarching emphasis on preservation planning administration and management. Those aren't just after thoughts slapped on a digital storage system. They are key components of the model. You could build a technologically perfect digital storage system but if you are lacking certain administrative features which includes things like sustainable financing, then it doesn't really adhere to this model and it's likely to fail. On the right we have a consumer who requests access a copy of the content and of the associated information is then packaged and shared with the consumer. So to understand why the model works the way it does, let's look at two key principals of archives prove unanimouses refers to the origin of the records or papers knowing the connection to the source is important for evaluating the authenticity of a document. The second important principal of archives is that of original order. The order matters. When did this author first develop this idea? When did Volkswagens en's leadership know they were installing defeat devices to cheat emissions testing. How do you deal with authenticity in a digital order and realm. The model provides a path however before I get into details keep in mind that something is better than nothing. We're trying to beat the clock. If we don't make any attempts to preserve digital content until we have enough money or a perfect preservation system, the content will disappear before we have a chance to reserve it. My first digital preservation system was two external hard drives. So think about it. The at electric has sis which is an Acadian epic is recorded on clay tablets that are nearly 3,000 years old. You can preserve those things by leaving them lying around in good environmental conditions. Try that fla floppy disk. So how do we get this stuff? We use principals of digital forensics including things like hardware writeblockers to ensure our data remains intact during transfer. Sometimes we have to use things like FC 25, USB 5 and a quarter or KryoFlux which costs a small fortune cryo flocks sounds like something within the had he had dron colieder. You have to supply your floppy drive which is why eBay is the digital archivist friends and we have gone down the rab by the hole that is retro or vintage computing. Now CS dat organize has a cool Andy war hall aus fans real estates queuing, I don't know why they saved it from oblivion but they did. At this point in the process we wrapped the incoming data with some information and we call it a SIP. We also calculate checksums of niles so we can verify that they remain unchanged during ingest and throughout their life times. Next we want to prepare the content for it's final resting place sort of. To do that we're going to set up some metadata because metadata. This will be part of a robust data management layer. We are going to need to store the original file along with a derivative copy in a long-term preservation format. What formats are those? Ask the folks at NDIP they do research on digital format sustainability. Then we will wrap the files and the metadata in a package and call it as API. Archival information package then we have to store the APE's. We want to store multiple copies in multiple geographic locations on more than one kind of storage media preferably not any of those. We will be monitoring the storage for long-term durability and accessibility but we are not done yet. We have to monitor file format obsolescence over the life of the file as formats become obsolete we need to create new derivatives from the original but if that is not possible new derivatives from the previous ones. We also need to monitor storage obsolescence over the life of the file. These are all facets of administering the system over time for decades and centuries. We're also going to record all actions taken on the files so that the chain of custody is transparent. Given everything I've said, if anyone tells you that you should get a one time grant for digital preservation, you have my permission to get mad. Finally we want to deliver these files to consumers. We wrap up copies of the files for delivery along with the necessary data and we call that a DIP. Let me be clear we call it a package of content a DIP we do not call the consumer a DIP that would be rude. The last question is about accessing that content in the future. We can do retro computing for awhile but we're probably going to have to do emulation. You may have seen the Internet archives port JS mess running their console of old video games. Carnegie medical on archive is testing the idea of virtual archives to maintain accessible. I know what you're thinking what does this have to do with me? When you start building a digital project think about the long-term preservation. Maybe you'll decide you don't need to save the content forever but if you want to keep it think about all of these facets and sustainable financing before you begin. There's a handout on my word press site with link from this talk plus a bunch of the fun stuff that I had to cut. Thank you [clapping] it looks like I have a minute and 25 seconds remaining out of my ten minute talk so if anybody had any questions I would be happy to pretend I can answer them. Anyone. All right you're experts now so you probably don't have any actual questions. You said receipt tow computing is what. Vintage computing two terms foft same the people who like to hangout in basements and spin up PDP 1's and things like that. Yes, I don't know what happened to it but it's TPver sew dot word press.com. For the catalogers the title page. Thank you very much [clapping]. Okay up next grill ra usable testing and communicating value with Ekatarina Grguric. We will try to get all of the slides on to the website on the talk pages so you'll be able to get them there as well as probably Twitter too. Hi everybody. Can you hear me? No, okay. Better. My name a EKA Grguric I am a library fellow and this presentation is whirl wind of gorilla computing and communicating value. I am going to start swt what of vis nl testing. Why you would want to do it how to get started with it and I'll finish with some value communication and I hope I have enough time. So what is visible. Jacob kneel son who is kind of a godfather of usable calls it a quality attribute that how ease a user interface is to use and what that means it's a measurable thing made up of a bunch of questions that you can ask of a design. Things like is it efficient to use, is it learnable, can you remember what you've learned once you've learned it. And also how people feelle while using it. Are they frustrated, happy or sad and is it something that's really slow to use. But really all it means is making sure something works well and that's a quote from the godfather of gorilla testing, Steve crook. So in essence out of these two definitions you can get this idea of having -- of usability of something measurable that also has the clear goal of making sure that everything works well. But how do you test for it? Well there are different strategies that you can employ at different stages of your prospes and the strategy that I'm going to be talking about the most is task based usability testing which really falls into the middesign stage of a new design. You can use it to assess existing designs but it's really most efficient to get the most bang for buck if you use it in a middesign state and what that means is somewhere between where you have a low visibility prototype in your finished design and also maybe in a common project work flow. Also maybe in a common project work flow into a development cycle because designs aren't always implemented as they're thought out. So essentially the way it works is you get a real life user and you give them tasks to complete and you ask them to think out loud as they do it and you observe them and that's it, that's the backbone of usability testing. And in a project management work flow as I just mentioned it kind of fits into a common waterfall approach it fits into the design section and a little bit into development section and it's really the most effective in a an iterative design approach so where you're able to have regular check in's to assess the way the design is going. And the best way to really integrate usability testing into any kind of project that you're dealing with is to schedule regularly occurring sessions and then cancel as necessary. If your project is a six month projects maybe that means once a month, if it's six weeks maybe it means once a week but you don't have to do testing every single time you schedule it make sure you have a slot saved in the project time line. What's the difference between gorilla and standard usable testing. For one thing it's a lot cheaper and the thing that makes it cheaper is you're using minimal equipment which means that you don't necessarily have to have a lab. You don't have to have anything pricey and you can kind of get a.m. chers to dive in really early on you don't have to have a full-time researcher on staff. Recruiting happens during the test and you're aiming at broad strokes you're doing it as reduce loosely as possible to get at the general gist of your population not at an absolute representative sampling of it. They are going to lose that on statistical validity but you will get actors much faster and it will still be fairly relevant. And it's content specific because of these things because they don't have these to deal with because you don't have to be recruiting beforehand. This was really good for libraries because we have this physical context most often in which our users are doing things interacting with the services and we can call on them to bring them into our process. Why would you want to do it. Libraries are user centered virmentsz everything we do every service we provide, every space we design interface we develop is for our users, for a very specific subset of the population. This is a strategy which you can bring users into your design process really early on and even speed up your design process by supporting things like objective decision-making. It's a conflict resolution strategy that helps you maintain objectivity in your work flow. It challenges the assumptions we all have about our users. It's impossible to not have those asuvenlgss especially if you're designing something and it works for you this is the way to check that, to check whether or not works for you works for everybody else. It's a communication strategy that maintains stakeholder engagement. What that means is that you're not only bringing down the wall between the people building the thing and the people going to have to use it but bringing down walls within your project team itself. There are silos that can occur between your project managers designers and developers and people who are dealing with the users who may not be directly involved with the project. By doing usability testing and by openly communicating your results you can be the communication venue between all of these silos and also ultimately uncovers problems while they're still cheap to fix there's not a really strong argument even if all of the above hasn't been so far. It's a way to make sure that you avoid and really costly end stage failures and it's a way to make sure that what you're building is actually useful for the people you're building it for. How do you get started. This may seem banal but you figure out what you're testing in here stakeholders are which is really hard to do sometimes and you want to be able to really make a list of the specifics of what you want to test. You're not just testing the drop down filters you want to see if people understand how to log out, you want to see if the color scheme works. And your stakeholders aren't just the people involved in the project and they're not just this kind of broad group called users they're the sub sets of your user group. If you're an academic institution maybe it's under grads and graduate students and faculty those are user groups and taking that knowledge you should make some perfect sew in as and some tasks to get a grip on your process. And for some of the really bad wrap because they can take a lot of time to make and very detailed. You're not aiming for something that's really robust you're not trying to get at every single subset of your population you're just trying to get at the needs and goals that are most relevant to your broader user group. The goal is goals this is a really good representation of lien UX perfect son a development by Jan in a Fraser and the way the goals will support your task development is by focusing in on the user rather than on what you want to get out of testing the design. And here is an example of kneel son Norman group an excellent resource they have an amazing website and really good tools. For the user goal look upgrades a really good task would be look up the results of your midterm exam you're taking the goal and reformatting it in a way that's personal for your participant potentially and very clearly tells them to think of this goal and pursue it. A poorer task is you want to see the results of your midterm exams, go to the website sign in and tell me where you would click to get your fript and the issue with this is that second part you're leading them you're focusing on the design rather than the user goal. So again if you look at the user goal and then the good task you can see how well it's formulated the task and the quality of tasks is really the thing that can make and break your design your study design and can make and break the data you get from it. So the practical needs are something that you can totally look up. There are a lot of resources out there for how to do gorilla usable test. You need a representative device or design something that makes sense where your design is going to end up and probably two facilitators a talker and a note taker you can do it with one if you record everything and take notes after the fact. But having two people, two sets of eyes and somebody who can focus in on the participant is really useful and you also because you're doing the recruiting on the spot and none of it beforehand you need a busy location that isn't boxed in and ispo dentially full of your population. Screen recording software is really great makes life easier but it's not super necessary. That said silver back is popular and inexpensive and I almost can't do this kind of presentation without mentioning it. It's really great. You want to keep your incentives really simple and very visible. Again this helps with the recruiting process. Avoid misleading signs because students substantial from you. And you want to be creative. Make the best use of the resources that you have. This is a genius method for testing mobile devices. Created by Jen downs when she was working for Mailchimp. Laptop hugging method. She is making use of the resources that she had on hand and the user is in a comfortable position and that's the most important part. As long as your user is in a comfortable natural position doing stuff the way they would normally do it you're going to get better results. During the test you want to aim for few participants and you want to test often. You don't need a lot to get it like 80% of the problem. You're trying to be, to have very broad strokes 3 to 4 tasks is going to be probably enough to get it mostly what you want to know and you want to be informative and friendly but definitely avoid verbal prompts. It's just the same as with the tasks if you're introducing prompts you're biassing your data and you're going to have have problems after the fact. An example of a verbal proment is maybe you should try log out after your participant has been floundering a long time. The better thing would be describe to me what you're expecting to see or happen right now. And after the test you want to really quickly summarize the key findings and make some action items and this can be a bullet point list you send to your designer and developer. It can be a face-to-face meeting just a conversation between you or it can be a documents, as detailed as you want it to be and I think what's really important it think about is what's most useful for them. Is a checklist most useful way to get that information across. Probably do it that way. And just maybe enough for smaller projects. But if you are doing more robust data recording and you do have a longer project some useful data points include or demographic info but keep it anonymous just broad strokes. The success of failure of tasks and items that were always into 3 M rngs misunderstood or misused. And now I'm going to talk a little bit about communicating value which was in this idea of value communication, it comes up in marketing and it's about essentially showing your investors, showing your funders and showing the people that are making the thing happen that what you are doing is somehow supporting their mission. In usability testing is something that ultimately answers questions for the people building a tool. So you want to always make your products user centric. Making it a spectator sport which is a really good way to think about it. Ways to make your products user centric are to be open in the way you do it. Share your documents it can be as simple as having a shared drive folder or a drop box folder. Blog about it in possible. This is a more expensive way of doing this kind of engagement. And doing many presentations, do them during regularly scheduled planning meetings. Only mention your major findings, be really brief and make sure that the process is interactive when people feel they can ask you about what you are doing and how. Ma I can it a Q and A. And then in addition to that there are other little things you can do especially if usable testing isn't commonly occurring in your institution and if you kind of have to sell it a little more. In the early project stages you might want to set up one-on-one meetings with key stakeholders that might mean you only talk to the project manager and designer at first and they then carry that message onto the people that they report to. And it also means you should try to attend planning meetings if possible and get your usability sessions on that project planning calendar. You don't have to do all of them. You can make it clear they will be can Delled if there's nothing to test but have that space reserved. In the midproject stages always check in with your designer and developer before scheduling a testing session. If there's nothing to test do not test that's a waist of resources. And also something that won't necessarily give you new information. Line and the final project stages record what you did but keep it really simple. Use the template this should be a page and this is a communication tool with which you can ask for feedback. And that's really important. Bringing the people who are involved in the brojt process into your process to make sure that the kind of testing you did actually supported what they were trying to get at. So to recap, libraries are user centered environments and gorilla usability testing is a low cost and high gain method for making sure things work and it's really relevant for an environment that's so user focused. Keep things really simple and be creative with your resources. Thinking about user goals makes for better tavengs and that's really important. Avoid prompts because even in your tasks and in your communication with users prompts are what's going to bias your data the most. And good communication results in higher perceived values so be user centric with your process be open as possible and invite people in. There are some resources to get started if you've never thought about this before and also to check in because OSS is cool coming out. Are there any questions, that's all I've got? This is like the bear minimum resources you should take a look at to get started. [Off mic] . I fought the gaunt let to get here. How do you handle the gorilla part you talk about busy places I'm thinking in an academic library students are actually there to do work. Right. So setting up near a doorway or a cafe, anything that's kinds of got a lot of walk through so people are already in motion, they're not necessarily sitting there to study and. It's really good in the library but testing in a student Union Pacific yes and no people who might be using library resources but not actually actively sitting in the space. And it's really, it's not as invasive as you would think. It's relatively personal communication one-on-one between you and the a participant it's not too loud and you don't do it for longer than a couple of hours at a time at the most. I was just curious if you had a example of a project where you saw a great benefit from this technique. Well, the JC 2 libraries we recently did their room reservation system, built a new interface to stit on top of it. And users have been in the process of user testing that system, it's still kind of in the early stages but users have been really receptive and complimenting is so much just because it's easier for them to do something that they absolutely have to do. I don't know if that really answered your question. I guess the really -- our libraries have study rooms that are booked pretty much 24/7. Anything that made the students life easier can make that booking happen was much appreciated. [Clapping]. Thank you Kka. My pleasure to announce Kate Lynch arc successability coding and user testing for libraries. Hello everybody thank you for having me today I'm going to talk about advanced accessibility coding and testing for libraries. To begin definition of web accessibility. Web accessibility according to Tim Berners-Lee means people with disabilities can use the web. This means more generally content and controls are perceivable operable and understandable by disabled users and rebust enough enough in the changing technologies. It operates under two assumptions at least for the next 15 minutes that equivalent ability to access information and interact with websites essentially a fair and equal user experience is important and to truly work toward an experience at would be we be based accessibility measures are acquired. I am going out an a limb I'm not at that trying to talk down and we're turning off the echo chamber in hopes of good talks on accessibility. First things first best practices for coding in five minutes. The fact of the matter is a large percentage of accessibility measures are quite frankly addressed at some level by the basics the low hanging fruit of web accessibility. The key is to remember to prioritize these fundamentals and execute them flawlessly. Flawless can be a loaded term but consider best practices for we be accessibility go hand in hand for we be practices in general. Name check a few of thesis basics in the interest of time if you're unfamiliar and would like to know there are links at the end of the slides and quite a few who are into web accessibility so I encourage you to seek us out. Fundamentals like provide short textual description using bulleted or numbers list and list worthy content using properly ordered head goes these are the big 3 building blocks for properly web accessible kenlt. If you're working for fix a site that seems like an accessibility disaster start with these three things and you'll build a foundation. Other fundamentals splitting walls keeping color contrast in mind when designing and never using color alone as an indicator providing alternatives to rich media like equivalent tech or if possible captions or transcription and providing skip links to allow keyboard switch based and screen reader user to skip over repetitive content directly to the main content on a page. All of these things and just generally well formatted HTML, separating content from presentation are rewarding techniques for developers and users alike. It's like mathematics or many or forms of programming as you work as a program that's when they start to convey information beautifully. Similarly with HTML 5 sem man takes. They are to identify user agents to what they are. It's not semantics standard HTML there's not so much the only thing we're semantic cally communicating is where the browsers are. If we're able if we I am mroem semantic HTML the browser a sibl to tell so much about the content as well. Here is a blog post section here is a header and title and shorted description here article, title, figure related to that harl and here is a caption for that figure. Semantic elements are easily readable source and browse certifies include assumptions about content that just works for assistive user agents when HTML is employed as it identifies what's what from the browser to source. ARSE. Accessible rich Internet applications. It costs of roles and states and properties. ARSE serves same function as sem mat particulars. They tell each elements on the page. There are ar Rhea rolls where HTML 5 are not supported and can tell more information about a particular elements that is relevant to assistive user agents so for instance in example here the role alert treat content as if it's an alert the same as visual indicators convey this information to sited users. It's prioritized for communication to the assist tifr device and the role status here indicates an element is content the user should be made aware of. The ARES a accessibility in Javascript to you can do what you want to convey additional accessibility and finally two of the most powerful AR sechlt. A life how to handle relaying information to the user when an element is updated. If the light it waits until there's a lull in action this is the standard practice when you're using ARES life. It will interrupt immediately this is for an emergency like you have an insecure connection or where immediate feedback is needed reaped repeatedly rying too set a new pass ward. Without ARES life set it assumes the element is static and should be treated like one any class to that content is not reported to the user. This makes ARES life for best practices for accessibility of dynamic content without it a disabled user has no idea the content is changing. Also AREA relevant. This indicates what is in the life trigger reporting. Addition of elements or text cal changes there in. In this example we have our AREA life and relevant set to additions and deletions. We can assume the behavior in the chat queue as the new user hops into the queue the new list is element is added to thedom. It is the only information that would then be conveyed about this piece of content regardless of it's source order on the page to the user. When prioritizing accessibility tasks create a plan for what to address as soon as possible if I asking yourself what must be done to make this site accessibility perceivable and understandable and interfaces operable so the same actions are achievable by all user after that what should be done to make the site truly accessible like linking to a textual description is good practices for basic accessibility but we need captions. And finally after that what if anything else can be done to enhance the accessibility. Are there deck ra active images that really should be in the CSS do you have users for a high die lay toggle would be in the phase. In the can phase should be enhancing existing usable for users. Accessibility testing also in five minutes hopefully. We will begin with automated tegsing with value day terse. Accessibility value ta did he remembers you can plug in some code or URL and checks for various accessibility barriers. Think of the fundamentals I aed up front. Wave and A checker are comprehensive for use in this measure. Compliance share is paid and commonly used as a reporting mechanism where there are policies in place that require reporting and typically value day terse will flag definitely an error like image without an attribute signal what might be a problem I can tell what the foreground of this color is but I don't know the background so you may have a color contrast issue and they will flag stuff that is exceptionally good like you're using AREA roles sweet. There is testing suites available for developers I've got using cab by para-and pie thon accessibility tools hooks into the inspect element in chrome to identify accessibility requirements included or required in the environment. Value day terse are good up to a point they identify identify a lot of false positives and miss things impossible to a machine to test that need the human beings point of view. Resist teaching to the test with your code. As much as an application might need a certain percentage to be release to the public a human being needs to understand your content and be able to use your website. Use a value dayter for what you know you can delegate is there alternate text on it properly formatted HTML but resist coding to achieve the test results you can want this can result in crazy code that extracts and leaves everybody perplexed I don't know who this is for, thank you, question mark. So really supplement aud mated work with human self-guided testing. Make a list of everything that users must be able to perceive understand and do on a given test site. And this includes test results of interactions like form submissions as well and then as with any good testing go ahead and try to break stuff. I like to unplug the mouse and try to find a keyboard user experience is. One of my favorite tricks is to put a mouse in and flip it 180 degrees. This can perfect call out where interfaces can be problem mat particulars with fine motor skills disabilities. Ensure your content and presentation are truly separated by unlinking style sheets and evaluating what you can infer and if the source order makes linear sense. Turn off your monitor to keep your visual assumptions in check and use your screen reader in the navigation checks how a screen user might understand a user site. Really challenge the way you interact with the site and be mindful how much it takes to accomplish your tavengs in testing and what you can do to cut down on that. Try to make the experience not just do-able but in consideration of all users. You wouldn't consider a user test where it clicks frustrated and magically comes across the element to be a success in the same holds true for web accessibility tests if a disabled user encounter more difficulty in completing a task you would consider acceptable usable test it needs to be worked on. As my final point addressing advocacy working with vendors to accessibility and improvement can fall down a black hole but working with developers in our community doesn't have to be that hard. Accessibility education materials and distribution of knowledge is much better than it was five years ago but it is by no means distributed enough that we can astum that all developers should already know this stuff. Often people learn about it through venues such as this one. As an accessibility advocate use teachable moments rather than blame. Accessibility issues aren't done out of carelessness and real you have a gift to smar with the community. Embrace continuous improvement and never stop educating yourselves in helping others craft better user experiences for all [clapping] I actualry have about two minutes left if if anybody has any questions right now. Okay thank you the question is was there any combined document that was library specific technology vendors and what their level of accessibility is what they're working on right now. There might be a few resources out there. There's not one very distributed one. It would be interesting to put one together because a lot of institutions that had accessibility policies in place often have internal documentation to that measure and many of these vendors if they're addressing accessibility have something called a V pat their on file accessibility stamps and what, if anything, they're doing toward it. It would be pretty easy to put one together. Let's do it. Two part question. [Off mic] Do you find better accessibility for communities helps the over all interface off of. I didn't get the last part helps to be the over all. Over all usability even for -- Okay so the first part of the question was what's harder learning to do this stuff or actually doing this stuff and then the second part is as you're doing this stuff do you find that it's improving not just the experience for disabled users but for all users. So it's been my experience that it's pretty easy to learn a lot of the basics of web accessibility from the fundamentals that I name checked earlier. A lot of things that will be familiar to folks oh, I didn't know that was good for accessibility I'm already doing that that's cool or it's easy to learn but what gets harder is when you get into the edge cases where here we have a promotional video that you know it doesn't have any audio but how do we make this equivalent information without sacrificing something. And then the short answer to your second part of your question is accessibility questions make the user experience better for everybody. [Clapping]. Up next is Allison Jay O'Dell and Steven Duckworth with the fancy finding aid make over your collections with responsive design RSA circulation management and visual content. Hello my name a Steve Duckworth this is Allison. High Allison. We're both from the university of Florida I'm the processing archivist and you're the metadata. I'm the librarian. So I'm just going to give a quick introduction and context to what the world of finding aids and archival process and Allison will talk to the fun stuff. So in just doing a quick research into basically finding usage I found this quote from Randall Jimerson from 1989 who he's kind of like a big archival pedestrian did I gog in 89 talking about how we really need to bring ar kifl description to where users or how users are looking for information because they will tend to go with whatever is simplest to find and then looking, we obviously are looking at our own finding aids with are more in the static HTML stage. 1989 brought about the very first apple laptop very popular hand held gaming with the Nintendo game boy and this was the web so I just wanted to put that in context a long time ago. So the things that we're looking at that can help our end users access or collections better are things like circulation management, GIS information everyone likes to look at maps and analyze dataed in that way and responsive design especially for mobile devices and we can see here is the first kind of mainly white picture is what our finding aids look like on a mobile device currently and the darker pictures what Allison will be speaking about in just a few minutes. Also Google blts, be able to search more, search our collections in the way that people are used to searching for online content and so we see both here a Google search and also a search of fast access terminology. Researching are looking for more autonomy so they have less intervention from archivists and they can find information on their own without having us search. And also looking for results that are relevant to their own searching. And then just a slight different version here archivists in the old days tended to describe what something was rather than what something was about so you would have descriptions that are like, you know, these are letters, these are papers, these are manuscripts and drafts rather than saying subjects so we were entering a time where archivists are more success discussing what things are about and why people would use those records in their research and this kind of information can lead to really great access terms which can then be used for a lot of other technology and access. Also digital media everyone loves to see pictures and all of that plus we have digital records digitized records which I call reborn. So we want to integrate those into the finding aids so they're easier to access as well. And while retaining the context of how those records were created and original order all of that good stuff. And like I said visuals are really great for all sorts of social media and everything else. And I will hand this over to Allison. So thank you Steve for the three minute introduction to everything you ever might have wanted to know about archives and archival findings I hope that helps. So I'm going to talk about our attempts to update our findings basically for the contemporary interwebs and all that entails so on the screen is just a shot of what our current finding aids look like. The style sheet for this was written in 2004 its very text heavy, very bland. It looks dated. And I imagine that a lot of people in this room have collections with finding aids that also look similarly dated so I hope the story resonates and you walk away with action items something you want to talk about. So to append Steve's introduction I want to add you know quick note about finding aids they are typically encoded with EAD and transformed to MTML documents to the web. Archivists tend to be familiar with XSLT and query and that's going to become really important as I'm talking to keep that in mind. So as I mentioned these were written in 2004 since then both user expectations and the environments in which we work have changed drastically. A lot of our content has been digitized. Patrons are bringing their cell phones and tablets into the data room. Link data became a thing. Recently we've seen the publication of MTM L5 and all of these changes present opportunities to do more cooler more awesome stuff with our findings. So that's not a novel realization and other folks are working on this. Do you care Steuben steen published a finding aid a couple of years ago and doing a lot of stuff were displaying hierarchies and archival of metadata out there. Really intensive development projects I've got like an hour to make my finding look more snazzy so I'm like what can I do what tools combis that we can use around archives. So I was going to demo this but we don't have enough time. As a quick and dirty solution taking an existing CSM assembly and applying it to our finding and creating an experience that for the user is fun it's responsive, it moves, it's engaging, it takes a lot of images and puts them in there. So not only is this finding just an inventory of the stuff that you've got in the collection, but it's really a selling point for the showcase and you know this example is awesome we've got this or nell hur stin papers if you're looking at a white screen with a bunch of text so as we want to let people know it's cool and it you know the front end desefshs to be just as cool. So visual content. My institution was particularly bleeding edge with our dig at thisization projects we've got a lot a lot a lot of digital images so how do we incorporate those with the finding aid. Okay so image gallon reese are a thing that are skadz and skadz of them out there to be repurposed and I had this idea of wanting something, I want tin did he remember for archives you can sit there and scroll scroll scroll like cool pictures cool pictures whenever you're bored, maybe as a marketing opportunity that you would then share with your friends check out this new staff over at the library. So I wanted to find just an image gallery that kind of functionality and arrived at Hoyt box as really very simple. Basically it takes any on page images as links and as a swipe box class so if you had a lot of forethought and had been including links to digitized content in your finding all along you could just add that, good to go and it will launch the image gallery. If you had not been doing that you don't have sufficient identifiers to associate digitized content with what our collection box folder came from, what we did was instead just grab a bunch of images with links to the metadata from the digital collections and kind of create an image gallery at the collection levels. Here is a sampling of stuff from this collection that's been digitized that you can play through. That's kind of fun. I'll just actually for reference go back a few pages. If you're the orange arrow that's the link kirntly to stuff that's bfrn digitized so if instead I can launch this image gallery and scroll through and click through. That's a much better selling point for the collection right. So we're getting folks excited about our archival collections and so now they probably want to check out the stuff right so what are we going to do there? So basically we want some circulation management action. Want folks to be able to browse through the container list, choose something, edit two lists and send it over to the car kifs and say I want to consult these things. Basically just need a shopping cart and simple cart is a free simple job tool that integrates with existing HTML pages so that again work really well with our finding aids. I'm going to go a little ethics here. The way that I implemented this tool is basically folks will add stuff to a cart and that cart data is stored in their cookies and then when they go to check out and send the cart data all this he do is enter their name their e-mail address and what day they might want to stop by the archives or every third visit. So thinking that their e-mail address isn't a relatively unique identifier I use my same e-mail address every time I come to do some research, then you can say oh, okay well this e-mail address how many days did they come on and blah, blah, blah. You can calculate all of that information about that person but it can be totally anonymous. I can set up fake researchers at hot mail and I'm not registers with the system, giving the archives a lot of personal information about myself but we've still kind of balanced that need for identity versus the need for privacy. You guys are with me, okay. So what else do I want to say about this? So keeping in mind that archivists are comfortable working with HTML technology I wanted to create a solution simple to implement and your average archivists could run with and extend. So basically the cart information has XM coding and when the person goes to check out we just wrap their information their e-mail research date and send that over as an e-mail to the archives like hey so and son wants to come for a visit and then append an XML file with that encoded information. So basically what you ecentury get is this very long XML file about all of the research visits that people made what they consulted, and then doing any kind of reporting or like printing cost lists or whatever that's all you can do that all with XSLT which is kind of liberating and to me that was just a really fun aspect of this project. Skipping the database entirely. Okay. So while we're in there, snazzying up our aids this is kind of a freebie. Data that can be feeds into the knowledge graph and other nift at this applications. I have a minute and a half to say this is just the beginning so we are really excited about front end development for finding AIDS. Everything that I've talked about is kind of like the quick and dirty simple solution you know just front end technologies XSLT, HTML rngs tiny bit of PH P was used in anything I've ever talked about, yeah front end development for finding aids let's get on that. Steve and I are putting together a grant application if you want to be on that talk to you we'll be eating lunch and hanging out and being excited about the fancy finding. Thank you [clapping]. All right. Thank you. We are almost ready for lunch. We have a couple of quick announcements. The welcome wagon around okay cool. One last quick thing and then I have a couple of announcements and then we can all eat. Here we go. Hi everybody. So I'm Megan. My boyian and we're from the welcome wagon here and for those of you who are new we wanted to give you a really quick run through of how you might want to meet people and get involved and have fun at this years Code4Lib and we realize we're standing between you and lunch so we're going to be really fast. Welcome. You may be wondering why you are here at this Code4Lib especially if you're thinking well I don't know like I'm not a coder maybe I don't self-identify as a programmer if you see anything you relate to on this screen then you are one of us and you should be here and you are in the right place. Because these are the things that we do and this is the kind of the tribe. As you are noticing this can get a little intense so it's going to be okay. So in addition to the presentations that are happening in front of you we also as was mentioned earlier have some back channels. If you are looking at those and thinking I don't really know what slack is or I've never heard of IRC there are people who can help you with that in particular Ian and lots of other people do people have old-timer ribbons this year. No.But if you especially if you're on Twitter, if you fill out something like hey does know how to help me with IRC somebody can help you with that. There's options and also some mention on the Code4Lib website slash Wiki how to get started. Additionally we do have in in jokes but not all of them are super complicated some of them are self-explanatory some of them are sort of weird but if you are curious whether something is a joke you can absolutely just ask and some of them aren't they don't actually make -- I'm going to go out a limb and may not make a ton of sense and they're funny because we keep using them. It's been explained to me I still don't understand it that might be okay. And there are many, many different interests that are represented in the Code4Lib so if you think that you are not having one of the shared interests you will be completely wrong so whether it will be a meet recollect, gaming if it is beer drinking, we have coffee addicts here for certain, joggers, bakers feel free to shout out and find other people who share same interests and food is of course a big thing so if you are you know interested in food and you want to find someone who shares those kind of interests ice cream works very well. And we have obviously WIKI page and many should be familiar with this if you are not please take a look at it. I had a newcomer yesterday at the dinner who asked what it takes to create a code in the world it just takes organizing some people and put up a WIKI and you are official. So have a great time at Code4Lib conference and we look forward to seeing you around if you have any questions find about the case can I Megan or me, thank you very much [clapping]. Okay so that's it. We're breaking for l[Lunch. Lightning talk sign ups for today are posted now over by the registration desk as are ideas for break out sessions. Anything else important here? That's it. Yeah everyone is hungry go. If the people that are involved in the panel could meet me right before the give aways start after lunch up front we should go over some last minute logistics. Mat Miller if you're around I would like to talk to you. Good morning. We're about to get started so if everyone could take their seats we can get going. Before I start, could everyone, actually there is a lot of seats available, there is no one standing at the back. To get more people I'm going to ask you to raise your hands so people can find a seat, but skip that for now. Welcome to day two for Code4lib, I'm going to start off by announcing duty officers for the morning, Julie and Whitney, can be found on the Web site 2016code4lib, duty/officers, if you need to see what they look like. Secondly, I really want to thank our silver level sponsors this morning, we had a few of those. They are temple University Library, Princeton University Library, Kalliffa, Oregon state University Library and Press, and University of Maryland college of Information Studies, so can I get a round of apollution for our silver sponsors. Okay. And next I want to thank our childcare sponsors, which, that services sponsored by Equinox and by, Kuricamp, thank you very much for that service, it would not of been possible without you and it helped out, it helped out quite a few, we had I think six attendees use it and it was incredibly useful and they wouldn't be here without you so thanks for that, appreciate it. [APPLAUSE] Okay. So the next thing I want to say is that, you know, we have had a bid for 2017 Code4lib, a bid from Chatanooga, super exciting, but they are still looking for partners to work with them, particularly they are looking for a fiscal agent, fiscal host, which is absolutely essential roll, let me tell you, from my point of you view as 2016 organizer, you can't go aboard without it, can't sign contracts, can't move forward. If your organization is somewhat local or interested in being part of 2017 Code4lib and can commit something like that, come by Mary and have that conversation. So if we want to do this again next year, somebody step up. So all right, thank you. [APPLAUSE] We have some space for post-conference events Thursday afternoon. Right now we have two rooms, we're trying to negotiate with the hotel to get one or two more spaces, one expression interest for post-conference bridge of feather, from the Fedora community. They will be meeting probably, I don't know 12:30 or 1 on Thursday afternoon in one of the hotel spaces, we're going to short out exactly where. If you are interested in having a post-conference session, bird and feather group, come find organizer, we can sort out a room for you. We do have some limited space, come talk to us and we can try to find space are to you. So that is, oh yeah the Fedora 4 group is, Joshua and Andrew Wood, if you are interested in Fedora 4 come find them and talk to them, let them know you are interested. Okay. So we have another social event tonight, which is what we're calling the play and share. It's, it is going to be in the hotel, in the same room where the Hamilton room most of the lunch buffets were set up, it is a place to congregate. A space for playing board games, people bringing instruments and playing music and also a place to share, to share things that you want to bring. I know a lot of attendees like to bring local beers and craft sew data sourceses and those kinds of things. You can bring those tonight and share those. There wills be a lot of baked goods like a lot of canoli from a great bakery I highly recommend, so if you have anything else you want to bring and share and play and hang out, that will be happening tonight from 6 to 9:30. So yeah. Tonight. Now would like to announce name of scholarship winners, we have nine scholarship winners who attended this year. So I would just like to on who nor them all because this is an absolutely essential part of the work we do in this community of making it inclusive and more diverse and one step we take. So I'm really glad we are able to offer, bring nine people in and have them make it, even if it would of been difficult for them to be here. To the winners, Allison Blain, Eleanor, Ariade, Maria, Helen, Tara, Patricia and Lucy [Name?] so those are our nine scholarship winners, so can we get a round of applause for that. And a big thank you to the scholarship committee would worked very hard to make that possible. Just a few other announcements, we will have mic runners for questions today, if you have a question you can raise your hand and a mic runner will come find you and bring you the microphone, rather than make you sneak out through all the Chairs. I think that's all I have for this morning. So let's get started with the program. So our first speaker this morning are going to be Demian Katz and Matthew Short, with their talked Linked open Dime Novels or 19th Century Fiction and 21st Century Data. [APPLAUSE] My name is Matt Short, meta data librarian at northern Illinois University and Demian Katz, a lot of you probably know through his work, today we will talk about linked open Dime Novels, published 19th century popular fiction as linked data, some being used in NIU Dive Novel Collection, before camian gets into the detail of application, I will provide background including catalogging and bibliography and introduce the model we are currently using. Dime Novels were best sellers costing between 5 to 10 cents each, especially popular between working class people and children who couldn't often afford hard back fiction. While primarily meant to entertain, Dime Novels dealt with issues of race, gender and class, feeling towards the end of the century. Most early Americans genre fiction also began on Dime Novels, westerners, detective stories, early sciencification. Today a special interest to historians and people studying history of book and popular fiction. Dime Novel publishers not-for-profit, because it was much cheaper to ship through the mail they would often disguise them by issuing them in a series, sometimes including filler backup content. Sometimes the backup content might be a short story but could be a novel that was over a fan of several issues. They would also frequently reprint stories, whether or not they own the copyright, which was often more cheap than paying author to write a novel. Might be several editions of any given story, different formats appearing over span of decades. Sometimes published complete in one problem but also maybe a backup feature in different series. Title author and text itself would change to removing subtitle or an entire chapter or entire section of a book. The publisher also wanted to control the author, which is one of the reasons so many Dime Novels are attributed to synonyms, these house names shared by several people but the name was owned by publisher who would try to protect it like it was a trademark, between editions of a story, either to disguise acts of plagiarism or trick readers into buying repackaged stories. For almost a century Dime Novels were only catalogged at series level if even cataloged at all. Most users want more of an inventory that the library owns they want to know if they contain how they were written and who wrote them. Today we dialogue them and monographic stories, many are still missing from our records. Partly due to limitations of marked format but mostly has to do with a lack of time, since unpacking and tracing all these relationships can be complicated. Scholars and collectors like Albert Johannsen step into the gap by compiling Dime Novel Bibliographies, written over decades and consist of hundreds of thousands of pages with detailed relationships between stories, editions, authors and series. Both of two major bibliographies are currently available online, in various formats that make it difficult to combine the data or reuse that data in a different context. In late 90 NIU, converted to HTML, a project to build a relational database using the data from the LeBlanc published listing as a foundation. In addition to those NIU there are substantial Dime Novel at University of South Florida, Stanford University and bowling green, consists of thousands of novels with hundreds of thousands of pages. Because so many different manifestations of any given story unlikely any collection is complete or will be complete. Multiple editions of a story exist then go out to the various digital sources to search for additional copies, this encourages close reading of the Dime Novel, makes it so difficult to place any story within larger bibliography context. So obviously all these digital collections would benefit from sharing data, both between relationships as well as Dime Novel holdings. So in lightning talk 2013 camian introduced the work he had been doing on dimenovel.org, some way to publish the database RDF. At the same time I had been working on ways to model Johannsen bibliography, with plans to use it with a Dive Novel Collection I just started working on. That's me in the audience listening to camian at 2013. Not long after we started to work out with publishing the bibliography, a conversation on going over the better part of two years, mostly during our lunch breaks and commutes. We started by writing an ontology, which has since gone through multiple, especially through early stages. Four local classes, including creative works, editions, series and copies, all which intended to align better with Dime Novel data, most properties already unconstrainedded RDA direct links, so instead of an agent use local classes with domains and ranges of all of our property, all of our classes. Importing it involves relevant properties to XML and then intended to make writing code and query somewhat easier. Partial, to identities. In the case of editions we wanted to be able to query for every edition of the same story whether it appeared as a lead story or backup story, we can't do it right now with digital collections this data doesn't exist or might exist but only in form of uncontrolled contextual notes. Each can be assigned to one or more editions so the property HasRealization which title, at because, so on. It can be used for container of other through property of container of, makes it possible to trace changes to context of issue as reprinted which would sometimes happen if publisher sues for copyright infringement or remove from back catalog. This is especially useful in the case of these backup stories, allowing us to treat each part as constituent work. Finally, the model allows us to trace, make a distinction between the name that appears on the resource and the person actually behind that name. Each edition is assigned a name through property has credit while each work is assigned person through property author, name is then related to person through focus, the name is a concept. This brings together all creative works by person or all editions by name, which is helpful when a pick CreativeWork is attributed to multiple names. Now I can turn things over to camian to talk about implementing the novel in terms of Dime Novel.org. Demian: Thank you, before I start talking about implementation a couple of things, I know you probably aren't chiing to build own Dime Novel database, this isn't specifically about what we did, there is a talk about linked data at high level, we want to show you with limited time and resources you can do practical things with these technologies that give you real benefits, hopefully this has application in all sorts of domains. I will throw away some linked data talks, I assume most of you have seen high-level talks about linked data and have heard it before. If you haven't, feel free to stop me after the talk and I will try to explain it as best as I can, I understood it much better after getting my hands in it. This sort of project is a great learning learning person. What did we have for Dime Novels.org, built on my pet project that I've been working on far too long, which is currently in PHP my SQL framework application. This is built in such a way that's very friendly for RDF, it has pretty URLs representing things and it has a relational database that really these things together. Probably if you have any sort of project lying around that the database that describes things and the way they are related to one another, it looks something like this. And this is a good first step towards exporting data as RDF. So how do I get RDF incorporated into the software? First of all I need a way for my website that's presenting the Dime Novel information, to feed it to the reader as linked data. Certainly one approach I could have taken would of been to use RDFA to embed semantics into the existing presentation, but that doesn't really appeal to me. First of all, because I like to sort of keep my data and my presentation separate from one another, also because there are tricky problems in the linked data space of differentiating between an object in the real world and the data representation of it. And if you have one URL that is both the presentation and the data, it gets harder to make statements about these different things. So what I ended up doing was using the content negotiation strategy where my existing URLs use HTTP content negotiation to find out whether the client wants HTML view or RDF data and redirects to another URL accordingly so this gives us separate URLs for different representations but also a single conceptual URL for the real-world object. If you don't know anything about HTTP content negotiation, let me step back a second and say, as Web developers it is really easy to do good Web development and not know a lot about underlining technologies of the Web. I strongly encourage you, read up on HTTP, learn some low-level fundamentals, this will help you tremendously in on going evolution of Web technologies for things like restful design and linked data. I say from experience, I got away with not knowing a lot for a long time, but anyway, Preaching off. So anyway, once I had content negotiation in place then presenting RDF through interface basically just became a matter of implementing another view in my model view controller architecture. So take the same data that you are using to render things for the user, instead render RDF, I used easy RDF library to do this, whatever language you are doing probably has descent RDF library you could use. Then of course I had to make a few adjustments to underlining data structure to reflect the fact now that some of my local concepts have equivalent concepts out in the broader world. So you need a way to be able to say this person corresponds with this URI at the Library of Congress and so on. Ultimately it wasn't a whole lot of work, content negotiation, alternate views, little extra data. Once this is done, what benefits do we reap? From my perspective at Dime Novels.org, I want to steel his data, this catalog Dime Novels, I don't have all the Dime Novels in my database and they are using MODS, this is not linked data technology but MODS has hacked on but useful feature that you can associate URIs with elements of the meta data. So even with just that one little tiny hint of linked data in there t made it easier for me to write this, automatically add things to the database, the shared global identifiers provided by URI giving me much greater confidence the things I'm generating are accurate as opposed to having to do string matching and guessing and scoring and hoping. So this process got hundreds of new records into my database, it also proved that redundancy is not a bad thing, because my database already contained many entries NIU had catalogged and errors on both sides neither of us would of noticed if we hadn't tried to reconcile these two sets. Don't feel too bad if you duplicate work someone else is doing. Once I got my database populated and stolen all the data from NIU, I want to give something back. As Matt said, what NIU wanted was the ability to express relationships that aren't there in the MODS but that can be easy queried with proper RDF data set. So I now have um my website that can present all of this lovely RDF data, we need to be able to query it. So NIU came up with the query, but we had to provide the technology to run the query. So our first attempt was to host a SPARQL end point at dimenovels.org, first challenge is getting RDF from all individual pages of dimenovels.org into a triple storage so it could be queried. This proved to be more difficult than I would of hoped with existing calling tools, because they really seem to want to just suck up the entire Web and I only wanted to harvest data that lived at dimenovels.org. After feeling there should be a tool that could do this for me but not being able to find it I gave up and wrote one, named after village lane called Murpoint, you can find it on github, I used it to suck up the data, put it into instance, Matt implemented his end to do the queries, it worked really nicely but now we had the problem if my SPARQL end point goes off line for any reason, NIU website dies, we can't have that. So that brought us around to attempt number two, the same thing only not at dimenovels.org. But a challenge to this is that Matt has some limited resources in terms of not being able to directly install complicated applications on servers, so he found the nice compromised solution of Arc2, my SQL based RDF library and triple store, disadvantage of not being currently maintained but it works, and that counts for a lot. So he installed that. He got a copy of my Murpoint RDF dump, he loaded it in there and it just worked. One of the beauties of RDF the I can fundamental model of linked data is really simple and thus highly portable. So to a much greater extent than something like SQL you can take the data, move it around and it continues to work. This was a really nice example of that. And so just change to end point URL, kept going. And that is actually all we have. There's a few minutes left for questions. There also lots of links here if you want to look at our code, look at some Dime Novels. There you go. [laughter] [APPLAUSE] If I wanted, right now I need an example of some linked data related to resource, which one of these URLs could I go to, to get it and how would I formulate my query? If you want to look at an example go to dimenovels.org and do a search to look up a person or title or series and in the URL that you end up on, change the word show to RDF in all caps and you'll see a turtle dump of the RDF representation of that resource. And, you know, right now this is a Work in Progress, so the RDF dump is just enough data to meet Matt's needs, not a complete representation, of course we're hoping to work towards that so eventually the whole data set can just be dumped and shared and not tied to the other underlining infrastructure. Audience Member: Thank you. Demian: Well it seems that's it, I will donate our one minute, oh one more, sorry -- Audience Member: So a great presentation, when I see a chart that has work on it, I start looking for manifestations, very well considered data models for describing stuff like that, I notice you use a slightly specialized maybe way of understanding that stuff. So could you talk about the ontology developed in terms of how you were reconsidering existing work and describing the publishing history of fictional works? Demian: Well there is sort of a long history behind our data model because it is somewhat tied to the system I've been using for bibliography almost 20 years now, more of a two-level model where you have the abstract idea of the work and concrete manifestations and there are certainly other models that add more layers of granulator than that. The problem I found in conceptualizing those is figuring out exactly where the lines are drawn between say a work and an expression. So we sort of settled on the really clear-cut, this is conceptual, this is tangible with the possibility that we might consider additional layers of conceptual granulator in the future, right now sort of a pragmatic decision to keep using the existing model that works for us while mapping it into a somewhat more shareable space. But I'm certainly interested in discussing this if anyone cares to later on in the day. [laughter] all right thank you everyone. [APPLAUSE] Next one we have Marya Sawaf, beyond key word query search expansions based on DBpedia. Marya: How do I get this on the screen here? I just play the slide show? Okay. Perfect. Okay. So hi my name is Marya, today I will be presenting to you a tool I've been working on which allows you to search creatively and go beyond the keyword. What is this all about? I'm sure many of you heard about semantic search, I will talk about that briefly and I will show you how I took that one step further to do creative search or better known as librarians, serendipitous, I will talk about the code I've written and touch upon a bit of user experience and UI for your creative application. So what is semantic search? It essentially takes the query and map it is out to a bunch of related words in order, so the search is performed on those related words as well and you retrieve a larger set of relevant results. So taking the example of school, usually people who implement this will at least start with synonyms, so you have school map out to academic institution, educational organizations, even beyond that semantic key words so school mapped out to school teachers, school Districts, school bus, students, exams. So those are semantically related keywords. Where do these come from? You can use a lot of controlled vocabularies, people do that. The Library of Congress subject headings work as well, I know a lot of you have access to those, I use DBpedia, just a note that you can use something else. So if approximate we look at DBpedia, you'll see at the, this is the page for schools and at the bottom you have the Scott broader, so this is query again using SPARQL on a triple store, so this is really used to represent hierarchy in the way that you are used to for like just term for categories and subcategories. So you see the synonyms there. Also in the same page for school, if you scroll down broader of then you see a bunch of other words, school bus and just kind of take note of how many words there are there because that's going to be relevant later. Then if you look at the Wiki page reader, it is kind of like how maybe misspelled words like school SKOL or in other languages. I don't know, there was quite a few people in the semantic search on Monday, the prime example used there was kind of how do we map out dogcatcher to animal control officer and someone in the audience said okay where do I get a data set for that. So just kind of linking that together in the Wiki page reader direct for animal control officer you do have dog catcher. So that's semantic search and what is creative search, that's kind of taking it one step further. So synonyms, you have semantic search on external level you have creative search. So the limit with semantic search kind of restrict you to the same knowledge place as original query, creative query meant to go beyond that and allow ideas to be shared across disciplines, kind of screen shot these entries, it is linked to imaging, which is linked to image processing, linked to computer vision. And I don't know, many of you know about computer vision, but there are a lot of ideas that cab shared with photography and that are indeed shared in a lot of photography and image processing software. But I guess so the prime issue, when you are going to do it the way I'm doing, which is basically relating words that are related to the words that are related to the key words, kind of going a little far, you are dealing with exponential data set and prime, number one problem is going to be filtering. And so the way I do it, I'm going to talk about Google end grams later I use word frequencies to basically take out words and expressions that are commonly used in the English language and filter out those that are not so used and not so common. I'm going to talk about this as well, there is a few quirks I have to deal with in my code and automation, I will talk about how I automated the process of getting related key words. That's ugly okay, even if you think this creative thing is a little crazy and on the fringe, you can take the same code and tweak it for different applications. So I'm going to show creative book search and creative project search in my case studies, for cos-disciplinary, academic, you have examples like photography and computer vision. For children I'm also going to show that later, for non-native speakers I kind of -- my mom is non-native English speaker, I kind of looked through Google history and for example she wanted, how to solve a child computer game addiction. I think one of her friends son's was addicted to video games. But what she really meant I think is teenager and adolescent but there really isn't a term for that in her native language, everyone is just a child until they get married and have their own children. So it would of mapped it out to there and computer games always okay, the real term in most of the articles that are relevant user video game and DBpedia redirects video game and relates that. So non-native speakers. And then for everyone kind of lazy search, words you don't know, I know when I first started this topic, I had no idea librarians had thing called Serendipity, there is creative searchcreativity, you kind of get this at the bottom there along with other relevant stuff that I honestly should of, should of searchedded but didn't think to originally when I was looking at the topic. So moving on to case study, so I'm going to talk about creative book search. The idea is kind of just like a a regular book search like you would on Amazon books or own catalog. But bringing out books related to what you are looking for, also allowing that kind of cross-disciplinary learning I was talking about. If you run it for fashion design, if you take that as an example, so if you use my API, I would not recommend you all do this righted now, single-threaded and it will break. But you can do it later. So I show you distance one, distance two, distance three and you can see how the data set grows larger and larger and larger as you go further from the original query. But although you can't see most of the words here, a lot of them are pretty equal and that they are, you can learn from them and some are relevant, which I will talk about. So this is kind of how I imagined it would look like if you put into a real application. Here I'm pulling out overdrive books for the key word, that comes out in my list. So you have fashion design, then perfume, then for body art and photography. Then, yeah, so among this -- irrelevant searches you have surgery, it came from sewing and stitches, yeah. That's one place you do not want ideas mixing. But I guess it can, maybe it did once, I don't know. Yeah and for the UI, I do recommend for some examples, for some of the creative I guess words that come out to use bread crumbs to kind of follow the trail of the idea on, it is like following, you know, kind of going back to photography image classes in computer vision. So just have bread crumbs to explain where is this coming from or to have at least a heading on the top for fashion you had a luxury bands and luxury vehicles come up. You know, kind of like an idea exchanged between these two fields. And the other one, which I don't talk about, I did use this at the local library for like kind of a maker project search. I like to use examples of camel back because for real the camel back inventor was a paramedic really interested in cycling, when it came to fix his water portability problem, he thought why don't I use this IV looking thing, indeed if you follow DBpedia to dehydration to food replacement or IV you kind of get that idea. The kids had a lot of fun with it and did come up with some kooky ideas. I guess I want to take that example for more relevant to you I guess, is the UXUI part of it. In creative book search all of the kind of relationships and all the related word mapping happened in the background, so it was a query expansion, query expansions happen in an automated way. For this I had it kind of related key words on the side bar and the first problem I encountered again is too many key words and needing to filter that. And you kind of get that like if you take a book search, world cat, look at related topics, like a ton, then you get library things, I think Amazon hits a sweet spot with like eight or ten and I think my recommendation I guess is when, if you are going to use that for kind of creative or exploratory search is really use word frequencies to put out more common terms and keep them as few as possible. This is another example taken from the quote for lib, wanted to implement a related idea of related key words, just like there is just so many. If someone was to click on something related to original key word in order to go somewhere else, just, you know, it is a lot. So just kind of think about frequencies, world frequency filtering. Implementation, so I guess, I guess exponential data. I use Google end, so Google kind of giant list of end which you can find online, unprocessed so I have a script on github so you can process them. I also have an API that you can hit in this order to get the frequency of a list of words that you send. But essentially they like scrape all their Google books from the year 1500 to late 2000s and came up with a list of frequencies for one word, two words together, three words together. So end gram. In my case like you can't, when implementing the code you can't meaningful compare one grams and two grams meaningfully, because you will have way more two grams but each one is less frequent. So I ended up having to divide, so I took the ratio of numbers and I ended up having to divide the one gram by 11.75. So if it doesn't really make sense just kind of think about it, but, yeah. So you have to do something like that, some manipulation. More frequent words, like you see at the top, analysis, literature, planning, those are, I found they are words that are used in many different context so don't have much meaning, so I kind of filtered out the top, the most, most frequent words. They just didn't have much meaning because they are so vague. And then in that data set you have like newer words that are less represented like e-mail, because the data starts in the year 1500 you think e-mails is way less frequent because something that came out more recently. As well as scientific terms. So, you know, anything under like information retrieval, like a computer science is going to be less frequent than something like sky and sun, just because the English language overall uses less scientific terms. And then for automation, in Python, I have restful API, under the applications page I kind of showed you can hit that, so creativebook or/creativesever or get frequency or get Freg is what I called it. I have a lot of natural language problem issues, you know the common ones like combining singular and plural forms of the words. I had to deal to fishing and fisherman and fish, kind of have to treat it all as one word and some other DDP quirks that I had to handle. That's all kind of in the code. If you have questions about it I'm happy to answer. Yeah so in conclusion, many applications, you just kind of have to tweak the frequency that your thresholds are and like how many top frequent words are you going to use and you have, but you can have it for many different applications. And some user experience decisions to make. Are you going to do query expansions kind of like automated relationships happening behind the scenes or have it in the navigation on the side kind of related keywords and how many of those words are you going to display. And yeah so I have some resources on how you can implement this with solar, as query expansion or if you are go to use the side panel kind of related terms. How you can use that as well and I kind of list out the few technologies and libraries that I used as well. The code on github or should be tonight. [laughter] [APPLAUSE] Thank you Marya. Our next talk is so you think you want to migrate to RDF, Steven Anderson and Eben English. Here we go. So hello I'm Steven Anderson from the Boston Public Library. I'm Eben English, this is so you think you want to migrate to RDF. Let's get one thing straight people, we are here to talk about two things, kittens and RDF. Unfortunately due to time restraints we had to cut out most of the kitten slides. Let's get into RDF, if you are thinking of migrating data into RDF or providing serialization in RDF or consider yourself RDF curious, this talk will deal with some decision points, pain points, the ways in which you will be tested as you try and get yourself on to this map here. So RDF, aka, triples the graph, the data model specifying statements about resourcing in the form of subject expressions. A lot of the talks we saw today kind of focused on the last part of these triples, the objects. We're going to talk a little bit more about predicates, vocabulary and terms you use to describe your objects. So as you migrate data into RDF you have to make a lot of decisions about what predicates or terms of vocabulary you want to use to represent your data. As always, you want to choose wisely. So as you migrate you have to decide which vocabulary am I going to use, which schema, which terms do I want to use to describe my data, obviously a lot of different vocabulary, knowledge, domains, representations, so how do he decide which one to use as you migrate your data. Really key thing to keep in mind is that reuse is how linked data, how vocabulary gain value. More time people use URI, more value that URI has, greater the meaning becomes. So you should always prefer using an existing schema or vocabulary URI instead of inventing a new one, which is something that I think has gotten lost from time to time. So finding vocabularieses. A couple of great tools you can use, linked open vocabularies is a really fantastic tool, basically like a search engine for vocabulary and terms. Also gives you some information about the frequency which those vocabulary and terms are used. Really great resource, same as also really nice tool you can use, put in an URI will show you other URIs people have indicated that mean the same thing, can kind of move between vocabularies using that. Usually when we talk about migrating data in the past we talked about, maybe moving from MARK to MODS or this schema to that schema, always sort of one to one change. But with RDF you are not limited to a single vocabulary, you can mix and match and so possibilities become greater and you have a lot more ability to describe things. We have all seen cases of schemas, sort of people jamming data into places it doesn't really belong in MODS or MARK or other things like that. So yeah this is just a quick example of using double core elements to describe a single resource. With great power comes great responsibility obviously, you know, so the thing is you find a predicate you want to use, can I just do that, can I just use it? Well there are some rules that people sometimes are not aware of and so basically we're going to talk about domain and range and also the idea that just because of URI doesn't mean you can use it as a predicate. So the range of object is basically going to be the value of the predicate itself, you actually resolve the value to or predicate to. In ranges, if you had a MODS before you might have one photographic print you want to now move to RDF. In looking through previous search engine you may find these terms extend, you probably heard of that, that might look like a good thing that you might want to use for that data point. But if you look at the expected values for that predicate, resolving it or going to the official publishing documents for that linked data source you notice it does not accept string literal, type or size duration. You couldn't just use string literal with that would have to have blank element or something else first. But at the same time other institutions like [Name?] are using that with a string URL and not following that definition for the particular, is that actually a problem, are ranges important? Because we never actually do this in a database or XML document if you had a foreign key field you wouldn't stick text spring Bob into that, you would complain about that. Right now validation is kind of lacking at RDF and no semantic Web police saying if you are using predicates are you using them the way they are designed. Conform to excessive usage, the one that rest of the community you are in is currently using and dealing that predicate with or less popular predicate, possibility implementing your own, but fewer harvesters trying to use that data understand what that predicate is out of the box that you have to code something specific for it. But when they resolve that predicate they would actually be able to know what range to expect and not guessing as to whether or not you are following the definitions or not . So if you were to do the other choice, which is kind of split most of the time when I talk to people as to whether or not ranges matter or not in these cases where you have reason to break them of own institutional need. You can look at something with which does have a range of literal, it also has this use with property, if you look closely enough, called instance. That is what, we will go now into the domain which is basically going to be the object that your predicate is associated with. The one that it is being applied to. The latest thinking about this, from those I have spoken with, domain actually mean very little. It is like the fact that different extent has a domain of this bit instance, it is okay as long as the fact your object could be this instance even though it may not specifically say it is. The only caveat to this is to be weary that if there is something being this does invalidate its actual model, if it says press label refers to a book and you are trying to describe a music onto, you couldn't do it then. As far as, nothing that prevents you from using bit frame like that. Another topic is extinction, basically most of UR icon taken data more than URL, you are getting additional data from that actual URI source, URI have library community behind them so they will never go away. This example here, the official DC terms, documentation right now, they recommended this format as one of the main for file types. But if you U try to resolve any of those, you may notice that you come to a not found page and that's 2015 an issue about it, but still hasn't been resolved. There are people using these URIs in production right now. So there has been lots of talk about how to potentially handle this use case, as far as I know no one has done actual work on it, most of the URI resolved now so lower priority. I think the best thing that I heard that you can do is simply if there is data you care about from URI you have to share that and make sure you have this preserved at this current time. So we having fun yet? I want to talk a little bit about some modeling issues that going to run into as you move to RDS that might be potentially headache producing. What if no predicate exists for your term, specialized type of data and you can't find any possible predicate that makes accepts. Well you can create own predicate within your own vocabulary and that's, that's okay, that's okay. There are lots of people doing that, but you want to make sure you have some community investment, some kind of community backing behind that, name state.org is a great place that you can look for community creative vocabularies and maintain vocabularies, you don't want to just jam your data into predicate it doesn't belong. We have seen enough of that already with MARK and MODS and things like that. Use your judgment, but don't be afraid to create a new predicate if you really need to. Talking from moving XML to RDF. This is a pretty simple piece of XML, this is a little bit harder to model in RDF than it might seem at first. Probably not going to find has ownership no predicate anywhere. How do we deal with this in RDF, same data, publisher, place of publication, need to associate not just with the object we are talking about, but with the manufacturer of that. We are talking about a specific sort of event in the life-cycle of this cycle. How do we deal with that, a little more information than we can put in the triples. One way that people have dealt with this is blank nodes, so a blank node is not not a URI, it is anonymous resource, so you see here we have this manufacturer statement predicate the object of this triple, this, you know, weird underscore one identifier. Then that underscore one is the subject of another triple that's, you know, or a set of triples that has our publication information on it. Blank nodes kind of frowned upon though, because they increase the complexity and make data processing more difficult and there is sometimes systems don't support them very well. The other thing that you can do is create an object to hold that data. So we're used to thinking about records, marked records or MODS records, record being sort of self-contained document or thing. Moving to RDF you sort of have to think about more in relational database context. So here we have the different manufacturing predicate and then object of that triple is another object. Which is a provider object. And so this is sort of equivalent to providing public, in that table or separate triple that's where we put the information about that provider. Another problem with RDF is the lack of order F you put your data in triple store not guaranteed to get the data back in any order. That can be a problem with a scholarly publication, the order of the authors is something we really need to preserve. How can we deal with that in RDF? This is one approach that we kind of come up with where basically we've created a new predicate called name order and take name space, which actually doesn't really exist yet, just a theoretical. Then as the object of that triple is basically array of creator objects and those creator objects are represented by other triples in triple store. So this is a little funky, I love to hear if anyone has better approaches to solving this problem. I'm very briefing going to go over using lippinged data here. Real-time lookout, bottleneck, data providers not often available, if you look at this there is a limit of only one request every three seconds, which is quite limiting. If you try to use them for any non-trivial use case often times you end up getting blocked not wanting to use the site for high traffic uses. So the solution to that is basically you are going to have to cast everything that you want to use locally in that case. I don't have time to go over that now, in the community we are using data fragments solution that we are coming up with, so it is a little bit different not making data available but instead using it to talk about different layers, you can have something else behind it and it will do translation to that layer for you. The caveat it may not be as up to date, no longer on the Web, last batch down load 2014, I guess there has been updates since then. The advantage of doing that kind of thing, you get all performance response and standard interface and can build share code, autocomplete often not enough context to complete a form. In here you get broader or narrower forms to select from rather than just trying to select it from a single layer you are getting back from autocomplete. This example from Villanova that is part of that gem, being able to get more than just the label, you can get to broader and narrower concepts. There is also some controlled vocabulary managers I have slide for that you can look at later on, if you need to publish data sources that this is currently available and the best one I know of in the community. It runs the name face.org we mentioned before. Again you can look at that on your own. So we will get to the conclusions, which is that, is it worth it? Migration is never painless, it always is going to take effort and time for that to occur. And you have to wonder and question what are the real benefits to it? Often times people using public repository won't be able to tell difference if you are using linked or XML to power that. Just because data isn't, it doesn't make it harvestable because of the fact there is a lot of different uses of print predicates in a lot of different use how that data is used within those systems. Steven: The answer is maybe. [laughter] When tightly defined data structures, when people are following the rules, this stuff can work. So, you know, the theory is strong, the practice, you know, it is still catching up and the other thing is like this is just kind of where things are going and you are going to have to deal with it, just to be real about it. You know, as systems move in this direction it is just something that we're all going to have to deal with. So we have lots more slides about resources, further links, further reading. The slides will be up on the Code4lib website, so thank you. [APPLAUSE] And our, thank you, our next talk is how not to waste catalogers time, making the most of subject headings. John Mark Ockerbloom. Right there. John: Good morning, I'm thrilled to finally be at Code4lib, I've been seeing so many people in this room do great work the last ten years, what I've seen in this conference has impressed me to no end. I want to focus on what catalogers do and how this community can do best work. I don't know what sort of representation they have in Code4lib, how many of you are here, how many of you consider yourself catalogers in some capacity or another? Pretty good, thank you. Glad to see you. Thank you for being here. [APPLAUSE] I really love what you do. A lot of the work I've been hearing here fundamentally depends on data and a lot of that data has been created by catalogers. But when I look at the work that I've seen catalogers produce and discovery systems I've seen, I worry many coders are overlooking important aspects of how catalogers describe resources, particularly when it comes to subjects. I worry this means we are wasting their time. In the process, we're also wasting the time of our library users who still depend on subject descriptions for resource discovery. Yesterday, for instance, we learned how archivallals are the subject of archive into EAD to direct researchers to materials they may overlook, particularly when those materials aren't online. Even when full text is searchable online, simple term frequency searching doesn't turn up the most relevant text in most cases and certainly doesn't as we are just searching meta data, as we will see in a bit. For instance, in formal cataloging we have standard identifiers for concepts. Yesterday Christina an Missy told us why this is awesome and challenging, but if it is done right we can bring together a wide variety of works across our library that deal with common concepts, not only that we can use the same identifiers to explore across many libraries and even outside of libraries as we will see later. Catalogers do more than just classify and standardize, they note relative importance of subject and note. The first subject assigned is the most important one, it is the one that determines where a book will go on a shelf. And the remaining subjects are usually listed in descending order of importance. Those subjects around just unconnected set of concepts as we have seen in the earlier talk, these subjects are linked together, catalogers spent a fair bit of time specifying connections between related subjects and some different ways you might refer to a subject. No two people classify the world in exactly the same way and it is easier to navigate other people's organization of the world when you can get a sense of how it all fits together and what it might relate to in your own understanding of the world. Unfortunately, too many library catalog and discovering systems don't do much to help users take advantage of that rich, if we go to Library of Congress, I don't want to pick on them but it is a typical cataloger. Option to browse subjects is pretty limited, you can do an alphabetic subject browse, like you see here. If you've been out here in the historic sites got interest in American revolution and decide to browse that subject, first you will be offered 39 books cataloged with fast, click on another link to find more than 2000 books, if you click through this version of the subject those 2000 books by default are inial if bet Kcal order by title. You got a few options to change sort like to date, affection. Even then it is hard to find most relevant books in the subject with what you got here. Well, okay, you might say. But the catalogs we build nowadays are much more about search than browse, we got solar, trafficking, yes those are good things. But you can't just throw your subject headings into a term index and call it a day. We can do better than that. If I put subject heading for American revolution there the first thing that comes up is a book from 1954. Now it is a general survey of the revolution, but it is far from the most important book in the revolution for readers today. In fact it is not even the most important book in the revolution by this author. It does have a lot of same terms in my subject search, revolution, 1775 and 1783 so those term matches are what give it a great score for basic solar algorithm. Use this to narrow down 11000 hits into something more manageable, it is not so good at letting me explore related subjects are, might use different terminology like Boston tea party or battle of Lexingtn, we can make catalogs smarter when we pay more attention to what catalogers do and how users might want to explore. Here is another way we can do this, this is a catalog I built of public domain and other readable text available on the Internet. Organizer works on awareness of subjects and how subjects are cataloged. The works we see on the top of the list in the right for instance tend to be works where United States history revolution 1785 to 83 was first subject assigned. Books in that subject, where that subject was further down the subject list tend to appear further down in this list. Now I worry about whether I will still be able to do this whether catalogs migrate to RDF, you heard in the last talk, unlike MARK you have to go out of your way to preserve property, here is my plea to you, please go out of your way to preserve subject ordering. Thank you. This also uses other clues in meta data to try to bring other things, recent books will appear higher in the list, more knowledge and scholarship. That varies by subject wouldn't want to learn Web development from a 20 year book might be perfectly fine to have a book that old for math for example. This is mostly public domain, don't have recent books to show at the top for this subject. Instead what you are seeing is contemporary books, books published during the revolution or shortly that after. We know books published close to an evented is special importance to primary source, the time period of the event right there in the subject heading, it is easy for us to extract that and use to boost relevance of work close in time to that event. Now on the left side of the screen, various links you can use to explore a cluster of concepts around subject in various treks, like Serendipity again but a bit more control over which direction you go on, some include same division and sub traction you get with faceted -- from here you are only a couple clicks away from battle of Germantown, a battle fought about ten miles north of here, near my house. Thanks to relationships in LCHS or I can derive, I clustered those books with books about other things that happened in the same area during the revolution. Now some of those books might have more about Germantown or other things I may be interested in if I'm interested in Germantown, I don't have to stop with just what's in this collection here. I can click on the link marked your library up there and find a bunch of other resources I can use if I head back, we use the same subject after all. I can check out another library collection, Historical Society of Pennsylvania, I don't know if any of you visited, just a few blocks from here. They have primary sources and archival stuff, if I want to see what they have I click on another link and find all kinds of items I can look up over there, the family house right in the middle of the battle, but that's not all. We can use the subject heading that catalogers have signed along with data to find resources outside of libraries. For instance, you can click straight from catalog subject battle of Germantown to Wikipedia page, there is a house I was talking about right in that picture. Map to the battle and sites and links to a few dozen online and offline resources with more information. Including links back to hundreds of library collections you got a films o glimpse of earlier. This is where we can link professionals are doing, with catalogging the wild public is doing out on the Web. One very simple way of connecting, for instance, is taking the term non-librarians use and linking them to terms in library subject vocabulary, LCSH doesn't have a heading labeled Battle of Germantown, not as preferred or alternate form but we can add that because it got used in Wikipedia, with that earlier talk we can down load that data and use it to link to official Library of Congress headings, we can also do that with terms from fast, so users are more likely to find subjects expressed in their open language. Community can help us in other ways as well. I don't know if you have seen things about open syllabus project, collected over a million list of readings assigned by faculty for classes on various subjects. I would love to be able to mind that data. If someone is browsing historical subject resources being actively used in history classrooms could show up high on the results list, they do a lot of catalogging themselves. In some ways in fact they go farther than what library catalogers have been able to do on their own. So our key note speaker yesterday talked about Black Lives Matter movement, that's been going on three years now. There is still no Library of Congress heading for it. But there has been other things, a Wikipedia article for a long time, I wouldn't say it is the best reference source in the movement but does have links to over 150 articles in the media and activist run website where you can see what people in the movement are saying, many other ways people care about the information have organized information, often on periphery of lie prayerian, the Ferguson syllabus, and Ed, hashtag Tweets to document the sharing that's happening over social media. I've taken some stabs to try to integrate this kind of community cataloging with more traditional clusters you saw earlier. I don't really have time to cover the details, this is just a 15 minute talk, personally I needed to step back a bit from it to avoid overwork burnout that Yoose will be talking about next. I would love to talk with you after this session and share ideas. Really sharing is what makes possible everything I've talked about here. I couldn't of built the catalog I showed you without a lot of bibliographic, through projects or authority data LC has shared or cross-walking data OCLC has shared or article content data shared from Wikipedia, Wiki data. I haded a bit of my own data so launch some of of these services like severance into other library systems and the links to topical subjects in Wikipedia. I'm sharing that on github and happy to collaborate on it. For instance, when you can pick a lie prairie to search currently about 900 and some library systems in there, if your library is not on that list, I would love to add it, contact me or submit a request. Basically AA standing on the shoulders of a lot of giants, a lot of those giants are catalogers. Now I hope I have given some sense of some of the things catalogers do whether professionals or amateurs and how they can use what they do to better support information needs if it sounds appealing to you check out [Name?] I think there is a session, hashtag series of online and face-to-face meetings for discussing how catalogers and coders can work together. If you find it useful, these are places you can find me later on. I would be happy to take questions as well. Thank you. [APPLAUSE] Our next speaker is Becky Yoose who will be talking about the modern day Sisyphus, libtech burnout and you. Becky: Catalogers represent. All right. Good morning everyone, despite your best efforts, I am back presenting this year. [laughter] This time on burnout. Now when I first proposed this topic I had a general tone and structure in mind. However, after some reflection over the last few months this presentation took a different route than first anticipated. At least there will be no stick figure trying to roll a circle up angled line slide. Now before we begin, I have a few housekeeping items. This work does not represent the opinions of my current or previous employers. Advanced apologies to certain folks. This is going to be a high level talk. If you want the research behind this talk the link to the bibliography is posted. An article might also be forthcoming to get more into the complexity of burnout in libtech. Lastly, I'm going to be touching on abuse and various mental health issues, so please practice self care. Even if that includes needing to leave the room. In order for you to pay attention to the entire presentation I ask that you memorize these three numbers. 415, 70, 4000. Got it? Good. Let's begin. It would be an assumption that most of us have encountered burnout in our professional and personal lives, a burnout person is exhausted, cynical, frustrated leading to poor work performance, wheel spins but second place. A better way to approach what burnout is comes from Gayle North, they viewed it as a series of phases, the slides shows the twelve phases. These phases are no sequential, you can experience any one of these phases at any time during burnout. For example, you can be withdrawn, which drives you to work harder and change your behavior as a result, which then leads to depression. Speaking of depression, burnout and depression share many of the similar symptoms, but they are not the same. Maria wrote on this very topic for mental health week earlier this year where she found her burnout was masks serious depression. How do you tell the difference between the two? It's almost impossible at times. In her case it took a lot of self-awareness, time and trial and error. Now why am I specifically talking about libtech burnout? What is so special about us that warrants attention? Libtech straddles two fields, share traits, traits for markers higher burnout rate. Both fields deal with internal and external pressures to operate within tight resource constraints while at the same time expected to produce high quality products and services. Each field describes this reality with different terminology but they are more or less the same reality. And then we have the invisible often underrecognized or unrecognized work being done in both fields in the form of emotional labor. Emotional labor first defined by Harley is the act of regulating emotions to conform to certain rules and expectations while interacting with customers or co-workers, service productions, exhibit higher level of emotional labor requirement, partially due to the nature of these professions. One thing that might cause some of you to pause IT does indeed perform a high level of emotional labor. As the technology field evolved over the last few decades the focus on service to both internal and external users grew and technology workers are increasingly expected to develop and maintain interpersonal relationships and customer relations and workplace collaborations. Overall, fields are more dispositioned for higher burnout rates due to service aspects, high work expectations and workloads in each field. For those of you who would like a visual representation of what we face in libtech [laughter] with apologies to Ely. Since we are in a unique position between these two fields it would be wise for us not only to recognize the signs of burnout but also ways to prevent and recover from it. This slide shows some more effective ways in prevention and treatment, ranging from taking breaks from office, to therapy and work-life balances. You know what? We've heard all this before. There are a ton of talks, articles and posts about what you can do in preventing and recovering from burnout for yourself. There are many bibliography of this talk, instead this talk is for the rest of us. This talk takes into consideration one thing that many of these burnout talks do not. Burnout is a social contagion. One of us is burnt out then there is a higher chance of burnout for those close around that one person. We are surroundedded by burnout just by very nature of our jobs. You do the math. How do we address this contagious factor? I've identified at least three groups in which we can take on burnout in organized front. You might be wondering where you fall into a particular group. Will be overlaps and things you cannot do due to circumstances outside your control. Nonetheless this talk shifts some of the burnout prevention and treatments on to the community be it at work or the profession. The first group is colleagues and co-workers. These are the people you work with closely on a daily basis, you are the front line and most likely to notice when a co-worker is struggling. How can we give existing organization structures and job aton me, do to address burnout and co-workers. One of the first thing we can do is address teamworking style. Functional collaborative teams and departments provide better support for fellow co-workers and helps instill a culture shared responsibility and comradery, too much focus on competition leads into lack of social support and dysfunctional collaborations in teamwork. Collaborative departments see the value in cross-training and documentation and they give priority to them. Cross-training and documenting various duties and responsibilities provides a way to lessen the stress of taking another's task when the need arises. Additional way to make things easier for co-workers is to have cross-training workers work on the documentation with you. This way you build user testing for your docs. Even in collaborative environments we have trouble saying no from time to time. One way to help mitigate need to say yes all the time is to build an escape route, if we were asking someone to take on something that would be above and beyond their daily duties. Get that person the ability to say no or the ability to suggest an alternative to address request without having to choose all or nothing. Overall, load balance responsibly. Lastly, pay attention to your colleagues. Again, we work closely with them for majority of our workday. If you start to see someone struggling, say something. Do not give unsolicited advice or try to solve their problem, just a sin career check-in, hey is everything going okay or you have been stressed lately, always followed by is there anything I can do? Overall, be present for that person. They might brush you off and say everything is fine and they don't need anything, if they are still struggling don't stop after the first brushoff, the second or the third. And this is where the first number I asked to memorize comes in, 415 refers to a date. April, 2015. This was the date after many brushoffs, I did not brushoff a check-in from a close colleague regarding my well-being. So colleagues and co-workers, I ask you to do something. The act of doing something, no matter how small you think the action is, is all it can take to start addressing a problem. Managers are the second group. We have power baked into the organizational structures of workplaces to influence the culture and tone of the team we oversee. Even if your formal role is not management, any time you are leading a group or responsible for guiding others in their work responsibilities, you inherit the power dynamics of the manager employee relationship, like it or not you have that power. So you might as well use that power to make the team environment safer for your staff. The first way to do this is to give your staff clear and he can explicit agency, give them what they need to do their job then get the hell out of their way. Give them power to say no without serious repercussions, cultivate environments where staff feel more comfortable sharing emotions and stresses without fear of appearing weak or incompetent at duties. Acknowledge this comes from emotional inherent in their job. Emotional labor is invisible labor to most folks. The recognition of important role emotional labor plays in that job can be very powerful in terms of starting to address burnout in your team. Second, last time I checked unicorns aren't real, unrealistic skill sets, responsibilities that one person can handle in a healthy manner. We are mainstreaming positions built for those few outliars with such a skill set and can take on such a workload. As a manager, you too have the power to say no. When asked to do more with less, exercise that power. Better yet, be more constructive and more constructive way is to ask what services should be dropped to make room for the new. Your ground, if you fall, your team will fall. Last, while making team environment safer for your staff to essentially be human at work, you need to recognize your own limits, you are not a therapist or a doctor or a lawyer. You are a manager. Get to know the resources through your workplace. Including any Employee Assistance Programs and make these resources known to your staff. If nothing else put this information in a place where staff can access it discretely. Next number 70, after three months after migrating to new iOS, only myself and employee doing these tickets we were burntout by ticket 70. Staff was not happy to say the least, we were performing extremely high levels of emotional labors at this time. By acknowledging this emotional labor overload and creating spaces where we could talk about said overload we created a coping strategy for both of us to get through the next few months. The last group encompasses both co-workers and managers as community standards as expectations as well as culture involving based on what piece of the technology library worlds we try to incorporate. But as I presented on last year, the ununintended, to rip off of last year's talk, we need to be mindful of how we evolve our culture. More specifically, we must address the fact that many of us have gone through burnout. And yet part of the community that still produces burnouts at a fast pace. We have a great supportive community in libtech and we want you to join us. Just go through that right there, see we went through that long gontlet, if you make it out here you'll have our full support. In the case of the charming alcoholic, the charming behavior does not negate the alcoholism. Supportive libtech community does not negate the fact that we still expect others to go through various abuses, emotional, mental, physical, inherent in the practices and expectations of that community. There is a difference between just sustaining and healthy. Now a question related to the third number I had you memorize. Suppose you are burnt out and someone came to your office one day saying they can help you. This help, however, comes with two conditions. An initial downpayment, pay that now or opportunity goes away. You don't know when the next opportunity will come, finance. How many of you can write a check for $4000 right now? We keep putting the responsibility of burnout prevention and treatment solely on the individual. The asterisk from the slide of what individuals can do for themselves denotes the fact that only those in secure employment and personal environments can fully act on those strategies. This is why action in both our workplaces and as a community at large is so important. To keep the responsibility solely on the individual for burnout prevention and treatment. Completely ignores the fact that person might not have the vacation time to take a break from work or the agency to take any vacation time from work or the ability to express emotions at work without fear of their co-workers or employers seeing that these true feelings as an inability to do their job or to seek medical attention because they do not have health benefits. Or they might not have the ability to move and change jobs without serious repercussions to family, to relationships and to finances. I had the privilege of having that in my checking account. I had the privilege of not having a family or partner to be home when I moved to Seattle. I had the privilege of taking that opportunity for recovery and I will do what I can to give that opportunity for those who need it the most in the libtech community. So you might have noticed that I had three groups with three numbers. The first two groups had two succinct action plans, do something and make it safer. As a community we need to do things but one thing the community must do. As I mentioned in the beginning, this talk took a different path than first expected. For those of you who know me, there has been semiregular humorous saying of if you can't be a good example then be the horrible warning. I was the horrible warning because hey I was on 24/7 my job had me going, you know, all night, all day, and I was working all the time and I did a lot of stuff and that was the running joke a lot of folks, myself for the longest time. I can't joke about it anymore. Being a horrible warning in part broke me. What hurts me is that I see the exact same horrible warning scenario that I was playing out here in libtech, with the people I care about. That's all of you. We still joke, we still expect people to go through what we did, we, as a community, still consciously and implicitly place value on horrible warnings not as actual warnings, but as failed admiration to the unhealthy and unstainable amounts of time and energy they pour into their work to inspire us to go down that same path. So this talk was as much for me as for you. A reminder for us to, what we need to do as co-workers, as managers and as community members. In short, if I have to drag myself internally, kicking and screaming, not to be the horrible warning anymore, you bet I'm taking you all with me. [laughter] For that the action plan for us, the community, no more horrible warnings. [APPLAUSE] Thank you, thank you Becky. Sorry. That's our talks for the morning. We are going to take a break now, but before we do that I want to make a couple announcements. The first is, oh -- very unprofessional of me. I want to thank our, I already announced our scholarship winners but I did not thank our sponsors and we would not have had the scholarship recipients this year we had if it hadn't been for our sponsors. I want to thank LET libraries for sponsoring, Penn state University Libraries, the DPLA, Drexel University School of Computing, AV Preservation and then Marked Body [Name?] and finally lots of contributions from approxi Code4lib and Ashley who is working on our lightstream is another person that donated. Thank you very much, community absolutely needs me and appreciates it. So a huge round of applause for our diversity sponsors. [APPLAUSE] and two more announcements before we break. The first is that lightning talk sign-ups for tomorrow morning are going to be available now next to the registration desk. If you want to sign-up for a lightning talk please go do that. Finally, if you are doing lightning talk in about 15 minutes, bring your slides up so we can put them on the laptop. That's it, go have coffee and stuff. Thank you. (break) All right. All right. Folks, we're going to start doing a couple give aways to get you back in here, so let's get started. All right. So the first giveaway-tron gets to is Emily, Emily are you in the room? Hey, Emily. Wooh! [APPLAUSE] Go right back to the desk back there and claim your prize. All right. Moving swiftly along. Let's try this again. Number two, this thing definitely needs music. Whoever wrote this, add music. Attalot [Name?] get your prize, that's my consultation for me butchering your name. [APPLAUSE] Number three, yeah, sound guy, way to go. Oh yeah man, yeah. Renaldo. Renaldo? Right there, all right. [APPLAUSE] Claim your prize. Elliott, come on. Where we at? Sonova [Name?] sorry. All right. And one, we have one more, we will do one more. All right. Last one and then lightning talks. Looks like it worked, got people back. Come on down. Must be something about that seat right there, two people next to each other. All right. Okay. With the giveaways now done we will move on to the lightning talk. Can the lightning talk presenters come on up and line up in the order you are listed up here. That will work out yet. Let's get you up here. You are on desktop? Let's find you all right. There we go. All right. Hi I'm with libraries, I just want to talk today about a side project I've been doing to build a physical device for recording desk interactions. I'm sure, like a lot of you, our liar bra is trying to build culture of assessment, really important show we are being missioned aligned, that means collecting desk interactions which we have done for centuries or so, but yeah it's sort of mission critical now. We have seen a modified version of the lead scale references later, basically that translates to interaction one, easy, two medium or three, hard. Originally a six-point scale but that was a three. So we are using [Name?] to do the data recording right now. So yeah there we are. So why am I talking, what's the problem? Why am I talking about this at all? If you look closely at that form, that's a lot of little radio buttons to record the concept of one, two or three. You got, you know, where, where you are and that doesn't change from desk to desk, you got who you are, that doesn't change because it is a generic student account, always walk-in type, the rest of this stuff is not used, not used regularly anyway. So not exactly a super ergonomic way to go. I don't know about you, but our desks get busy. Maybe not, go on sale busy, but you get the pick it picture. One super fun thing, I don't know ours scan to the active window and when that active window is not the ILS like say a Web form, you might wind up checking out things to the wrong people or not at all. We had that a couple times, so this is an issue, it is a usability issue. So from the time we knew this was going to happen, okay. We will skip over. So let's, let's look at building something that is like a physical dedicated device to doing this purpose, something as easy as easy button, maybe like the device from NS3k, this has been done. You may remember from code for the journal 2013Tim and Jonathan, sorry if I butcheredded your names, built this, very similar kind of thing. This is not a new revolutionary concept. So how does this work? The human interface device is connected to the service station over USB and one, the way I'm looking to do it, way it is a little different they use an Uno which can only send serial data, they had a custom processing script to be able to do anything with it. We can human device like a keyboard use any hot key thing, whatever is appropriate for your production environment and then script that to send it off to whatever data repository you may want. They used Google Forms or Google Drive, whatever at Brock [Name?] but do what works. So here is a physical build, 2.0 for $15.95, three arcade buttons, large for $5.95 each. 3-D printed case, maybe a buck, I haven't printed it yet, we pay by the gram. And wire, USB cable, mini not micro, not what you are charging your phone with, but stuff you should have or you will hopefully get. So total cost to build, about $35, that's a cost of Rasberry pie without any of the stuff to make it work. I set up a keyboard took key mappings, figured non-conflict if you know better let me know, controllality F6, 7 and 8 seem to be as non-conflict as I could find. Exercise left to the reader is setting up of what your hot key data is be, data repository, not really this talk. This isn't hot air, there is code. See there is a snippet of code, I written that. A photo device, I was prototyping, on the team e-mail this is a thing, that's a screen shot from my sketch-up I haven't tried printing it yet, you can see no bottom or hole for USB, but this isn't just vapor tech, if you are not doing 123 scan recording you can do call for backup button. I need a clerk, you can do it how are we doing, do you like your experience, other ideas. References, thank you. [APPLAUSE] Our next talk is transferring digital records to the archives. Hi I'm Greg, at Sony Albany, I will talk about transferring to archive, word files, office documents, PDF from people on and off campus to the University archive. So I have a pretty basic Python app called Ant that does this when. It runs, it gathers file system, meta data and runs, checks them in records original creation environment and transfers them over existing networking or Google Drive, I will do a quick demonstration, if this will play. So it is just really on top of windows explorer, you can open up the browse or if you select any file or folder in windows explorer there is a link, it asks to elevate UAC privileges, which is going to let us access our disk as a device. It is a basic window where you can add description at any file or folder if that's practical. Navigate the directory tree and select only the records you want to transfer. So hopefully this encourages people to select records in the original creation environment. You can add some session level meta data and there is some configuration options, this comes preconfigured generally we don't have to worry about any of these options. We will do a transfer over Google Drive. What this does now, if you have the correct level permissions it can access this disk device and runs MST record which is a pretty basic digital forensics, and the four file name time stamps. It then bags the directory, compresses it and opens up the Google Drive work flow. So just pop it up in a browser and asks you to enter credentials and then it will push the files up to Google Drive. As you can see, showing progress to the user is, the biggest problem to this project so far. Still a little wanky, sends you user e-mail, then we will take a look at what it packages. Just a basic bag. So you have your data in it and then this is an XML data file that comes included some session level meta data time stamp of when submitted, identifier. For each file and folder you have 8 time stamps and territorial events that aren't machine readable right now. It gets nice little structure for us to, to do things later with. So this works sort of. There is still some usability issues um, I'm not sure it is sustainable, pretty sure not scalable. I think we can get it to the point that it will work okay for us and I think why I wrote this is because I really didn't have a better idea for this problem. I think this will offer real-world use case and proof of concept so we can move forward towards a solution. In that Spirit the things I've learned in this project is we need to package consistent concessions with meta data. This will help us later in description and making things available. Dates have been important for archival description and getting those time stamps is available later on and more efficient with our work flow. It has to be designed by record createdders, these are all different. People have different environments where they manage records and all very useable for them. This is a desktop problem. You can't really run hashes and get files meta system from Web app without some sort of client. Also this is authentication problem. I think that's why archives are still using box, DropBox and Google Drive that's really the value we are getting out of it is the authentication. I think both of these things are things we tend to avoid doing now. So I think that's why this problem sort of persists, but another thing is that I think we can do this now. The tools are getting pretty easy an even the forensic tools if we package them well we can get a good amount of meta data and consistent packages before sent to archive, that will reflect our whole work flow. Thank you. [APPLAUSE] All right the next talk is on the Instagram API change. Okay, all right. Hi I'm with NCSU Library, here about changes to Instagram API. So for those of you who don't know, rental is an open-source app that is an Instagram harvester developed by NCSU Libraries around 2013. It requires basic and public content API scope from Instagram, without proper approval to Instagram now the app is scheduled to stop working in June. What happened recently Instagram decided to go through fairly major API change, now every new app you create requires their express permission levels to be granted through a submission process. So where before it was a lot easier, you could police department it, reach their end points and get whatever data you were wanting. They changed that situation quite a bit. So that happened in November and we were kind of surprised by that abrupt change and so we started looking into the formal permissions review process to see how that would go and what Instagram has listed as a limited set of value use cases they deem appropriate for Apps that they will approve. So here we can take a look at, these are the three valid use cases that they list out. The top one seems like it would work with our Apps, the way Rental works harvests based on hashtag, we had my hunt library was hashtagged using collecting images students posted using that. Instagram changes this hope individuals share own content which is really what we are doing. They want you to get explicit permission from the individual. So they would sign into your app, require to grant you permissions to then harvest their items, doesn't really work for us because we wanted it based on hashtags didn't want folks to go through other processes to share them back to the library before we could harvest them. You notice the other two is more gearedded towards commercial uses, brands, advertisers, broadcasters and publishers. While we are all of those things, we are also different than all of those things. So it is kind of unclear where we fit in, in that situation there. So like I mentioned before, all of the clients that are created now are placed into a Sandybox mode. This will work for a lot of Apps. If you are trying to collect on one hashtag from your account or maybe you have five accounts within your organization, you can add those all Sandybox users and it will collect. If you are trying to reach broader Instagram community at all, this won't work. Users only 20 most recent photos in archival context if we are trying to grab as many photos as possible trying to put in archives we don't want just ten users and 20 of their photos. To get out of Sandbox you need granted permission, has to be approved by Instagram. This is really the most frustrating part of this process. The permission review, a few different submission requirement, a working demo version, at least, of your product, your app. A privacy policy, you need to writeout value use case, have a screen cast of your app, for any additional permissions that you require you have to write a separate use case for those and then you package that all up and submit it to Instagram. Where that gets difficult is there is no communication back and forth. You either get approved or denied and you don't know why. From what I found so far of other folks Apps denied they say doesn't fit in with valid use case. So in our situation I submitted essentially the same app three times. Two of them were non-production Apps we didn't want to lose access, currently they are still working until June didn't want to lose access, I submitted the same app three times with mildly different documentation and approved twice and rejected once for same exact functionality. Be specific, pick a valid use case, quote API use cases back it them and say this is how I feel it fits in with your situation. Be persistent, so just keep on resubmitting every time they reject and hopefully somebody will approve it and be lucky, because eventually maybe they will approve it and you have a valid key and it will work. I think this can bring up a broader talk about how we work with these APIs and corporations in general where there is no direct line of communication, particularly for our kind of subset of organizations, if there is ways we can reach out to the communities and open a little more so we can have dialogue back and forth to explain how our uses are similar, here is some links for more info if you want to check out Rental. I put out sample submission docs I have done, so you can check those out. Thanks. [APPLAUSE] And our next talk is why cats prefer boxes over fancy toys. Desktop, all right. Hello everyone, I'm Kate from University of Washington library my second ever lightning talk at Code4lib. Why do cats prefer box over fancy toy. Those of you who don't understand this meme, it's been around for quite a while. Since it has been around the cat, you know, when presented with a fancy toy or bed will just choose to go in the box it came in. I would like to tell you I could explain this weird bit of cat psychology, that's not really what this talk is about. It is connected, but really about sharing code solutions for all sizes of institutions. And kind of just start this off, I want to explain what I mean by sharing code solutions. Here, by the way, since it is not really about cats I have put tons of photos of my cat on my slides for your enjoyment. [laughter] So what do I mean by sharing code solutions? Well, what I mean, not saying about large open source projects here like Hydro island, talking about when we decide to post our code we have written to deal with an issue with a third party vendor project where it is just a little fix or something. In particular, let's talk about, you know, our discovery systems, IOSs, lots of little code snippets and stuff like that, Virginia Commonwealth University has a popular one, Notre Dame does as well, a lot of people are trying to customize these Premo instances. This is what it looks like out of the box. This is my job as one of the developers has changed ours into it. Quite a bit of changes there. You know, the standard branding approved usability, more accessibility, some new responsiveness, new functionality, all sorts of things with it. So people see this and they often ask, well can I have my, can I have your code? But they don't want all my code. They just want the code for just one tiny little bit. And I think can go, yeah you sure you want that? It is 4H TML files, only 236 lines, one giant file over 5000 one JavaScript file otherwise Java and this is all mashed together so less Web requests. They look at this and goes like you sure you really want to do that just to find the one little bit in there? And let's face it, dissecting code takes skills. Not even easy for seasoned Technologists. I worked for years as a programming instructor. I have read student code that will boggle most of your mind, still hard to do coming from a lot of developers. Most importantly not all libraries have developers on staff we need our code better. The smaller, you know, more resource-constrained libraries can use it. How can we do this? One, don't share monoliphic, share things as independent exponents. Some of you are going, just been github, snippets, even those don't really help. Support for what I'm talking about, hard to show all sorts of things. So we need to share even better, need to be able to show an handle dependency that exists between shared code, we need customization easy, various lines commented in the JavaScript. Needs to be easy easy deployment, 53 files and change this in HTML file, you are golden. So I'm involved with some approaches on this. University of Washington is part of the cascade alliance working on putting together a Premo software tool kit of just a select few independent fixes, going to be available in about mid-april or so. We're wrapping it up right now. Also building this very giant Premo widget library, a Work in Progress, that's the bucket Repo out there, I will post my slides later. Pretty much trying to take my code and break it into small parts. Now, you know, there are a lot of dependencies listed in each one, trying to make this as user friendly as possible. But guess what? It's really not enough. It is a Work in Progress and I realize that. Check dependency takes expertise, customization is still an issue. I'm trying to do ongoing work, will keep adding more widgets this is off my regular time, lowest priority on all the jobs I have going on. Also want to build a customized downloader tool for this, running out of time. Real briefly why does this have to do with cats, toys and boxes, library use things out of the box, most small libraries have to stick with what is in the Repo and there is that. Thanks. My timing was off. [APPLAUSE] All right, thank you. All right our next talk is Michael [Name?] with Excellemeter, sort of butchering it, are you on a desktop -- [speaker off mic] I think it was minimized over here somewhere. There we go. There you go. So I want to talk briefly about a tool that has been really, been developed at University of Pennsylvania and been really useful to us in working with large amounts of XML. So XML, everybody is familiar with and there are a couple of use cases that I want to mention and have in mind. One of them is dumps of bibliographic meta data, Mark XML is very common and when you are talking about a dump of catalog meta data you are talking about a lot of XML. And XML, and XML despite the sort of document centric nature of XML as serialization format it is really a sequence of more or less independent records. So one of the things that, that I certainly have encountered in if dealing with XML is that you really don't care. The distinctions of what constitutes single document for the purposes of processing and transfer is kind of arbitrary and so you are forced to choose like, I don't know, 50 thousand records or something like that. And you also end up doing a lot of file system, I don't know if you are doing multiple transformations and bundling things up in ways very difficult to modularize. So XML is serial in nature, document-centric. This tools joins multiple together, things like multiple small documents and stitching them together into one. It can also split large XML streams like arbitrarily large like millions of records into smaller chunks in configurable way. Also by treating it, by treating XML sequences of records and split it, it allows you to take advantage of parallel processing an subdividing records on failure of transformation using XMLT, also some other things in there like the ability to read from a database using an SQL query and rep that as like flat XML then also take that flat XML and sort of it stitch it together in a way to take advantage of nature of XML. Configurable modules for output, including writing to files, writing to standard out. And posting to URL, et cetera. So this is documented at the project, the project repository github library, kind of a weird name so quite Google, I will have the link at the end of the slides this is a focus on command line interface here and that's for the ease of adoption and integrating with existing work flows and really like effort into making this useable and configurable and flexible. This is one example here where um just to follow along with what is going on here, we are finding a bunch of XML files in a directory, piping that to XML parameter program, which reads those together, joins them all, the dash-A flag joins everything together then splits it into chunks of size 10 and then outputs it G-zipped which is the Z to a base name of file defined as Temp.xml. That will look like 001, 002, 3, et cetera like that. Just another example here reading from the database, some configuration options including defining how it connects to the database and you can see that it is allowing you to separately specify the selection of records and the retrieval of information associated with those records. And it's designed so the pipeline is Java process internal. XLMT on files we are finding XML files and transforming them and defined a record X-path to be able to log the ID of records that fail transformation. So it is false-tolerant in the world where you can't depend on everything working fine always. It is nice to be able to not fail completely. That's all. [APPLAUSE] Thank you. All right. Thank you. Our next talk is resolving meta data from DOI and are you on the -- [speaker off mic] there you go. Hi, I'm Michael from University of Minnesota libraries and when I put this lightning talk on the schedule yesterday my thinking was to focus on our own specific implementation of getting meta data back from DOI streams we have collected at University of Minnesota Library, Ruby-based library up on github. It turned out, as I was preparing the slides, as we get there and underlining technology in how that happens is a lot more important and a lot, probably of a lot more use here in the audience. So luckily this lightning talk turn into four-and-a-half minutes, would of liked to take on content negotiation during the Linked Open Dime Novels talk this morning. So hopefully this will be a enough of a quick overview that you can start thinking about adding content negotiation to your normal toolbox of things you might ever want to use. So the gist of it is you need to reply service or service providing it, you need to respond this, provide accept header that lists out the content types that you want to get back from the thing. So normally if you have a DOI and you hit the link, send it to the Proxy resolver at DUI.org redirected to a third party site that serves you the content that you wanted. But you can ask for other things, you can ask for XML formats back or different types of citation formats back and it is very easy to do. All you need to do is send accept header with the content header you want in a separated list. So you can also give preference treatment to one or the other, in this case I set that little Q value there to 1.0, highest possible value, that's the one I want. But if it is not available I take XML format in return. So looking at this in a example that you could copy paste from this slide and it should run, we et cetera is the header, pass the URL we are going to, passing through the DOI.org proxy service highlight down at the bottom is accept header being sent in the request. And you can't forget to follow any location headers that the thing gives you back, in a lot of cases it is going to redirect you to another, another place that has the specific format you are asking for. So in this case the capital L flag curl, it sent back location header down at the bottom, so it will follow that. And the next thing you need to do is pay attention to what content type it gave you back. You probably asked or possibly asked for more than one. In this case it sent back Jason type, here it is, that will come back in the response body. Or maybe it was XML that we got or maybe even a citation. So doing this with Ruby is turns out to be somewhat less important for us, we have a library called simple DOI that is, URL, you will want to look at the DOI handbooks section on content negotiation because it explains this pretty well for this use case. But this is applicable to all kinds of other places where you might use content negotiation. And for the types of formats that you can get back for DOI meta data in particular, those are listed at the cross data site documentation at that bottom link there. And that's all I have for you today. That's me. I'm typically in this if you have any questions for me or get me on Twitter or here. Thanks very much. [APPLAUSE] Our negotiation talk is quick search, now open source. You are up. Right there. All right, hi. My name is Kevin and I'm from NC Libraries, last week we open source quick search finally. Here is the URL to the github, put that back up at the end of the talk. What is quick search, a single search platform with federal box style results, we originally launched in 2005 to our users and we rewrote it a couple years ago using Ruby on Rails at this point I want to give a shout out to Cory and also his mad Ruby skills. So there is what, here is what it looks like, it is not full screen so you can't see our header, but that's basically what our implementation looks like. There is a smaller view of the whole page, which you might not be able to see real well. The features of open-source version, really easy to get up and running, get development environment up and running or test it out, included vagrant files, so really you can get it up and running, spin up a server in a few minutes with one command. Modular and extensible, so we modularized searchers and themes, we have statistical, Google Analytics, best bets and most of the other features available in our implementation of quick search. What are searchers? They are plug-ins that provide search functionality for particular library service an these are published in Ruby and quick search and very easy to write. How easy you ask? Here is a quick example of minimal viable searchers, things to note about this, we subclass the quick search searcher class, this one is for a fake catalog. And then quick search only requires that you define two methods in your searcher. It leads first one is search, does the search and second one is results which just returns a list of results. You don't even need to define a template to render results if you don't want to quick search knows about common field, title, author, thumbnail for example and a few other ones to include this into your quick search implementation you just include the Gem and Gem file and you just tell in quick search Config you want to use my catalog as a searcher. Then put into search results page and use the quick search render module helper and just put the name of your module in there and pass in a view ultimately if you want a customized view. Then that's it. So I mean, maybe a few more things if you wanted to release your searcher or run it in production, so you can, you know, as I said add customs results template. You might want to use configuration values so other people can override, KPI things and things like that. Internationalization, override default error messages and you know, more results, see more result strings and everything. We released a few searchers so far. Here is a list of them. We're still working on releasing the [Name?] just need to find out if breaking terms of API before we put out that code. And as I said earlier, themes are also modular, also published as Ruby Gems created own theme entirely from scratch or override parts of ones already available, it is really easy to put in own header or footer. We released a generic, responsive and based on the framework looks a lot like the theme that I just showed you for NCC libraries, but here is what it looks like. And as I said, we statistic collection is automatic, as soon as you say like you want to use the searcher in quick search automatically starts collecting statistics for that. And we provide view for you to kind of see how your modulars are doing, the searches, all anonymous. You know, how is the best bets are doing, how many times they have been served, how many times they have been clicked on, in a nice view like this. Just a disclaimer, extracted production code still fairly early stage, been running it in production a couple years, just modularized it and everything. That being said we have a few to-do items, fix test we broke, a couple more features that haven't been like pipe-ahead we have in NCSU version, then make core Rail engine. We want to work with you, so if you have any more questions about this, if you are thinking about trying it out or using it, just contact me. You can see me after this or, you know, e-mail, Twitter. There is a github Repo, at the bottom is our implementation of quick search. Thank you. [APPLAUSE] All right. Our next talk is using this document -- My name is Da in h, program manager with artifactual systems and this is my first Code4lib, wasn't intending on talking but in the Spirit of participation I decided at the last minute to reuse some slides actually that I presented at Code4lib BC in the fall. So a little bit of background about this project. So I'm going to be talking in the context of our project access to memory, open source for standard space description and access. And basically it was first developed with sponsorship from international council archives starting 2007. When we were developing it the idea was let's againize base framework and call it Q-bit then we will do different themes like I see was the first version then we can make different packages one DCB packaged for library special collections and what not. Then as we got along in time we had different themes introduced so we had about four themes in the I see version different in DCB, in late 2013 we had a 2.0 release. In terms of our documentation this was all kept at Wiki at the point. This meant for one project we actually had three Wikis, for different themes and a total change in the interface on the way coming. So this was kind of a nightmare for giving people instructions on how to find things, we have to say okay for this instance, you know, the button is here. In this theme it is on this side, if you are using this version we removed that module, go to that page. Getting a nightmare and we were looking for something else. A lot of this we found was the problem with the way Wikis are they can tend to be untendedded gardens unless you are conscious about how you set it up. No way to create versioning in our Wiki, no enforced structure. And often times because community users were also creating it often orphan pages not linked. Often a request for PDF version of our documentation, that just wasn't easily possible. I mean, there is some solutions out there in libraries, but because sometimes people drop HTML right into the Wiki output of tools or other things for conversion was not necessarily consistent. That's where Sphinx came in. So it is a documentation generator that's open source, it was first created by the Python community as a solution for their documentation needs. But it has since been taken up by hundreds of projects. It builds upon some existing open source projects, restructured text which is Markup language and Docutel, a text documenting system. So this is a kind of basic overview of what's going on here. The user writes documentation in restructured text, that goes into Sphinx uses parser to generate the outputs that you want. I just sort of shown in this image some of the common outputs, PDF, EPUB a lot of different developers, add-ons, themes and other goodies that the communities added along the way. So a quick overview of what this looks like. It's fairly familiar if you ever worked with a Markup language. There is, you know, you got the ability to set anchors so you can link to different places. The section headers, you can actually use whatever kind of formatting you want and Sphinx will figure it out, as long as you are consistent. You can add glossary terms, images, or figures with annotations, building, emphasis, all this kind of stuff. It includes a whole module of admissions, notes for notes, tips, warnings, cautions, hints. You can add links and reuse them with a simple shorthand or you can use one-off links, you can drop in code blocks and all kinds of parsers in there, there is an example up there for using XML, you declare what language it is, if there is one in there already it will handle the rest for you. Of course, standard features like bulleted, lists, numbers. This is a brief peek, squished what the code looks like, themed on the website, we generate in HTML and push to our website. Running short on time here, so these are some of the advantages we found with Sphinx and I just want to say Wiki still has uses and we definitely still use it for some things. There is a link to our documentation and where our ducks are kept and thanks. [APPLAUSE] Our next talk is GW programming consultation. Desktop, should be a browser window. I'm standing between you and lunch, that will teach me to sign-up faster next time, my name is Justin, software developer and lie prayerer at GW Libraries, a program we are piloting, proceed great manying and software development consultation services to anyone at GW. The idea is pretty simple, as people come to the library to get reference services and to meet with a reference library they can come to the library and get software development assistance and meet with a software developer. The goal is to support academic or scholarly inquiry at our University, which requires coding. Like I said, it is open to anyone in the GW community, faculty, students, researchers. It is staffed by software developers and librarians, two others are which in the room, Laura and Dan [Name?] we draw support from people throughout the library. Right now for the pilot it is limited to two hours of consultation per week, which is scheduled in 30 minute blocks. appointments are made via research calendar, which is how people book the other, book their appointments with the reference librarians as well. As you can tell from this nice slide, we got an assistant from the libraries communication staff to help us with the publicity. So what's in scope? It is fairly broad. Pretty much anything that has to do with coding, software development, scripting or programming. You can read full list here, basic idea is support wide-array of cases where people need help with coding. Maybe a person that doesn't know what tool to select, done know how to use that tool, person that has gone through code academy or similar and gone through first program and hit a sort of a bump. We are trying to help reduce those sorts of barriers. So CS assignments, we're not here to help with that, we're not here to fill function or replace TAs or professors, not here to help with tutoring. We got in a bit of each of those, people showing up with those sorts of requests and we had to turn them away and redirect them to the appropriate resources for that. Why are we doing this? Obviously we want to support scholarship at GW that requires coding. We want to lower the barriers to people doing that sort of research. But the real reason we are doing this is to force us to get out of our cubes and talk to real people. And it has been fairly successful at that so far. So this is only been in place for a couple months. But we already learned a couple things. First of all, there is a demand for this service. So here is three examples from consultations I had in the last week. We had a research assistant who wanted to scrape PDFs of security resolutions. Turns out that is really hard, that's a separate talk. We had a student who wanted to develop a portfolio website. So we walked through all the different ways you can create a website and ended up settles on github pages and walking through how to create a site with github pages. That same opportunity was also preparing for tech interviews so spent a couple minutes on how do you prepare for tech interview and what sort of advice could be passed on about that from our experience. So the other things we learned, as I mentioned, students do need a lot of help with CS assignments. Something, some need it not be met there, that's not what we are trying to solve with, solve. We have learned something that any reference librarian will tell you, meeting in person is way more productive than going back and forth over e-mail, which you can resolve in a minute or two of conversation often takes numerous e-mails. This is, we found this has raised our team's visible so we are getting a lot more requests for assistance that are outside the scope of this. The sort of things we would normally do, helping professors, write programs or gather data, but the publicity for this is making the people on the campus much more aware of what our team does. So that's a definite positive benefit. Definitely finding it does take more time than 30 minute consultations, usually helpful to do some prep work ahead and then some of the consultations have involved work that has extended outside of the 30 minute consultation. So if you are interested in learning more about our program you can go to this website. We're, if any of you have a similar program or considering a similar program we would really like to talk to you about your experiences. So thank you very much. [APPLAUSE] And our final lightning talk of the morning is the arm project. Okay. Sorry. [speaker off mic] Hello. So I'm Karen Hansen, I want to tell you about the [Name?] project a fun project, Portico. So the underlining idea is that the context of a scholarly work is completely changed in recent years, where it used to be all about the articles and other articles connected to that, a lot of the time it now looks something like this. Which is you have multiple components that are in different repositories, you might have a data set that supported the article, but that might have been derived from something and then some savvy programmer written some code to abstract and analyze that, been posted over at github. So you end up with this big map of resources with all different kinds of identifiers, all different relationships between them. Some of them might not even have identifiers, so maps might let you have articles within them. How can you capture these new scholarly works? So that's where Armitt [Name?] comes in, grasp these linked data and store and preserve them. We are calling these maps Armit distributed scholarly compound object. So broad use cases related to at because, reproducibility, impact and discovery, I'm not going to go into each of those, but we have those on our Wiki if you Google map project you can find a lot of stuff for that. So when I acknowledge that typically you don't get this map all in one shot at the moment, a lot of data sources have pieces of this map and because we are primarily focused on harvesting at the moment, we're working to get, little pieces and star to connect them together. So for example, from data site might look something like this where you have references to versions of the code. You have the creators of the code, an article related to and then other citations to random websites there. So I want to show you what the Disco look like visually. So taking you on a quick tour each gets assigned URI and then there is a search, creator and a description of that Disco. If you can imagine scrolling down, this is the Web page. So you list aggregated resources, these are the things you want to be considered as compound object. In this case, because we are harvesting, we just have one thing, describing one thing. But in another case research, want to create compound object they would have multiple aggregated resources. Then you can just, whatever you want as additional statements, as long as it forms a nice connected graph. So back up at the top Disco can have, so if it is active assumed to be still accurate. You can make it inactive or update it. Each time you make, do something to the repository it gets related event which just captures, anybody can create an account and create these things, just associates it with an account. You can version your Disco, so if the links have changed or landscape has changed for your work then you can update it and people can track that over time. So the overview we have is the resource view, if you want to look at what does our map know about the single resource, all of these Discos. Then visually it looked like this, look at DOI for example you can see all the relationships connected to that DOI and you can see on the right what mentioned that DOI and look at those individually. So shown you the preview, most of the work is done in rest API, which is responsive support different flavors of RDF and it is basically towards getting and posting Discos, you can look at resource views, you can look at all the ones created by a specific agent and then some filters. So future plans, we plan to make this open source, we are looking at doing some inferencing just kind of make sure things references cross-map. Much clean up the data, we are being a lotting at a point and click Disco creation tool and interested in links, all of those links and articles, maybe there is something we can do with that where we provide links associated with that. And also about to start working with some pilot collaborators, so if you Google us you will find a lot of our websites are have technical documents, have a Twitter and you can talk to me, I will be here all week. So thank you. [APPLAUSE] All right. Thank you to all the lightning talk presenters, two quick announcements before we can get to lunch. If you are giving a talk after the break, make sure to bring your slides up to the podium before you go to lunch so we can get those loaded and ready to go. Secondly, there is still, you can still sign-up for, you can still add ideas for breakout sessions out by the registration desk and that will be up until the end of lunch. So with that, we're breaking for lunch. (lunch) Hello.  Welcome back.  The afternoon session of Wednesday.  A few announcements real quick before we get started.  We've been getting some noise coming from the back as people exit and enter from the talk.  Please try to use the side gorse and maybe we can minimize that a little bit.  Make it less distracting.  Also, a few people have asked about slides.  We will be getting the slides up on the website.  If you go to the schedule and click on any of the talks, that's where I will be.  If you want to add your slide to the website, I'll show you how to do it through Github.  Otherwise, we'll make sure those are up there as well.  I would like to also just read the list of the breakout being proposed and maybe we can get a show of hands so we know what size room to put them in.  I assume one of the volunteers is going to be taking notes with the hands so that we can organize that.  Number one, community diversity with the programming committee.  If anyone is interested in talking more about that, can you raise your hand and let us know? Okay.  Thank you.  Web library IT accessibility.  If you're interested in talking about that, can we see some hands for a minute? Okay.  Kaitlyn.  As her hand raised.  Dev options.  Please raise your hand nice and sigh so the volunteers can see.  Okay.  Thank you.  Can you see back there? Should I do the dray method of measurement? Metadata assessment.  Anyone interested in metadata assessment? Nice and high.  Okay.  Great.  This is actually very even so far.  Number five, building a coalition of libraries running tour.  There's something that's not legible after that, oh, exit node.  Running toward exit node.  Okay.  Awesome.  Number six, view find.  Folks interested in view find? Okay.  Thank you.  Number seven, hashtag MashCAT? I don't know what that is. Maybe I should go to that.  Number eight, VIVO/linked data/Gitfor data.  Nice and high.  Okay.  Thank you.  And number nine, archive space APIs and integration you? Okay.  Anyone interested in archive space, APIs, and integration? Excellent.  It seems like a Carol even distribution. That's awesome.  Thank you.  One last thing before we start, I just want to announce some of our sponsors, and I also wants to say, you know, basically, a big thank you to our partners.  Without con Sentra event management, really, this would be a disaster.  On the contrary, it's going very smoothly.  Mice and loud so they can hear you out at the registration desk. [Applause] also volunteers.  Speech running around and sometimes missing great presentations to make sure things run smoothly.  Nice round of applause for the volunteers, please. (applause.) and finally, I want to read the list of bronze sponsors from the podium, and we'll get started.  So bronze sponsors this year, villa Nova university library, or I guess Penn State university libraries, Rutgers school of communication information, the Princeton theological seminary, Ithaca, J store portico creators, UC San Diego library.  Archive space, DLS, by water, Nashville public library, try college libraries, NiNite pro, Thompson routers, and the cherry hill consulted.  Thanks again to our bronze sponsors.  And [Applause] And just so I know, the general sponsors have a sticker so you can identify them and thank them personally, too, as you're talking and mingling. Without any further ado, I want to bring up the first presenter, David naw ton.  His presentation is Janice, Node.js happened her for all library services.  He'll pronounce it for you. Can everyone hear me? It sounds like it.  I am David naw son from the University of Minnesota libraries and this is my first Code4Lib talk.  Excited about that. [Applause] Thank you.  And I am here to talk about Janus, as Sean struggled a little bit like people often do pronouncing that.  I have a coworker who is a little bit of a linguist and he was admonishing me for pronounce it go that way, because he thought it was an Anglinisatien. I guess it should be pronounced Janus.  Good luck with that.  I think people understanding what you're talking about, and I'm not sure exactly what's going on right now.  Apparently I need help, so thank you you for that.  Okay.  There we go.  Okay.  The slides for my talk are out of Lean, if you'd like to follow along. I'm actually a little surprised to be here, because Janus actually is, as you'll see, I think a very small application.  It really doesn't do much.  I've been a programmer for a long time, but even though I've worked in academic libraire toys a while, I still feel like there's a lot that I don't know about economic libraries.  And some of my coworkers suggested this idea and I wrote this little application.  They said, you should submit a talk proposal about that.  And I thought, really? And apparently some of you find this information, because the proposal was accepted. And so the problem, apparently, that a lot of us have is we have lots of different search targets for our library search forums or just the library services.  Excuse me.  And this was especially a problem for us, because you may have heard about our library data and student success services, and I see there's a lot of interest in that sort of thing, gathering some statistics on uses of libraries and tying that to student success to demonstrate the value of libraries.  And so we had this problem where we'd really like to be able to gather some statistics on how students are using the libraries, but how do we do that? The services are going off to so many different places. So one solution to that, we've seen a great talk on it already today, is like a Bento Box style search.  One search forum to rule them all.  And one example is a quick search from NCSU, which I think is a really nice project.  I'm impressed with what they've done there.  And I know there are others.  We were talking about Slack.  There's Jonathan Rotchking.  I hope I'm pronouncing his name correctly.  He was mentioning his Bento search.  I'm sorry I didn't link to that.  And it's a really nice projector a nice approach, but it may not meet your needs, and what if it doesn't? So we decided to do something different, an inversa preach, and we had one handler handling the search forums and that's what Janus does. Now I have to apologize.  This is, without question, the worst, most horrendous dry Graham I've created in my life, and so many of you have given beautiful presentations.  I can tell a little bit of a story here.  We actually are working on open source in Janus right now, but when I submitted the proposal, I thought I was going to have months to work on it, but then we were hit with, like, a surprise critical ambitious deadline project, and the deadline was right before this conference.  And I valiantly, maybe unwisely tried to get the code ready to release as open source before the conference, but that was a bad idea.  So I was frantically finishing my slides trying to get ready for this, and that's the best I could do. This is the simplicity, the application really doesn't do much, yet on the left there is an example of some of the many search forums we have throughout our many websites, and those are all the different targets.  Corresponding sort of horizontally, where all of those forms are going, and then the -- what we do, then, I think what's probably most interesting, I understand, to people is that we have this one point where, yeah, we can collect all that search data.  And I know that you may be wondering about that with respect to Kay crews' talk -- Krause's talk, but wait for it.  There will be more about that later. I think this had some advantages, like the Bento Box style does, too, because in both cases, we have a single code base for maintain.  It helps to make it easier to protect privacy and enforce security.  You have fewer places to look for bugs, apply security patches.  And just in light of what Kay was talking about, one thing I'd recommend, something that we do, is all the deleting of that search data.  And right now, Janus is logging that data to text files and we used log rotate right now just to automatically delete it, according to how long we want to keep that data. Okay.  So another benefit that we got, and one of the things that led to this project, is that a lot of the targets, especially some of the vendors' search engines, you know, require us to construct some really nasty URLs, and this is one of them.  And that's even incredibly -- that's like the simplest *est search you do, really, so littered throughout our code were URLs like this where previous developers had tried entering some services and these vendor engines and just copied and pasted, and they were even worse than that.  That's what I did for this project to try to pare it down and make it as small as possible. So one thing we did with Janus, we re-ate a simpler URL API f that's not too pretentious.  So now that's what a Janus target looks like.  And we can give that to some web developer or designer who may want a special, you know, search forum for their collection.  To get that right, okay, so I'm running low on time.  I'm going to have to go through this real fast.  Small and simple, it's actually not that interesting to talk about.  Looking at sources, I'll talk about some of the challenges I face.  First, you've done a lot with peach.  We've tested that these URLs were actually giving us positive search results, and I tried to do that with the guzzle HTTP client library and I ran into JavaScript problems.  There were some vendor problems that required JavaScript to take place and that didn't work.  I've been pant to go play around with knowed S.  Why -- Node.js.  Why don't I give that a to the? except some people, when confronted with a problem, may think, I know, I'll use Node.js.  Now you have an infinite set of problems.  And so challenges related to Node.js, that's the rest of what I'm going to talk about real quickly.  So the first first, you know, just installing it.  You may be able to use an OS package, but if your institution is like ours, our SYS ad minutes may not like that.  It's something unofficial -- admins.  I use an installer.  There are other ones that is an easy way to install it.  Another one is that Node.js is not a robust HTTP server, even though that's really what it does.  You need a reverse proxy.  Most common recommendations in Genix, but you still need something else to keep node itself running from it goes down, because it doesn't do that on its own.  So people recommend forever or supervisor, and that may seem daunting.  And naturally, you amend if there's a rumor, I heard, and it doesn't surprise me, that OIT it exclusively stated we are not going to support again.  NIX. so our solution was to use an patch and I partner.  We're already familiar with Apache running and passenger.  There's a link to a tutorial using Node with partner.  It supports -- with partner.  It supports it really well.  Even though the tutorial is good, there are some things that are not documented that I just had to figure out based on what it does for rails.  There are some good configuration options in here that are undocument, but I just tried them, trying to infer what it would do, and it worked. okay.  And I have hardly any time left.  I was going to talk a lot more about challenges with Node, so I'll really request quickly go through some of this.  A big one is an sink Ronnie, and I thought that -- asyncrony.  I thought that was not going to be a big deal, because I've done functional programming.  Callbacks? Yeah, that's fine.  But I was surprised at how challenging it was, so I'd recommend reading that last article there.  And I'm almost out of time.  There's a lot more information here that you can look into.  And come and talk to me about it. [Applause] Thank you, David.  Okay.  Next up we have Getty Research Portal Reboot with Susan Ley and Adam Cahan. Howdy, Howdy, Howdy.  I'm Susan Ley, junior software engineer at the Getty research institute.  This is my first packet Code4Lib and my first time talking in front of people, so thank you. [Applause] I'm here today to talk to you about being the new version of the Getty Research Portal, an application originally written in Solr and Java, which we are reconstructing with angular and Elastisearch.  There we go.  All right.  So some of you may be familiar with the portal.  Others are surely not.  What is it? It is a meta data search aggregator, providing access to digitized art history text.  It's comprised of catalog records with meta data with links to full digitized sources hosted by one of our 18 contributing institutions. So because the cataloging for the books in the portal is performed by the libraries, which has the actual physical copies, its records are authoritative.  In addition, the texts are free, downloadable, and entirely digitized according to international standards.  The portal's contributions have grown steadily and as its scope continues to expands, we do hope to include more resources and in non-western art. so if the portal has already proven itself to be a widely used application with extensive search functionality, why throw it in the trash and begin from scratch? One of the answers to that is maintainability.  The original portal was completed before all but one person on our current development team even began employment at the Getty, and that one person is retiring in June.  As you can imagine, the departure of two of the three original developers has considerably limit our ability to alter and enhance the code base without catastrophically and enigma tickly breaking core functionality. Now let's talk about design.  The original pooral actually looks pretty good and it looks pretty well.  What you see here is the desktop version.  Not bad.  Right? Now, what you see here is the original portal on iPhone.  This is a beautiful example of responsive design.  Just kidding.  I don't know about you guys, but I had lasek surgery, so I basically have Eagle vision, and the only thing I can see there is the logo. So as more scholars and users in general like the shift to conducting research from their mobile devices, the need for the port to retain core functionality, while also maintaining user interface is crucial, and it is something that, as you can see in this side by side comparison of the old and new you version, we do feel we have slightly improved. So what is angular JS? What is Elastisearch? Why use them? I'm sure many of you are familiar with one or both of these.  I'll touch on the former very briefly.  Angular JS is a Java framework developed by Google.  It is open source, so it can be freely used and changed by anyone.  Also, the framework itself comes with a few key benefits and is hugely popular right now, so there is a large community of other enthusiastic developers out there working on projects. one of these benefits is did he perch dense I injection, which my colleague will touch further on when driving into the architecture of the project.  Secondly, one of the most useful features is two way data binding.  So this keeps the model and the view in sync at all times, a change to the model updates the view, and similarly, a change to the view updates the model. Testing is another area where angular really shines and testing and unit testing are both done very easily.  By the way, the components are broken up.  And for the appreciators of model controller framework, it is easy to develop your application in a clean MVC structure. Lastly, angular is hugely useful for controlling elements of the dom, using directives and other angular features.  If you would like to see the plethora of websites out there built by angular, you can.  Made with angular.com as a list of notable projects, including Pay Pal check out, Weather Channel, and healthcare.gov. Finally, Elastisearch.  How do you use it and what have we been able to accomplish with it? Who's to say necessarily really.  In all seriously iness, if you dive into the architecture of the Newportal and answer that question, in a few more I'm going if hand over the reigns and introduce my colleague and the master architect of this project, Adam Cahan. [Applause] Cool.  Hi, everyone.  Thank you, Susie.  The Eagle slide is always awesome.  I'm going to first talk a little bit about our angular architecture and kind of at the beginning of the app and dive into our journey with Elastisearch. Very quickly, we like angular because it's popular, because it has a lot of structure.  We like Elastisearch because we're doing some other projects coming down the pike with Elastisearch and we wanted to try it out, because you don't have these.  Java you can write a bunch of funky things on instead and it scales.  With Elastisearch you can just keep adding stuff to it. So we decided to follow a sale guide and pattern for angular that's been endorsed by the angular teaks, written by someone named John pa pa, an angular guru and well-known in the community.  The link is up there.  I highly recommend checking it out if you are interested in doing an angular project. The main thing we liked about this is the the way that, like, services stay Lean, controllers stay Lean, stuff you is structured in the project direct at this, and it just makes a lot of sense and takes advantage of what angular does. The big thing, you can see we've got a couple layers to the application, the controller layer, which is attached to pages.  A service layer, which this is an earlier version of the app a couple months ago, which has search service.  Search service is our conical of everything searched.  Search results, search options, all live in search service.  When a controller needs to know what's going on in search, it talks to search service.  When it needs to execute a serve, it talks to search service.  Search service talks to a lower level service called data service which interacts with sources.  They build the Elastisearch query.  It's passed to the client, which is an even lower level service, so we ever three tiers here.  The S client actually uses the official Elastisearch JavaScript client, and they actually talk to our Elastisearch cluster. What we like about it, its architecture scales nicely.  If something changes at a lower level, it doesn't affect stuff at a higher level.  Our application has grown.  We've added a bunch more stuff, a bunch more pages.  Sorry.  I'm checking the time.  And as you can see, the architecture holds.  Search service is still injected wherever it's needed.  We have a facet that opens up.  Sorry, a modal that it opens up to look at all the different facet options for our facetted search.  Search service gets ingested in there.  We have advanced service.  It gets injected in there.  Elastisearch grew so much we broke it into its own service, this is lower, middle tier, and we added a whole new feature.  We want you to be able to save individual records that they're interested in and save services.  Right now we're using browser local storage to implement this, so we wrote another low level service called storage service, which interacts with local storage, and then a save records service, and we're using data service basically as a pass through layer. the cool thing about this is when we create user accounts and have stuff stored service side, I don't have to touch saved record service.  I don't have to touch data service.  If we write better tests than we have now, we're not there yet, and really mock our services with our angular tests, we wouldn't even have to touch the test.  Right now we would. We would just create a new remote storage service at the low level, swap it out, change data service a little bit, and the rest of the application stays the same with regard to saving records.  That's the cool thing about this kind of architecture, for those of you who have seen it before.  You know that this happens on the service side a lot, too. All right.  Elastisearch.  So with Elastisearch, we did create our own mapping.  This proved to be really necessary to implement a face facetted search and also return mark records for users.  I'm just going to sort of walk through a search and talk about what Elastisearch is doing. So I just searched for art.  We've got a bunch of results.  You can see we've got about 32,000 records.  We have our facet options on the side, type, and creator.  It's a little hard to see, but there are the number of records associated with each facet option.  I'm going to go ahead and filter by creator.  Now, we're filtering by the American association -- the American arts association.  Excuse me. So our result has shrunk, which is expected.  We've also filtered our facet options.  Ads you can see with text under the typed facet category, there's 523 results before there were tens of thousands.  If you check out the creator facet, the facet options there do not get filtered.  Originally, that would happen, but we didn't really want this, because then if I click on American art association, all the other creator facets just kind of disappear.  We want logical or behavior within facet categories, so that if the user picks on another facet option in creator, the results actually expand.  Then when we click on a type facet, that shrinks again.  So to do this, originally we had one big plastic search query with the agregations, which gather facet options, as well as the filters that apply facets to filter the search.  We had to break it into two separate queries, because in Elastisearch, agregations are scoped to the filters in the query.  Our aggregations were getting filtered by the filters that were applied.  We have one query to get the actual results and another square toy get our aggregation. We then build a set of filters for each individual aggregation.  So right now, for the aggregation that gets our type facet options, the filter says American art association or metropolitan museum of art.  For the filter that gets the creator option, it just says type.  So that was one of the big changes. I want to give a quick shout-out to our team.  There are a bunch of us working on this.  Our team leader was Josh Gomez, who was working on this for a long time.  Also quickly, Alex Rosetti, who wrote a bunch of python.  Talk to any of us if you have any questions. [Applause] Thank you, Adam and Susan.  Next up we have D Andreas Orphanides, and he's going to talk to us about architecture as politics, the power and perils of systems design. Why is my slide not open? History.  There it is.  Glad to see this is not counting towards my time.  Arrangements.  Mirror, unmirror.  All right.  This, this, and then do this.  Can I get the monitor up, please? Over here.  Presentation.  There we go.  Okay.  Now my time is gone. So a couple years ago I was teaching a systems analysis class, and one of my students came up to me and he said, Dre, I was thinking that sis temperatures is a really powerful position because the design of the system influences so much about what users can and can't do.  And I said, yes, that's right.  And that's why we're going to do a whole unit on design ethics.  And he said to me, design ethics is a thing? Hence, the alternate title for the presentation. Whether you're a web designer or metadata person or a manager, you engage with systems design and you make ethical choices about design all the time.  So what I'm going to do today is run three key lessons in the ethics of systems design.  They're more like uncomfortable truths that we need to come to terms with if we're going to design ethically. Lesson one, systems design influences user behavior.  Reading this, your reaction is probably something like, yeah, okay, because you can obviously control the user by containing them overtly.  Right? But there's a whole school of design that's about encouraging the user to make certain choices, sometimes kind of subtly.  It persuades the user rather than explicitly controlling them.  What are we talking about? Design choices where we steer the user towards or away from particular choices so that we might want them to make. this can be overt as in the practice of disciplinary architecture.  This is where you control people's behavior in public places.  Here you provides constraints that don't absolutely limit the user's choice, but they aggressively discourage certain behaviors that's considered undesirable, like the fin yells on this fall that prevent littering or the anti-homeless spikes. Design can also persuade users in more us the and nefarious ways by exploiting cognitive biases to encourage users to make the designer's desired choice, even if the user doesn't intend it.  On the web, these are sometimes called dark patterns, and so here's a couple of examples.  Here's one from AVG software install, the notorious auto opt in check back where a user makes a selection on the user's behalf. Another dark pattern on the same page, the check boxes here represent a mixing and matching of constraints and afford'ses.  Just one of the check boxes, the one that's marked, is required and the rest are opting into okayalal services.  So you have a constraint hidden among afford'ses. The end result you is that the user is likely to opt into more services than they really need, even though they might not actually intend to do so.  There's actually so much devious stuff going on in this page that I can spend an unreasonable chunk of the presentation taking it apart, but I'll spare you.  If you're interested, see me later. Here's another example of dark pattern.  This is a fair search page from British airways, and in this example, the designers are mill leading by using visual cues to steer the user toward options that are not actually best for the user.  So I'll zoom in.  You might notice that one of these is labeled low some of the has some color-coding, but it's not actually the lowest priced option.  Right? I don't know how they think they're getting away with this, but there you go. Persuasive design can be used to benefit the user, too.  If you've used an ATM recently, as you get cash out you'll notice it gives you your card back first and then gives you your cash.  This is to help you prevent a post completion error where you get the cash and your mind goes, okay, job done, and you walk away without your card. Early ATMs used to work that way and a lot of cards got lost, you might remember.  So this is another example.  These are shark tooth stripes that you see on the road here.  They get closer as the driver approaches a dangerous road segment, and so it gives a visual illusion that the driver is accelerating with they're not actually accelerating, but nevertheless, they instinctively slow down. Even something as simple as a spinet on a page can help alleviate frustration and confusion.  For example, close to home at the NTSU libraries, we have several different search options, but we make the all one, the most comprehensive one the fault to alleviate confusion.  The person doesn't know what the best place to search is. The uncomfortable truth is design persuades, whether you intend to or not.  For instance, if you want people to click on links on the website, you need to make them look clickable.  Here's a quick quiz.  Which of the things on this page are quick targets? I will leave that to you as an exercise. Calls to action, likewise, need to be in the area of an attention or they're going to be missed.  So this is a site I have actually occasionally shopped at.  When they did their site redesign t took me five minutes looking around this page to figure out how to buy something.  The add to car button is pretty big, but it's at the bottom of the an page in a fixed footer.  Despite being pretty big, it's actually easily mixed.  If you don't see it, it's down in the bottom right behind someone's head. So here is an appropriately focused or appropriately con strained interaction.  It's unclear what information is being asked for, where the user is in the process, and how to proceed.  Yes, I am complimenting pro quest design.  Don't get used to it. Design persuades, and this, in itself, extends an ethical dilemma.  Design can persuade for good or for ill in intended and unintended ways.  So here is a few principle to his guide us toward ethical design.  First, provide accordances and impose limits in a way that benefits the user.  Second, ensure the users can recognize their current options.  And third, if you're going to disguise -- if you are an he going to constrain the user, don't disguise that fact without a very good reason. Lesson two.  System design reflects the designer's values and the cultural context.  Design is inherently political.  This is something that came up in the keynote.  This can be overt or subtle and it can manifest in many aspects of the design process.  So here's a fun fact.  The New York parkway system, as you may know, has infamously overpasses.  These were actually designed as a form of social control.  By making the overpasses too low for the transit buses, the designers limited the availability of the parkway system to low income users who couldn't afford cars by design. This is an example of something that this paperwork calls architecture as politics, design choices that act as a reflection of the designer's value systems.  As we've just seen, this can be overt and deliberate or overt and just tone death.  In 2001, a condo opened up in New York where all the subsidized units could only be accessed through a side alley or a side door in the alley and the full price units you could get to from the front entrance. As we've seen, design can send a clear message about how designers value or don't certain users who interact with their designs.  The names we give to categories, the permitted values in a form field, the relations we define in databases all have political weight.  Consider this simple example.  Does your state's marriage license say husband and wife or does it say spouse one and spouse two? Your metadata schema is a social justice issue. You probably all used an accessibility ramp at some point.  Some of us may rely on them every day.  Yours are perfectly serviceable, imminently useful.  But what message does the ramp's design convey about accessibility? Is it something off to the side? Secondary? Afterthought of necessity? Or is the method one of inclusion and integration into the mainstream? Your design choices reflect your values, even if you don't intend it.  Consider this common problem in responsive design.  When you switch to narrow view, you need to stack order your site features.  So what goes on the top? What is your institutional priorities? Earlier, I complimented the design of this interaction here from breath works, but what is the information that's being requested actually have to do with using breath works.  If you can't see t you're talking about name, institution, and institutional role.  None of the functionality depends on this information, but you I can't bypass this screen without entering it.  Pro quest requires that the user share this information, because presumably, the metadata is of some institutional value to pro quest.  They've chosen their organizational interest in using demographics over user's potential interest in not sharing. The counter point to KPOR's architecture and politics is, perhaps, predictably politics as architecture.  That is, your political and cultural context will influence your design.  So here is a simple example from my neighborhood.  This used to be a perfectly useful line of parking spaces, but then some local political squall about property boundaries came up and a fence went up, rutting in fewer parking spaces, plus a totally useless strip of pavement.  If some system decides that your name is inappropriate, let's say it contains the wrong sequence of letters, you may find yourself blocked from participation.  This is called the Skunthor problem after a name in England whose name gets blocked from web forms for this reason.  Then how do you provide your address? It's also possible for political influence to result in good design.  The DC metro map is often hailed as a triumph of a design, due in part to a clever use of markers of local significance in a design.  It's got the beltway, Potomac river, and boundaries.  For better or worse, these boundaries distinguish local communities, so their inclusion on the map provides useful markers of social context.  Here the political ill positions of the larger system enhance, rather than limit the design. And once again, we have come to the terms -- we have to come to terms with the uncomfortable reality that we are all.  Us biased politically and culturally.  Our own perspectives influence our designs and our cultural context inform and constrain what and how we can design.  And to an extent, this is beyond our control.  We can't avoid the influencer of culture in our design, but by recognizing it, we can try to manage its effect.  Here are some principles for designing ethically under the influence of politics. first, actively attempt to identify and recognize your biases and how they might be influencing your design.  Second, design is to encompass users as universally as possible.  Not only in testing your systems diversely, but also by recruiting developers with a diverse set of skills and experiences.  Finally, attempt to recognize what sorts of controls your culture or context might be exerting on your designs, either explicitly or implicitly. Before we move on to lesson three, let's do a quickly view.  First we had system design influenced systems behavior, whether you wanted it to or not.  Systems design reflects your cultural context and your values, whether you want it to or not.  Ill police knit both of these truth is our third lesson, which will state it explicitly.  The interest in values embodied in the system that you design will sometimes conflict with the interest and values of your users. Consider we often design by the 8020 rule, also called the Prado principle.  It's a common sense Heuristic that says you should exert where it's likely to have impact.  A key way to represent this, 80% of your system usage is going to come from 20% of your users.  Thus, the PR ADO shows you should use majority of your work supporting those users, but that means 80% of your users are only getting 20% of your development time.  What does that mean about the value of the majority of your user base to you as a designer? Another conflict that emerges this, time on the open web, it rises from the role of advertising and site funding.  So if you're using a site that's a for profit site, advertising plays a Rome T manifests acutely in how site pay lids are distributed between contest and advertising.  Some popular websites have a content advertising ratio of about one to five.  That is for every megabyte of article you download, you get 5 megabytes of adds and tracking JavaScript.  What does this communicate about the value of the user relative to the value of the advertisers? here's an interesting three-way interaction.  To travel the U.S., we have political structures that end up requiring traveler names to match your government issued photo ID exactly, which is what it says on the traveler information form that TSA requires when you register for a flight.  This particular traveler's culture identifies information in Arabic or [indiscernible] but the system designer arbitrary at this limited Latin letters only.  What does this communicate about the value of the traveler in this situation? Once again, your metadata schema, or in this case your data validation schema is a social justice issue. Even with responsive design, which we've brought up earlier, mobile use of websites often incur design compromises.  Sometimes we give the mobile view less shrift, because it's a secondary mode of interaction. 15% of internet users depend on their cell phone as their main device and 70% are whole I reliant on their home.  How do we he know sure a meaningful web experience for this population? I guarantee you you have users in this category in your library or using your software, et cetera.  Your mobile website is a social justice issue. Once again, these conflicts can't be entirely avoided, so how do we minimize their impact? Obviously, no system that we design can be wholly successful and satisfying to every user all the file.  Every design choice incur compromises.  The important lesson to take away is this.  When you make a design compromise, you need to be fully cognizant of its implication.  With each chase you make, you should reflect on how it might affect your various user populations, even the most marginal.  Recognize that this chase may exclude some user segment or make their life in the system more difficult, acknowledge it up front, come to terms with the limits of your system, and then work to make it better. To do this, you must know the diversity of your users and their needs so that you can understand the implications of the choices you make.  Finally, and most of all, make these choices with empathy.  Reflect on the experience that you would want to have and strive to make that available to all of your users.  Thanks. [Applause] So I can't take credit for most of the ideas in this talk.  They come from lots of different places, especially the pictures.  So I encourage you to take a look through credits and also read the articles that I went through there, and I sent out the link to the slides on the media of different times.  So I encourage you to take a look.  Thanks.  Questions? I'm not sure I'm the best authority to answer these questions, but if you have questions, I'll do my best.  Thanks, everyone. Thank you, Dre.  Next up we have Sebastian Hammer with constructive disintegration.  Reimaging the library platform as microservices. All right.  HI, everybody.  I am going to do something a little bit unusual.  I'm going to talk about a project that we are involved in, a collaboration that we've become a part of at index data that up until this morning, we didn't really speak about in public.  It doesn't have a website T did you want even actually have a name, but despite that, we also felt that there was really no way we could come here today and not talk about that project.  So that's what I'm going to do. I'm Sebastian, cofounder and partner.  We have been around for about 22 years.  With we're a group of about 20 librarians and developers, kind of con founding each other and doing projects together. Of particular focus over the years has probably been mostly on moving data around, doing systems integration, making systems talk together to support various types of ways that libraries might want to talk together and work together.  And in doing that, I have probably spent a disproportionate amount of time in my working life, which is really all my adult life I've spent in this business, being frustrated about library systems, and in particular, integrated library systems which hold a lot of the data and information that we want to work with in libraries and sort of gatekeepers for a lot of interesting things that we might want to get to.  And there are places of systems of software that we might want to use to manage various bits of information and importance, various funks that are important, but they can be hard and difficult to work with. I was lucky enough a few you years ago in 2009 to be invited to give a keynote address.  That's one of the cooler things that I've done in my working life, but I felt frustrated afterwards, because I think I growsed about these issues and complained about them as I do, but I didn't feel like I really had a lot of constructive alternatives to offer.  I think at one point or another I said, I think we have to enable libraries to form up into giant robots of interoperability to do cool stuff.  But I was very light on the implementation details. Beyond that, over the course of the last year or so, this opportunity should try to actually do something about this.  And that's what I'm going to talk about today.  So the background for this story you begins actually also in 2009 with the formation of the charter of the Qualiet group of liies.  They -- libraries.  They came together with a vision to create an open software platform in which libraries and software vendors and others could collaborate and work to build better software, better systems for libraries.  And that vision was inspiring to me, and I know it was inspiring to other people, but there were certain issues in implementation of that vision. One of the things that happened was in putting together a system that could ultimately replace an integrated library system in the library, there was a lot of focus on gathering requirements and figuring out what those systems needed to do, and along the way, I think perhaps sight was lost of the platform aspect, this need to build something that was beyond just an ILS.  So in fact, what they ended up with, to a certain extent, was another ILS, and that was an opportunity lost. Another issue that arose was that there was a choice made to use a platform and a basis for implementation that was cumbersome and would make it difficult over time to attract people to the community to contribute, to it change and extend it.  What did happen, though, was that it inspired people, inspired myself, inspired others.  They attracted, amongst other people, EPSTRIL into their collaboration.  EPSTRIL was looking for ways you to stimulate innovation, to discover new types of services and activities they could engage in.  They saw open software, in part because of Qualiet as a good way to pursue that.  So EPSTIRL joined Qualiet and also contributed to projects and what happened, then, was that Qualiet and EPSTRIL came to the realization that the software platform was problematic and something different needed to happen.  So it was at this point that they approached us and said, would you do -- would you be interested in having a collaboration with us to create a new platform based on the principles of the original Qualiet project, but based on more current practices and something that would be more aggressively engineered, perhaps, to form the kind of community that was hoped for. So that brings us to your specifications.  We were asked to create a platform that would be easy to extend, that would be actually fun to work with.  And this is part of the specification that sold me on the project, because that's the sort of software that we like to work with ourselves.  It's the kind of thing that we like to build.  We were looking at something that would be licensed in an open way, so we are looking at using that Apache two license, so really inviting everybody from libraries to individual developers to commercial companies to come and play.  Work with the software.  Extend it.  Host it.  Do whatever they want. We wanted a piece of software that was designed to be cloud ready from the beginning.  Something that could be hosted in the cloud or locally in a multi tenant mode so the library or library network or commercial company could host the system for many libraries at a manageable cost.  You want something that's built around an art space, because we wanted to explore things that we could do in terms of providing better electronic and other types of resource management and knowledge spaces are siege as a de-element of that. -- key element of that.  We wanted to explore how linked data could be integrated into the guts of the system to actually bring value, not to use it or employ it uncritically, but to look at real opportunities to make a difference.  We wanted something, again, that could be hosted by commercial lenders, but also something that the library could stand up on its own.  We wanted to build the software to be hosted, for instance, on Amazon web services, but we are doing integration testing on [indiscernible] because it helps keep us honest in terms of scale ability and in terms of modularity. And finally, we wanted to be community-based, so we wanted to really try you to engineer and to behave ourselves in a way that would attract the most possible people and organizations and groups of people to this project.  And I'll speak more about that.  We wanted modular, so we wanted to not just build another sort of monolithic system based around a known set of requirements for what a library needs to do.  We wanted really an operating system, a platform in which you would plug in functional units that you needed, something that could be extended over time, something that will be flexible and could grow sort of horizontally and vertically.  And I'll talk a little bit about how we imagined doing that. So we start with an empty box, and this is our operating system or our container, the idea being that this is a system that runs in the cloud on your local server, and you can load it up with types of applications or module that his suit your requirements.  So this might be your cataloging module.  Then we add in a thing that we kind of call an app store, although we are very careful to do the [indiscernible] whenever we call it an app store.  Although I think the idea and the concept is inspired by what you might see on a smartphone, it also is a different kind of beast you see in a hosted environment run by some library or commercial entity.  We wanted this notion that, again, you could start with a Blanck slate and then install just the things you wanted and if you don't like our cataloging module, you might install somebody else's.  You might have two or three different ones based on different sets of requirements. Then you add in other types of modules, other functions that you might need.  Patron management.  You might want to use circumstance other you layings.  You might want electronic resource management.  Resource management more generally.  So we have a real ambition in this project to try to make a fresh look at some of these functions and maybe, in particular, acquisition and see what we can do in terms of bringing different modes together in a meaningful way.  But we don't want to stop at just looking at the sort of traditional and core library functions.  We'd like to kind of broaden or push open the cubby hole a little bit what have it means to provide library services.  So the project is initially, at least, focused on somewhat of a research library area or that slice of the universe.  So we want to be thinking about what are the types of things that you might add to a system like this to help the library extend its footprint into the academy and into parent institution.  So you start to think about, might somebody build citation management that's a part of a library system? Would that be meaningful? Might you have research information systems? Might you have an institutional repository? A lightweight or heavy weight institutional repotty Tory built into this -- remonths I tore Toya sift in building that? Might you have a publishing platform to make it easier for a library to assist? Research has been publishing things.  Might we do something we haven't even thought about yet? Many times, from a platform perspective, begins to kind of open a conversation about, you know, what are things that haven't been tried before? We'd like to think of this in a way as an incubator or as kind of a maker space for new types of library services.  By in addition to these kind of complete applications, complete functional areas, we also see this, quote/unquote, app store as being a repository for modules, for plug ins, for extinction points, because one of those big frustrations that we have coming in was how hard it was to get a traditional monolithic library system to talk to other things.  So we wanted to use this as a mechanism by which folks can build plug ins or gateways or blue layers, add them to the app store, enable other people to benefit from them over time. so we wanted to be a maker space, incubator also for integration points.  This might be between your acquisition system and a bookseller.  This might be between your circulation system and a resource sharing platform with someone.  So that's another key area.  And then finally this, kind of gives us the basic universe.  And so in order to have this also appear to be a nice and a pleasant system, we want to have a common -- we're prototyping a sort of UI tool kit based on reacts.  We're working on our team with UX experts to try to make sure that what we produce in the end is not just an attractive sort of architecture, but also something that is meaningful and easy to use, something that's designed with the end use, the librarian in mind as the user. And then the final piece of the puzzle really to make this a platform on ecosystem for these apps, we want to provide a set of kind of system services that are storage layers for different types of data.  The idea is that if rather than building this as a kind of a conglomeration of business logic wrapped over a relational database or whatever, we provide a set of domain data models and storage layers to manage those.  We can hide a lot of the complexity of multi tendency.  We can provide some shielding between the application code and the database, and we haven't the means to replace the storage layer if we want to.  So if somebody wants to use a triple store for their catalog, they can do that.  If somebody comes up with a better type of database engine to use for something other down the road, they can do that, and we can version the interfaces to these things so that there is control and we can approach and collaborate around this platform in a distributive way without getting lost in the weeds. The last bit is thinking about building a platform like this.  How do you do that? What sort of approach do we take to make this fun and easy for developers to work on? And that's when we end up -- that's when we come in where the language was.  We've got about, you know, as I said, maybe ten developers in the company and probably about ten different opinions about what would be the perfect programming language to use for this platform.  So we got into a debate about programming languages that lasted probably about a month or maybe two months.  At one point in desperation, I went to EPSCO, because EPSCO is provide ago lot of funding for this project and I said, can you mandate something? Tell us anything.  It could be Java enterprise, because then it's your fault and it's not my fault you.  They said, no, no, you figure it out.  Just hurry is up a little bit. And this reflects the fact that, you know, to their credit, they really don't tell us what to do.  They ask us to figure it out together with the community, with the libraries.  Just kind of get a move-on things. So what we eventually realize school district there really wasn't any single choice as far as programming languages or development staff that we could mandate that wouldn't annoy some people and that wouldn't be completely obsolete most likely in ten years.  So we decided to kind of turn it upside down and not mandate a single platform at all, and that's when the microservices idea comes into it.  The notion that each.  These modules, each of these applications can potentially can potentially be developed in a completely different stack and what ties them together are the interfaces, so we Lean on interfaces that we version, that we manage very carefully, and the implementation of different modules becomes a choice of the developer. So in a sense, the box falls away and all we're left with are these modules that are collaborating.  And some kind of interesting things happen when we approach things this way.  One is that we end up with very few big pieces of this.  There's really no part of the base platform that's so big that we can't throw it away if it doesn't suit our purposes, and we would imagine that if this platform was successful that the choices for the you feel I layer, for the different storage layer options and so on, they will change over time.  What will remain are the interphases.  The interphases and our agreements about what constitute good interfaces between components is what's important here. The other piece that's interesting is that you can start to imagine these components actually hosted and provided by different people and by different organizations.  And you really start to imagine that if we focus on these interphases and the domain data models and the boundaries between components, we need -- we can really kind of split what used to be a very integrated and monolithic function up to be something that just kind of comes from all over the place, and to my mind, I think that reflects really the idea of what putting this kind of function into the cloud reflects T reflects what we see, I think, with other types of services that line up from web based providers, and I think it's a really interesting perspective in some ways, although we don't call it an integrated library system.  It could also be a kind of transition phase away from having that function be integrated at all.  The universe that we can imagine is one where you just kind of buy the different functions.  You combine them when you need to. So that's a key.  The next key is the notion of the community.  We are hoping that by having a platform that can beam out a lot of different things to different people, in libraries, in research libraries, in other kind of libraries n other kinds of institutions that they can attract the critical mass, not just of developers, but institutions and companies to actually make this sustainable.  To create a larger conversation about library services, about work flows b how things worked together that I believe the space needs, because libraries are pushed.  They are threatened by budget cuts.  They are threatened by different models of accessing content and information.  I think there is a need for innovation and in my own sort of personal appearance, an open source project is something that can bring that kind of conversation.  So one of the big roles of Olay is to help stimulate, to help drive that conversation or to help build that community.  You don't have to be a member of OLAY to join and to play with this.  All you have to do is show up and be interested.  Be part of the conversation.  Maybe contribute code.  Maybe not.  So that's what we're hoping to accomplish. So as I mentioned, there is not really a website as such.  There is a place where you can go and put in your e-mail address if you'd like to be in the forum.  Once we put the website up, you're welcome to contact me.  I want to give a shout-out, also, to Michael Winkler, the managing Director of OLAY, because he's somebody who can answer a lot of questions about this.  Are you actually here, Mike? Wave your hands and make yourself known.  Somewhere hiding.  Behind the light.  And Christopher Spalding from EPSCO.  The two of them are sort of my partners in crime.  Feel free to look up any of us here and ask questions.  Send e-mails and so on.  Thank you very much. [Applause] Thank you, Sebastian.  Okay.  Our final take for the first part of the afternoon is -- will be given by Camille Sal ago, John Nelson, and Will Boyd, transcending traditional systems and labels, an API-first archives approach at NPR. Good afternoon.  I'm Camille Salas, product owner with NPR's research archives and data strategy team, also known as the rad team.  I am pointed today by Will Boyd, RAD team developer, and John Nelson formerly with NPR's digital media team.  We have a lot of ground to cover today so please bear with me as this moves quickly.  And you're more than welcome to visit with us after the presentation if you have questions. Our presentation today is about our current project to re-factor NPR's internal digital archive of broadcast, audio, and transcripts and before we delve into the technical overview of that project, I like to share a little bit of historical context about how this program came to be. NPR's archive started in the 1970s when we received a grant from the corporation for public broadcast that go allowed us to hire our first librarian.  I'd like to read a few sentences from the letter that kind of explains what our archive is like.  And little at the bottom of the screen.  I know that's been an issue throughout this conference. Let's see here.  It is our intention to make this library an active depository, which will make programs and program material available for production and broadcast purposes.  It is also intended to be an archive in the academic sense for historical and scholarly purposes, but this will be an evolutionary development.  And the phrase evolutionary development continues to remain true today, and I think our presentation will demonstrate that. So metadata has been average integral part of the archive since its beginning for many, many reasons.  Not only for the stipulation of the grant, but our metadata is used to create a quarterly report required for federal licensure.  If it wasn't for the librarians entering metadata about our on air content, there would be no report.  Without the report, there would be no NPR on air content.  Not only is there metadata used for federal licensing, but also used by organizational partners.  It's used to diversity of our honor serves, used for budgeting purposes, and it's also used for grant reporting as well.  So as a result of librarians categorizing shows like all things considered and morning edition, we've been able to find stories from our very rich 40 years of history through our current digital archives. However, a lot has changed since the 1970s in terms of technology, and in fact, we have adopted a strong and annal framework toward the way we conduct our cataloging, the way we do our research, and as product owners as well. So our current project came to be after two prefight races of the internal archive.  Around 2000, we had our first CMS and it was akin to an active database.  It was very time-consuming.  Reporting was very difficult and challenging, and in 2011, we moved toward our current archival database, which is called Artemis, and Artemis and XML transcripts that prepopulate and also delivers transcripts to NPR.org.  It goes through Artemis first.  Even though Artemis has met many of our system requirements, it's pretty inflexible, and for our growing archival and business needs such as an emphasis on new NPR content, including podcast and his video, and we want to think about future content that's coming our way.  Something that we like to call brain cast.  We just don't know what's coming down it is pike yet, but it's going to happen and we're sure it's knowing to happen as media is disrupted all the time. So our team came to the realization with our digital media partners that the current system was impeding us from moving forward and we also wanted to think about what else was impeding us.  And we had made this name change and rebranded ourself.  We no longer called ourselves library.  We called ourselves research and data strategy and we wanted our system to reflect that name. So our department learned a lot from previous implementations, and we understood that metadata report is gone a growing priority.  Our deadline driven environment means speed and flexibility is important to our stakeholders, especially our reporters.  And we have the skill and framework in place, in-house talent to create our own solutions.  So in short, NPR has very unique archival search and business needs and we wanted something that we could show the rest of NPR that we could adapt just as quickly as the rest of the organization around us has. So these are some of the requirements that we have kept in mind that ring true for our previous system as well, but you'll notice at the top.  Flexibility is our number one priority.  And I'm going to turn it over to yawn Nelson, the primary architect of the system, and we'll explain how we went about implementing it. Thank you, Camille.  I just first want to switchgears slightly and take a second to give a quick back story about the first time I met Camille.  She comes over to my desk and says, hey, are you John? Have you worked with Artemis before 1234 and I said yeah, I've spent some time with Artemis, working with previous iterations and doing stuff like that.  So she asked me, she said, can you come with me to this meeting I'm holding about Artemis and the things that I'm, you know, I'm kind of trying to, you know, give everybody an overview about Artemis you and what it does.  And I said great.  I go to this meeting with her that day and I'm sitting in there and she gets up and she introduces me as the expert on Artemis and says, now I'd like to have John come up here and explain the infrastructure in Artemis and how it came to be. And so to my surprise, I kind of get up there and, you know, it wasn't bad or anything, but I get up there and I give an overview of kind of the infrastructure and how, you know, what its current state is and that sort of thing.  And I use that time to actually pitch the new idea that I had kind of mulling around in my head about this API first application process that could basically revolutionize Artemis and its infrastructure. That's when I knew that she was going to be a great product owner.  She really went out and found the person that was the best at, you know, at what they did and had the knowledge of the product and, you know, and we're perfect for that.  So that's kind of where this API first application approach came from and where it really started out. And so from there, we really kind of started in 2015 with this idea that we needed to build something kind of from the ground up to kind of revolutionize the idea that our content and how it got distributed and how the build process was going, so we started with our data model, because that was the most important part of our system and how we needed to get it to move forward.  And the reason was because we had a lot of problems trying to get our new content into the system, and a lot of custom development and a lot of things just to get new programs or things into our API.  So that was one of the key parts that we started on with everything. So from there, we also went to -- let's see.  We started working with -- I'm sorry.  So from there we kind of, like, settled this analogy of a trapper keeper.  So with content, like, separated into the dividers and the folders and pages, and as that all kind of fit into our structure, and then we moved to relational data -- I'm sorry.  We moved away from relation remember databases and embraced [indiscernible] world, getting away from all.  This very structure data into something that was a lot more flexible that we could sort of handled this new and improved data model as we moved forward.  We used hypermedia as our object or our data structure.  Weighed lot of familiarity with this in past projects we worked at with NPR.  So again, we were kind of working on this model of code reuse and sort of this fast prototyping of application development. We also separated our UI and API so we could make UI changes without any real data model changes and that sort of thing.  So it made it much more rapid development environment than we'd previously experienced. So with that, I will turn it over to Will Boyd and he will talk a little bit about our [indiscernible] As John and Camille mentioned, I'm Will Boyd, developer for the red team.  I came you onto this project, and my first goal was to start the front end.  You know, before working on Artemis and being hired at NPR, most of my front end development experience was with backbone and J query.  I was a little intimidated.  I thought the docks were confusing.  And the lingo was really fancy.  And in fact, if you're just learning angular, I probably wouldn't look at those examples first.  After a few weeks of working with it and reading a lot of best practices, I realize that had an other you already is actually way easier than backbone.  It does so much more for you and it's really fast to put things together, especially forms and views. So onto our setup.  We set up the front end development stack with Yeoman and Jenkins.  We use get.  The code is pulled, installed, tested, and served automatically T only takes a couple minutes, and this has provided us a ton of value.  It's probably saved us dozens of hours of development time, because there's no overhead. Setup also means that I can install local copy of the app from any machine that has knowed in five minutes -- node and beat developing anywhere. Since we don't have a time on the front end, I'm going to cover a couple of my favorite features.  The first is every application state is stored in the URL.  It would have been a lot easier to not do this and just have the application state kind of relevant to your browser session.  And you might have noticed this with other sites.  This happens when you're clicking around on a website.  You'll save the URL and you'll notice when you go back to that URL that you're not actually on the same page you remember being at.  You might even be at the home page.  They don't store the application state anywhere else. so we took the extra time and code to make the URL reflect all of the application states that there are.  And that's been incredibly valuable for us.  I so on this slide, you can see example of a search URL, the hid Russ URL at the top.  It doesn't even really fit on the screen.  That's okay, because I can share this URL with colleagues and they can see how I built the search with the basic search terms, the dates, and also any of the advanced settings that I set up, because that's on the URL.  And also, when they come to this page, it will automatically search for them. . Here's another example of a story URL.  This is an archived story, and it's coming from the same search that I just performed.  If I can scroll down, I see the stories.  You see at the top we have the search stored in there, and that's coming from the URL again.  So they can go in and see the story or they can rerun the search, change the parameters, and an unexpected side effect to this that I wasn't really thinking about when we first started it is that this makes dealing with the browser history much easier, and if you've ever built any apps with JavaScript frameworks, you know that is kind of a pain. So the next feature, NPR is always adding new content.  This means that we have to be able to quickly support that content with our archiving.  We don't want to add developer hours when we build a new podcast, for example.  We created a CMS for the Cal an logger and it's powerful. This here is the editing view for schema.  The schema defines a content type, and then it shows the attributes.  And going back to that trapper keeper diagram, the contents match the diagram.  You can look at add, remove, and change the order of the attributes, but you can also organize them into groups, which she also has full control over, and that makes it easier for you to understand.  It also means that we're flexible in the future.  If you want you to change the way an attribute works, you can move it toward the bottom or even hide it from the user. We can also add content simply by clicking new and then I want to show an example of a show here.  So here we're adding a new show, maybe because we're creating a new podcast.  You have to create the parent document and then you're ready.  What if we want to start arrest diving NPR.org's metadata.  We can create extra fields on stories or a new content site that's specific for online only content.  We even started using API to create utility types like managed list, audio format, cataloger names, and role coats.  Those are used in conjunction with other content types to provide a rich CMS.  And the point is the type doesn't matter.  The API that John developed is extremely flexible and the CMS just lets us take advantage of it. So we've got this front end and this API that's kind of agnostic.  What about the stuff that runs on the server? Some can be put in the browser on the front end and some isn't suitable for the API, because it's agnostic.  This is logic that needs to be specific to Artemis and should be on a server.  We have to do authentication with private keys and we also need to log in with or company user name and password that.  Can't be on the front end, because our active directory requires permission to authenticate and we can't add everybody's IP address to that. So what we ended up doing is creating a proxy layer.  This proxy layer is really just an API in front of the API, but it's specific for our legend.  It's build with no JS express.  It helps with the stuff like EPI authentication and log in.  It's also grown a lot, because we keep finding more uses for it. it's been really useful.  There are a bunch of internal APIs at NPR, so we can put that communication on the proxy layers and we can catch the results if we need to.  We can keep private things that are meant to be private, private, and not put them in front of us. There is an example of pulling in information from NPR.org using our internal API at NPR.  We can also query and use our taxonomy.  Our taxonomy is stored in a system called MONDECA, which is cool, but it was never meant for realtime querying.  Querying for the taxonomy is slow, so we catch is on a proxy server.  When a ago an logger is going in and cataloging a story, they can use that to apply terms.  We also use the problem toy run bulk updates, get and set user preferences, importing serve results, and you don't want to run that many queries on your browser.  And we also browse the mounted file system to view our audio files. So migration hard stip.  Not everything is awesome.  It turns out it's been a real struggle for us.  When I say migration, I mean moving all the content from the my sequel database into the API for Artemis two, which is built out of last stick die an know DB.  It's mostly been a struggle because of volume.  We have over a million records that we need to my great, and because of the nature of the technology behind the API, that means we need to make at least a million requests to actually get the data into the API. So just to give you an idea of the time that takes, a sequel dump of a year of data in my sequel takes about a Midge.  Insert that go data into the API takes about an hour.  So we have over 40 years of data.  So that means if we miss a field or make a mistake and we need to re-my operate everything, it takes 40 hours at the least if everything goes well.  So that usually ends up having a full sprint for us, which is about two weeks. So there will be mistakes.  HTP calls and they get dropped.  There are always duplicates, so you have to think about that, and the list really goes on.  So my advice is if you're doing it in migration be don't under estimate the effort that it requires, because we definitely didn't. But because we work in an annal environment, we can adopt.  So Will Dave an overview of lessons he learned.  I wanted to provides a little bit about the product owner's perspective on things.  API has been awesome for us.  It meant our front end options were limitless.  We worked with our internal UX team to come up with an interface that Looks like and felt like N approximate PR, as you kind of saw on the leads to that were brought up.  Deploying the front end was really easy, so whenever he gets going, I can test and meet within minutes of him making an update, which is awesome, because flexibility is definitely what we desire in the roles that we have at NPR. Another thing I would say is documentation is key.  In addition to the trapper keeper model, we had a lot of decisions that we made at the outset of this project, and we go back time and time genere valuate and update that documentation.  It's been invaluable to us. The other thing that happened during this project was I had to take a brief hiatus.  Our awesome tax object mist, who could not join us today, she fills in for me and kept the momentum going on this project.  Additionally, John mentioned he's no longer with us, but was able to pick up the torch and keep going, Will was.  Planning on backups resources is key. The other thing that is critical on a project like this is not all of your team members will be familiar with the jargon that you are using, so managing expectations is pretty critical to get stakeholder buy in.  It's hard to demonstrate oh an API.  Our colleagues are more concerned with the interface, as well as the functionality of things creating so you have to work on translating what is happening into a language that they understand. So the other thing that happened was we mentioned SCRUM a lot, but it's been really invaluable to the way we work.  We started out with a SCRUM master and product owner and backups product owner and then our team kept getting whittled town and we lost our scrum master.  Even though we're losing some of those key roles and we didn't have all the ceremonies that the scrum framework has, we still kept moving on and adapting is definitely key. So next stems.  We are anticipating a spring/summer 2016 launch if all goes well, and it will.  We are looking to do auto tagging on our podcast transcripts, and hopefully feature additional NPR content and we are looking at adjusting additional NPR content, which is why we went this route, and we wanted metadata.  We wanted to put it out in the public to help come on that.  And finally, we want to support our NPR one app colleagues.  If you haven't downloaded the app, please do.  You cannot only hear our stories that we archive on the air, but you can also hear them in the app, and we encourage to you follow our adventure at NPR underscore RAD.  We thank you so much for your time today. [Applause] Okay.  Thank you NPR folk.  Okay.  Just a couple of announcements before we go to our you break and our breakout sessions for today.  First off, I just wanted want to make sure that everyone knows, during snack, the baked goods, we are informed that they were made in a facility that has allergens, so if you're allergic to nuts, I don't think any of those would probably be a good idea for you.  Sorry about that.  Number two, lightning talk signups for tomorrow morning are going to be -- you'll be able to sign up for lightning talked over by the registration desk.  One slot left.  Go for it. Okay.  And finally, I'll just announce where the breakout sessions are going to be meeting.  I think we had one addition actually, so I'll list them.  The addition on this page? No.  It is? Okay.  Great.  So pet an data assessment in the Bromley room, match cat in the Coy pool room, web library IT accessibility in the cook room, VIVO linked data for data in the flower room, and just be sure to write these down, because they won't be posted to the website in time or if at all.  The breakout sessions will.  The locations won't.  Archive space, APIs, and integrations in the frame ton room.  Community diversity in the William pen board room.  Building a coalition * list of libraries running tour, exit nodes in the ballroom, far left.  That's your left.  So all the way over there.  View find, ballroom far right.  You're right, all the way over there.  Dev options for lid, ballroom left to middle.  So over here.  And rethinking the SUMA data collection client, which is the new one, in the ballroom right middle.  Kind of in this vicinity.  Thanks, everyone.  Enjoy your break. [Break.] Hey, everyone.  We're going to try to gather people in here and get going.  Lots to fit in this afternoon.  We're going to do three give aways and then we're going to start the presentations.  Also, if anybody is interested tonight in the Hamilton room, one of the rooms where we ate lunch and breakfast, we're going to be doing play and share.  It's lots of games, board games.  There's going to be an open jam, acoustic jam.  If you have any requests, you can either let me know or you can post to the shared drive, lyrics or maybe even cords.  So that should be fun.  You have to be in it to win it.  You have to be in the room.  That's why we're doing these after break.  On the count of three: Three, two, one, here we go.  Who is it going to be? Thank you for the music.  Andrew Gehrhart? Are you here, Andrew? Andrew is not here.  All right.  We're going to try this again.  That's why you've got to debt back in.  Second try for the first give away.  Ann Slaughter? Is Ann here? Yeah, all right. [Applause] All right.  Second give away.  It was at least three years ago when I grabbed it for the Chicago conference and I found it as I was migrating files.  We can't see this.  I can see it.  Benjamin Murphy.  Is Benjamin Murphy here? Yeah? There we go.  All right.  I wasn't lying.  Is Benjamin here? No.  Okay.  Sorry, Ben.  Second round, take two. [indiscernible] Sheeley.  Happy birthday.  Sarah, I know you're out there.  There she is. [Applause] Web three.  Final round before our talks start.  I know I'm wearing my lucky socks.  They have floppy disks on them.  Laura Smart.  Is Laura here? Uh-oh.  She's not? Okay.  We're going to have to go with one more round.  And then this doesn't -- we'll try one more time.  If the person is not here, we'll just carry on so we can get going.  All right.  Round three, take two.  Bonnie Gordon.  Is Bonnie here? Yeah.  All right.  Congratulations, Bonnie.  Okay.  Without any further ado -- do we have time for sponsor announcements? No? Okay.  We'll do that next.  So I want to bring up Jason rethat would oh -- Jason ROnallo, building desktop applications. I'm going to talk about building desktop applications.  You can get some slides and links here.  I also posted in the black.  And so I'm going to ask some rhetorical questions and then show an example of applications. So why in the world would anyone want to build desktop applications these days? Like so much cool stuff has now moved onto the web and onto websites.  Why would you want to do it? A little survey.  Who here has some role in creating web applications? Any role.  Project manager? Developer? Lots of hands.  Who here has some role in creating desktop applications? Some, but kind of grouped right here.  Many fewer.  Okay. So the web has won, except on mobile.  So yah! Now we have too many tabs.  And so, like, on chrome, like you see the Savvas an con and on Fire Fox you've got to scroll park and forth in order to see all of your tabs.  There are whole classes of extinctions for browsers just to deal with how many tabs you have.  And then I use book marks mainly so I don't feel guilty about looking at that tab anymore, and then I have good intentions, but never go back to it.  That's what pin board is for for me and then I'll sometimes claim tab bankruptcy and hope that nothing important got lost in there.  And so, you know -- oh, too many tabs.  Just open another window.  Right? And then you've got, like, ten Fire Fox windows and it's like which Fire Fox window? And some of them probably have the same content that's in them, so how do you find what you're working on? So why desktop applications? So you stand out.  So you can focus on something.  It's really hard to focus when this tab is setting there with your G-mail on red count, so you want to focus.  So why not desktop applications? Any time I've tried to did a desktop application, like trying to dealing with GUI tool kits, there's ugly and difficult to work with, at least for me.  People in this section? Yes? I'm not off base there. But the thing is all you people who work on web applications already know how to create user interface with web technologies.  HTML, CSS, JavaScript.  You know that stuff.  Okay.  So there's a couple different platforms that I know about for developing desktop applications with web technologies.  I'm going to be talking about electrons. And I want to give some flavor for the kind of things that you might build with electrons to give you some ideas for how we might be able to use technologies like this in libraries.  So there's lots of companies that are using electron right now, lots of open source projects as well.  As I was creating this slide alone, like, every other day I would have to go back, because they were adding new little icons of different companies to their home page. And so who's using this last desktop application? That's written in electron.  This cord used for PC gamer communications, that's written in electron.  Lots of collaborative tools.  Avacon helps collaborators communicate.  So does team task management.  So why would you develop a collaborative tool using Electron? That's kind of a tension.  Rather than having it in the browser, you can focus. Notifications in the new you web technologies, like web RTC and web socket the which make developing this kind of application much you easier.  And then we get to code ethically iters.  Adam editor from get hub.  New Clide from Facebook.  Big companies are using this technology. And then editors.  The new WordPress desktop application is an Electron application.  YHAT rodeo is an IDE for data science.  And then lots of mark down editors, of course.  So why code and text editors? Because it's hackable.  It's all just markup.  You could create lots.  Plug-ins and develop a community around your tools.  HTML editors, HTML game development, and one I thought was really interesting was hardware configuration.  So particle and a board for doing development.  Steel series engine is for configuring gamers, and GBO, a social robot, a little tangents.  I wasn't convinced about a social robot, then I watched the video with one of my daughters and she fell in love. The Y hardware -- why hardware configuration? You can build cross platform configurations.  That auto update, that report on crashes.  You're able to run another binary as well.  And then there's all sorts of crazy other applications that partial are doing.  At one point, what is electrons? It's just chromium.  Because it's chromium, you have access to all of the different web APIs that you already know and we have access.  My entrance into Electron was through Adam, the code editor.  It was formerly known as Adam's shelf.  I thought it was really cool, because I can go in and see how it's working by opening up the developer tools in specter and doing things like changing the color of something and actually see how it works. And then what really hooked me was this plug-in called I'm done Adam, which creates like a Can ban cord where you're working.  Really cool package. I'm going to show a demo application and some of the features that I think make Electron really interesting.  Who uses jpeg 2,000 images? Okay.  Some of you.  Who has memorized all of their jpeg conversion command line options? Anyone? No hands? Okay.  I haven't either, because these are ours.  Thanks, John Stroop.  I don't know what most of us does to even remember what it would do, but it works.  So I developed this little demo application to do jpeg 2,000 applications.  Hand peg is to FM peg as [indiscernible]. So you can open up a window.  This is what it looks like.  And the first thing that you'll notice it has desktop integration.  So you can control tabs to be able to get to it.  And then here's what it does.  You drop some files in.  You click a button and it goes through some steps to create jpeg 2,000. Pretty simple. It also does -- it also ties together a bunch of other command line tools.  You've got to make sure that the tips that you're using to feed into Pacado or jpeg is RGBA.  Things like that.  Cleans up temporary file.  You can do this totally as, like, a command line script, for problem.  I just want to play around with what some electrons can do.  And this all starts with a script tag that's in a normal HTML page, and then here's some copy scripts where I required J query and on document ready, I hide some stuff from the view, and then do the image conversion.  Send notification that all the images are done converting. And then this is the code that actually converts the image, required CAL process and FS for file system.  And then actually execute the commands.  Oh, wait.  Okay.  This code is running in a browser from a script tag, and it can do things like this.  With child process, you can shell out, be able to lift the contents of a directory, but you can also, with the file system module, like delete files.  Again, this is code, basically, running in the browser.  And this was the moment when I was like -- my head was, like, mind blowing.  Like whoa.  Yeah.  So like Electron apps allow you to connect directly to a database, manipulate any files in the file system, kick off processes and desktop notifications, use any of the native APIs, like the microphone, video camera, record media, all without asking. Okay.  So a little bit about opening an image view near a new window to see the results that we had converted.  This is called interprocess communication.  It's really simple.  You get this IPC renderer, and then when you click a button, you actually just send a message just like you would in a lot of other JavaScript, and then the main process, so Electron has a main process and then it has render processes, and you can open up as many windows as you like and you communicate through the main process.  So you just listen for the open jpeg two message and you do something with D you click the link here T opens open on open C drag viewer, and you see the image that we just converted. And again, you have the full power of Node.js there.  So you can do something like embed a whole web knowed application into -- web Node application so you can use all of the same source of web endpoints that you would in a regular web application.  So here we can get the triple IF I know know dot json.  We can extract the requested impact from the jpeg two and turn it back. so this led to a little bit of yak shaving while I was doing this presentation prep operation.  So I ended up -- preparation.  So I ended up creating Node modules using the triple IF for creating a triple IF image server and then I used those in the jpeg 2,000 converter and then ended up also building a full triple IF image server as well. So what are some issues with Electron? So one of the selling points is cross platform.  I can develop this thing once and I'm going to be able to deploy it on Mack windows, Linex, but packaging is pretty automated.  When you're actually trying to build installers, I had hoped by the time I came here to this presentation that I would be able to, like, give you a download link of some application I'd developed and you would be able to, like, install it right now if the wifi was working and actually use T but I couldn't get any of the installers, other than, like, creating a Deb to actually work. And then there are subtle differences between the different operating systems and how you're going to actually integrate with the operating system, integrate with the docs, pass the bar, notifications.  And then there's the issue of if you're using any native modules, how those get rebuilt or distributed.  And the simplest thing is just if you want more resources on Electron, just go to awesome Electron.  It's got the best list of resources there, so I'm not going to try to reproduce the resources. And that's it.  Thank you. [Applause] Two minutes for questions? I can just go back to that slide and watch it for two minutes if you'd like.  Question over here? [indiscernible] Runners running? So I'm pretty happy with the Slack client, except for the amount of memory it eats you have.  Is this a general characteristic of Electron apps? Yeah.  I don't know.  It may very well be, yes.  It could be a fair criticism.  Anything else? [indiscernible] For to print r no, I haven't done any code to printing.  Yeah, that would be interesting.  I mean, you basically have chromium, so it would probably work the same way as chromium.  Create a PDF, printed from that.  That's usually how I end up doing it.  Thank you. [Applause] Thank you, Jason.  Next up we have beyond the Bento Box using linked data and smart algorithms to integrate repository data in context with Jordan fields and Mark Noble. Hello, everyone.  My name is Jordan Fields and I'm the program manager for the digital archive at Marmot library network. I'm Mark Noble, R&D manager and program manager at Marmot, too. And we're going to talk to you a little bit today about what we are doing with discovery in library.  I know most of you know the history of discover any library.  But I do wanting to through just a little bit. In the beginning when we first started discovering our online resources, we were a lot like hunters and gatherers.  I've heard it described as Berry picking.  We have all of these desperate systems and they required skills.  You had to know where they were.  You had to read the manual and how to search them.  And it was complicated was complicated and you needed to be, essentially, an expert in a bunch of different things. And then we thought, hey, we're going to combine our indices, and this is going to save us a whole lot of time, and you're all, again, familiar with this concept.  Instead of going out and gathering the food and having to cut it up and prepare it, we now have salads.  It's a lot less work for us. But along with that came a few problems.  I don't know how many people at the buffet today got the waffle fries and wished they had more room on their plate.  Or got to the end -- yeah, I see you.  Or got to the end and said, there's a giant thing of liquid cheese up here and it just kind of ended up everywhere.  I got some in my soup.  Ended up places liquid cheese shouldn't have been in my food. That's kind of what happened once we started combining all of our indices.  It got really, really, really messy sometimes, and our users could not find what they needed.  So what many, many academic libraries use now is the Bento Box and it's perfect.  It's the right balance.  You can see where you are.  There's no eating.  There's no unfortunate overlap between resources, and you're not actually missing anything.  Like I couldn't get the waffle fries or enough of them to my plate to make me happy. But you can sometimes get stuck in these boxes.  Our users don't even know exactly what it is that they're looking at or they get stuck in there, so they're looking for articles, and really, they need to be in your archive, but they didn't choose the archive box in the beginning.  Or their search as evolved.  They thought they were doing one thing and then they kind of followed down the street, searching and narrowing their topic, but since they're only looking at articles, that's obviously biasing the material they're getting. This is all academic libraries.  Forget public libraries.  Public libraries are still stuck with hunters and gatherers.  They ever completely different problems, because for them, the Bento Box doesn't meet their needs, because articles are not given the same weight as their regular resources in the catalog, because Public Library users tend to want the resources that are in the catalogs first. The problem is that they do have archives, but again, you have to know where they are and know how to use them, and they do have articles that libraries spend thousands, tens of thousands you and hundreds of thousands of dollars in public libraries and are drastically under utilized. So for what we're doing, I tried really, really hard to come up with a food analogy far too long.  Too much time was spent thinking of this, and I just couldn't.  So I decided on choose your own adventure, because this is really what we're doing.  You start with a search.  You pull your book off the shelf.  But then at any point in the process, you're given relevant options, relevant to what you're searching right now, relevant to what you're seeing on the screen or what you're seeing in your search.  You're just seeing the relevant resources from the other places.  So you have the catalog.  In our case, we're using EPSCO discovery service EDS to combine some of our articles, and then also our different tall archives. Before we get into that, I want to tell you about it * us.  We're from the Marmot library network, a nonprofit technology library consortium.  We have 27 consortium members, all in Colorado.  There's a heavy Public Library bias.  We do also serve academic libraries and high school libraries.  If you can imagine trying to design one discovery interface that meets the needs of all of these people, it's really hard and then we had people in addition to our regular members, we provide other services to, but we had people that just wanted discovery, again, mostly public libraries, and also one crazy place that combines one discovery for both their public and their school libraries that are actually on two different IOSs.  If you're curious about how that works, James is in the front row and you can ask him.  It's crazy. Our discovery layer was originally view science.  We made changes to it and you'll see them today.  It got too confusing, so we renamed you it PIKA.  We've been too busy doing development to come up with a logo.  Get it? Marmot? PIKA? Small mountain rodents? Everything below that you can discover in our discovery layer, not in all of the implementation, but we work in a number of different IOSs.  We were some of the first to implement some of the digital resources, and then our newest things we're talking about today, digital archive, Ilandors, and EDS with discovery service. The Dole of what we're showing you is for every research process, users presented with relevant suggestions from across all of the library resources.  We're not doing everything right now, with you we do hope to some way include some of the other Bento Box places that you'll see, places like the library website for things like events.  So when you're looking up you an offer, you can see it. A few caveats.  This is still in alpha.  We're still working on it, although we do have screen shots to share with you.  Lots of things like placeholder icons.  We've done some usability, some TSS, but it's not perfect.  We have usability studies scheduled over the next few months actually.  And a lot of what things are labeled and also, algorithms, refinements.  Every time we add a collection we discover something new. So I'm going to hand it over to Mark to give us a quick tour of what we've done so far. All right.  Thanks, Jordan.  So yeah, I just wanting to through a little bit of what we have so far, what we're presenting the user with.  And we can talk a little bit more about how it works.  So we're going to start.  So start.  So we put these slides together thinking about a student doing a presentation or preparing a paper about camp hail, which is a little -- it was an Army camp in Colorado up in the mountains.  They were training, the tenth mountain division was training there to go to Europe and learn how to fight on skis.  So there's lots of great information there.  We've got some articles.  We've got some books.  We've got images.  But we haven't really been able to find those.  We want to really get you the all of those resources in front of our target audience. So in this case, the student has started with a search of the catalog, so we're seeing regular catalog results here.  We've got a couple of books.  These are great books, but if the user scrolls down a little bit, now you we've presented some other resources.  So we're starting to show article information, academic journals, the magazines.  If they scroll on that, we can see some other things, so we've got all results from EPSCO and then we start diving into some of the archive content.  So we've got prospects related to T we've got images related to it.  All kind of great resources that the student might not otherwise know to use.  And we carry through that.  When you start clicking on this and go to start looking at dark let's look at the EPSCO journals, something like that.  So we're getting results from EPSCO, looking at the EPSCO-APIs.  Looking at different information from EPSCO, linking back to catalog resources, as well as that digital archive information as well. So we wanted to make sure that everybody is getting the right information at the right time, but we really also wanted to make sure that they were getting the best experience for where they were. So if you notice on the last slide, we had facets related to the catalog.  We've got different facets now that are based off the configuration of EDFs.  And then going into it, we get other results from the digital archive, so those, again, have different facets.  So we want to make sure that the students are getting the best results you possible for the section that they're in. And then we follow that up when we're looking for an item specific page.  So we're looking for a book that is specific to camp Hale, so we now see in the side bar we've got information about that specific location, so we've got lots of good details about the place itself that we want to present to the patron.  We've also got similar titles from the catalog, and then additional resources from the archive if we scroll down some more.  We have additional resources from the archive. And then looking down even farther, it's a little hard to see, it's cut off at the bottom.  It's probably behind other people's heads.  We've got similar titles there novelists.  We're driving 245 to get people more resources, but we're hoping for that serendipitous discovery that hey, I didn't know I was looking for an image.  So now it's there and I can use it. As we dive into looking at resources from the archive, so this is a picture of a ski trooper jumping down the mountains with the gun on his back.  So something that would be very use to feel a student in their report potentially.  From there, we can also see more of their similar types of information, information from the archive.  We've got our collection where you could find more titles related to this, more images, more articles, more old histories. The third item down there is the United States Army tenth mountain division, so we can provide additional information about specific organizations.  And then the same related titles. Similarly, when we're looking at an EPSCO page, we can get back to the archive.  So with we really want to make sure it's a consistent user experience for the user, no matter where they are.  We don't want to get in their way and present too much information overload we want to make sure they're getting nine information they need without it being overly intrusive. So let's talk a little bit about how it works.  In that explore more bar, we're pulling information from a couple of different sources.  So basic just EPSCO-EDS -- API services -- got keyword search there Novelis.  That's not T that's archive.  The first first we're looking at the archive and pulling related entities there.  These are all entities related to the search terms, so we're doing some keyword searching subjects, searching for those. individual articles are coming out of islander as well.  So we've turned the Islandora Solr a little bit to try to get us the right matches based off the keywords that students are entering.  And then we're also looking at the relationships as well.  So we want to say in camp Hale, if I search for camp Hale, I'm going to find a bunch of articles that are tagged with that, but the people may not necessarily be tagged with camp Hale, so we're looking at the metadata for the objects and then the relationship, so we've got linked data between the images and the people themselves.  So we can kind of show those relationships. So for an individual entity page, so in these people, places, events, we're looking at some linked data that we can pull you in.  So we were pulling address information from who's on first.  We can call it from geonames.  Standard basic stuff.  We can get linked beta from Wikipedia, you can say this article is specifically related to camp Hale.  So our catalogers can put the correct information in there. We're looking to add some more information on the linked images, oral histories, all the articles that are related specifically.  That information here, that's going to go on kind of the main page where we want to say this is all content about this entity rather than just related content.  Or similar content.  So another image here, this is the camp Hale billboard where they have camp Hale punching out Hitler there with her an me toe in it the background -- hi rahito in the background.  We've got your linked data that we're part of. The books and more is the important part here.  What we're doing is pulling out the subject for the image and trying to high those back into data related to which titles from the catalog might be interesting to the student.  So if they find this image and them to find out more about it, we want to direct them to the correct titles in EPSCO and the catalog so they can get some things they can cite. So some of the linked data sources, who's on first, geonames.  We're doing direct links within the catalog itself.  We've got a genealogy database that our catalog starts linking to for individual people.  We're linking within the archive itself.  Linking out to Wikipedia, find a grave, and it seems like every week people are coming up with a brand new source that they'd like us to integrate. so I was talking a little bit on the last slide about hour we're doing the subject-based searching.  A lot of complexity we're hitting, like we've talked a lot about this week or the past couple of days is the subject terms and how they don't necessarily relate.  So when we're in the top record is from a cataloged record.  We've got lots of great LC subjects, but that's not how they're described on the bottom left there.  We've got the subject as they're in the catalog records for the images.  And then on the right we've got a couple of different examples of the subjects in EPSCO.  So it's really difficult for us to pull data directly over and say, oh, well, if you want to find images in the article database, you need to search for United States Army mountain's tenth history, which is the LC subject, that wouldn't get you good results within the archive or vice versa.  We had some fun stuff where we were doing some searching based off of information in the archives and very reasonably they had been tagged with things like pilots and planes and hat, which makes perfect sense for an archive.  That's exactly how you want to describe things.  But when we try to start relating that to the catalog, we got here's how to fold the paper airplane.  And that wasn't quite what we were looking for.  So we're needing to do some things where we're starting to filter out some of the subjects you and say, here's things from the archive that we don't necessarily want to promote when we start looking at catalog results and article results. We think we're probably also going to do some cross mapping of some of that, so there's probably some linked data there that says either buildings are relate to go this other one subject term or potentially when we start seeing, hey, a combination of buildings and military, we can use that to say here's the correct LP subjected heading to use, so we've got refinement of algorithms to figure out how the best way to do that is.  So that's our biggest challenge. Some of our other challenges are we just have a lot of new tools to learn.  We've had this archive now for a grand total of six months.  So we're working hard to get it all integrate.  We have a lot to learn in terms of Islandora and linked data and learning some of our best practices, other new technologies. And then I'll have other stuff that people want us to keep doing as well.  Always a challenge to get everything to fit in. so some of the things we've learned so far about when to use linked data, right now we're primarily using linked data when we have a good known relationship.  So this is the same as this.  This is related to that.  That's our primary use of exclusively linked data, at least within the archive.  We're using a lot of those algorithms to improve results, so we're looking at doing subject searching.  What can we do to filter things automatically? What can we do to improve our results? We'll use a combination.  As we start processing those subject terms and say here are two of the things that we think are going to get good results, for an LC subject heading, then we can do some automatic filtering to get catalogers down to a point where they have to maybe make a choice between five to ten things instead of 5,000 things.  So we can use a combination of refinement there. And then we're also using it to supplement places where we have exact matches.  We want to use that serendipitous discovery still.  Even if a cataloger says here are five books that are directly related to this Camp Hale image, then we can still do it some promotion that hey, we've got another 50 books that might be useful to you.  There's additional ones that might be useful as well.  Our next steps respect a lot more usability.  We've done a lot.  Creating mockups.  We've put in front of librarians.  The next step is getting working software input into real users right away.  So we'll get some of those scheduled over the next couple of months, like Jordan was saying.  We'll look on performance.  The more we look, the -- we're moving to islander as soon as it's ready.  We've got refinement of algorithms, a little more linked data entry to go, and a few more usability studies. Thank you very much for your -- whop.  Wrong arrow.  Thank you very much for your time.  I wanted to thank the team, thank pass Cal who is in the audience who didn't get to come up.  Thank you very much -- Pascal. [Applause] Thank you, Jordan and Mark.  Next up, Monica Maceli is going to tell us how we can level up and get a job.  Her presentation is called what does it take to get a job these days? Analyzing jobs.code4lib.org data for understanding current technology skillsets. All right.  So who here is actively either looking for a job or looking to hire someone for a job? All right.  And I'm not trying to get myself a job.  I have a job.  Just curious to see, how many people have looked -- sorry.  I'm a pacer, as you can tell.  My students always complain about it, so I will do my best to stand in the middle. So how many of you have looked at the jobs at the website? Occasionally.  And I will make the assumption that we've all seen the jobless feeds come across the code for listserv before.  So you're probably familiar with some of the source of data that I'm going to be talking about today, so as I was introduced, my name is Monica Maceli.  I'm incises than the producer at Pratt institute.  Back in the day I was a web developer.  I think I have come to the terms with the fact that I am an * a developer.  Part of what I do is manage the technology curriculum at Pratt institute school of information, so I am kind of on the front lines as far as trying to shape the next generation of information and library students to have these skills that are actually going to be useful out there in the real world, as they call it. So this all started when I was tasked with just add new technology courses to the curriculum.  So I had a list of probably, I don't know, 500 things.  So I'm a researcher.  I have a research background.  So I decided to be a bit more formal.  I did quite an I know depth study looking at curriculum of all RLEV credited programs, looking at tech courses offered.  Pretty soon I realized what does it matter what you're teaching if it's not the one they're looking for.  I follow that with an analysis of the data from the jobs for libs website and practitioners.  And we all look like that at work.  Right? So lastly, I was interested in looking at people that are currently in the field, currently practicing.  What is it that you would like to know about technology that you wish you had learned? What is it that you wished you had discovered once got out the into the field? The Goal is to bring in ultimately full circle and inform curriculum are his creating the right kind of students to go out there and fulfill these positions. So if my curriculum -- for curriculum analysis, I looked at 56 accredited programs, myself and a couple dedicated graduate assistance qualitative coded 822 topic courses.  That was quite a year.  At the end of the talk I'll have some of my citations, which are available through open access.  I cannot really even scratch the surface of the amount of data and tables and other interesting outputs, but just you generally, just help me do things like say, all right, what are some huge areas where we are offering a lot of tech courses? What are areas where we're not offering ones? Are these kind of up and coming? Are they on the way out? And then secondly, when I first started, one of our faculty said to me, oh, you're doing the study looking at curriculum.  That's cute.  Somebody does that every couple of years.  If you have any programming background, when someone says something like that to you, that's kind of a red flag.  You say, not on my watch.  So this is my attempt at not on my watch.  So I have a web briefing tool, pie common? beautiful soup.  That is scrape okay a monthly basis the catalogs of all these RLEV accredited programs.  It's been running about a year now.  And it will allow me to eventually, over time, be able to understand the trend.  So these are just some of the highlights from the past year, so I will be able to, over time, kind of see what areas are there growth in tech courses.  Where are courses kind of disappearing or being phased out? And they're not always what you would think. Early on when I was talking to my graduate assistants or self-work on qualitative coding all of these courses, so these are all the topics that are offered in the common information technology introductory course that any RLEV accredited programs are going to have.  So this is a whole lot of stuff.  And in particular, I was tasked with rolling out new courses and, you know, what do I pick from this list? There are many, many things there.  And this is a nice job of highlighting some of the most important ones, and nothing on that word cloud will be foreign to you, but where do I focus my efforts? Where do I encourage my students to focus their efforts? The future? So that led me to the jobs analysis.  And as most of you are familiar with the jobs for lib site, it problems a platform for searching and posting attractive sites, kind of bare bones.  You can explore the different jobs and you can also, if you choose to, volunteer your own time to curating and tagging the post there.  Has anyone done any curation and tagging of jobs? All right.  A few of you.  I've done a few myself.  I felt like I add it to the data set.  So we have kind of an interesting mix of data that is representing the actual job itself, so we have lots of tech space data like title, as well as the description.  Would he also have data that real humans have tagged the job with, which obviously can be weighed heavily as being a pretty good representation what have that job entails.  This tool was originally developed by Ted summers.  He is still very active in developing the actual jobs for website.  The whole source code is available.  If you're in it from a developer perspective, Ed has been fantastic to work with.  I'm sure many of you know him already.  If you're interested in working on the job's web facing piece, he's a -- web facing piece, he's a great person to get in contact with. So what he actually gave me, and this was so exciting, as a researcher, you just can't Beat someone willing to give you a fantastic set of data.  So that's my Christmas, I guess.  I don't know.  So all the data that's collected on that site is available for use, so Ed handed off to me an Anon amazed database, just a job listing with some of the other data and that included job titles, job descriptions, and then all the edit era signed tags that are all kind of contained in a relational database structure.  So I could see pretty easily what tags were associated with a particular job, as well as the raw text. So then I entered the world of text mining and I went -- anybody into text mining? A few of you.  That actually makes me feel better, because then I can make get away with speaking authoritatively about it.  Anyway, it's super fun.  If you're not into it, it's fantastic.  Now I have that classic problem where if you have a Hammer, every problem looks like a nail.  It's like, let's go through and do text mine okay it and see what comes up.  Sometimes it's good.  Sometime it's bad N this case t actually turned out pretty well, so this is a good first case for me. So obviously, with a database like that, I had a huge amount of text, you know, largely unstructured.  The job descriptions could have all kind of information about the job itself, but then also maybe the organization that was offering it.  So I used R with the TM, which is the text mining package, which is very easy and fun to get up and started with to look more deeply into this collection of jobs, and specifically, I was interested in, you know, what technologies are being asked for frequently? And not just what technologies in isolation, but what are these kind of clusters or groupings of technology so we could start to say, okay, if went to make someone that's going to be a web developer, they should know kind of collection of skills.  It's easy to say there's PHP used or something like that, but what's the context? And the nice thing, when it comes to text mining with technology topics, we have all of these weird acronym and his all kind of things that are very distinctive.  So there's no other word that looks like SQL.  That means something else.  So it's actually a text set that is very amen annual to mining. I also did a fair amount of plotting to visualize these connections.  I'll show you a few shortly.  And I also you did some clustering.  When you have a huge amount of text like that, understanding, not just what terms correlate, but what similar collections are there was very helpful in kind of digging through this data. So what you're seeing here is the latest and greatest.  My published papers actually covered from 2011 to 2014. This is the analysis I just did a couple weeks ago looking at the 2015 jobs.  And just to give you an idea of the scale, there were about a thousand job listings posted in 2015. These are valid, approved jobs that the editor said yes, this is appropriate for the community.  Let's send this out to the Code4Lib -- jobs four lib side.  This is a distinct I've been working and there's far more not librarians, in other words, not explicitly titled as librarians.  The cluster shows common title terms. And just to give you a sense of the top edit era signed tags, anyone know any of these skills up here? What is this.  Right? So I would bet we have pretty good coverage here of all of these.  So these are the frequency of editor sign tags.  I use that as a starting point with text, my text analysis to have an idea of, right, we have real humans saying these are valuable skills, so let's use that as a starting point to see what goes along with them.  These are, again, just for 2015. Historically, through PHP, JavaScript, and metadata, XML made it out since the data had went back to 2011. So since that time, just to give you an idea of how much work the editors do versus what I will extract from the text, so in this case here, you see about maybe 150 editors or 150 tags of a job, and when we actually look at the raw data, the word metadata is always number one, as you can imagine, but he appeared around 1100 times, so we actually have a lot of jobs that aren't able to be classified by editors, quite simply because they're volunteer curators and they're not going to sit there and curate a thousand jobs every year probably.  But this is a good starting pointed. So I use those terms to then took at the correlated terms.  So for these top ones, jobs for PHP, XML, and metadata.  All of these are generated using the find associations function.  That's it the TM package, and you can set the correlation, coefficient, and it will tell what you terms are closely correlated.  Essentially, what this means is if a job is, say, looking for CSS, it's pretty likely there's a strong positive correlation.  They'll also ask for PHP, my SQL, and some generic terms that make my life a little bit more difficult.  Something like experience in particular is difficult in a job setting, because it could mean experience.  It could also mean user experience.  So there were some kinks I had to kind of work out over time with the data. So when looking through these top rated terms, my editors tried to highlight ones that would show kind of a diversity of positions.  So starting to kind of work deeper, going from a a language that might refer to more of a client side work, starting to go back toward the server side.  Looking for terms correlated with PHP, start to pull out other related languages.  We start to see web servers, relational database, management systems, still going deeper into the stack, as it were, with terms correlated with PHP.  So the metadata one is always fantastic.  This one I can actually fit on the screen.  Sometimes there are a whole bunch of Node s vying for position there.  And there's some really interesting, a lot of common standards, but I'm also interested in looking over time at things like the little link data cluster emerge there go in the upper right corner.  There, if you can see, I think metadata in particular will be interesting to explore over time.  XML wasn't too exciting in this past 2015. We have all the Xs, as you can imagine, and looks like they're doing a lot of web based stuff with XML now-a-days.  And I have many more of these term correlation plots in the full paper. And just in my last few minutes, I didn't stop there.  Why stop there? There's more data out there.  If anyone saw the results of John Burke's survey or maybe you waited participated in it, he sent it around to the mailing list, and he did a survey back in 2015 asking current library practitioners a number of questions around their technology use.  Of most interest to me, one of which was what is it that you would like to learn, and I take that to mean this is a technology skill that if I had it right now, this is a good thing.  So I'm very interested in that, because that tells me that's a skill that probably should have been rolled into curriculum at some point and then also, what they were doing in actual practice.  So one of his questions asks them, well, what are you actually using on a day-to-day basis. And first I look at the wants here, clustering to kind of see some of these different topics that emerged.  A lot of them aligned pretty well with these skills coming up again and again in the job listings.  And I also wanted to look at with their current skillsets, what are these common cocoa considering skillsets? So this is one form of technology use, so maybe this is more oriented Ward the patron side or the user side N this particular correlation, I looked at skills that commonly occur around Linex.  That was one of the questions on the survey.  I figure if you're using Linex, you're probably on the tech side, I would imagine.  And I have time expired.  All right. The last slide, my summary.  I'm doing a lot of other questions kind of digging into this stuff over time.  My full papers are up there and available if you want to look more deeply into the different technologies that I encountered.  All right. [Applause] Thank you, Monica, and we are slowly posting the slides to the website.  So you'll be able to get all of those links that Monica posted at the end. Next up, Frances Webb * Webb and Jennifer Colt, building a user-friendly authorities browse in Blacklight. Hi.  I'm Jennifer Colt and I'm a user experience developer at Cornell University, and yeah, we're going to talk about the authorities browse that we've been building in Blacklight.  We recently -- well, over the last couple of years moved the public interface for our catalogs from Voyager to Blacklight, and one of the things that we needed to do when we did that was replace the heading search and browse that had been part of Voyager. So to give you an idea of where we're starting from, here's a subject browse in our Voyager interface.  I was looking at sciences fiction.  You can see that there's 13 entries for it.  And so here's the same browse in Blacklight, and there's only -- it's only showing up twice now.  In the Voyager version, we were listing the term according to vocab other you Larry, so it ended up showing up many times.  Here we're showing it according to what field it appeared in, so you've still got a choice to make, but hopefully it's a little bit easier, and you've also got some easy access to the narrower term related to science fiction. This is the author browse in the Blacklight interface now.  And you can see that looking at Kerrie brownstein and she is there, along with Slater Kenny, and both her name and Slater Kenny are linked, because they will go to a search result in the main Blacklight catalog.  A couple rows down, some people can see, there's Daniel J.  Brownstein, and his name isn't linked, but his C, Brownstein Dan, is linked.  We don't have search results for the first version of his name, but with we do no the second.  We're trying to avoid any dead-ends.  There's no -- headings only appear here if that heading or its C or it's C also will give you some search results in the main catalog. So if you click on Kerrie Brownstein, these are a couple search results, just a couple of them, and when you look at an item, if the -- there's an authorized version of the author in the field there, a little info button will show up, and it's going to bring up a box with it biographical information if there is some.  There wasn't much for her, as well as some search linked for the works about her, the comes up you again, and if you click that, you'll be able to get search results for them.  So that's sort of the obvious interface integrations that we made, and Frances is going to talk about some deeper stuff. Okay.  So I'm Frances Webb, another developer at Cornell.  And so there are a few kind of cross-reference that his we have in the browse, but I want to talk about the C references a bit.  The C references are different, because we're telling people we think you're searching in the wrong place.  And in this case, we, in fact, have no catalog records that list heart attack as a subject heading, because that isn't the preferred form of that term.  So this is really vital information to expose.  Unfortunately, in this case, what we've done in the browse, while it makes the information available, it doesn't quite help if somebody just comes and does a subject search for heart attacks.  They won't realize that they're missing something or they may not. So the cross-references come from these [indiscernible] records.  So as of 2015, if you were to do that subjected search for heart attack, you would find nothing in our catalog.  As of 2016, you get this, which is arguably a more safety result.  And so what we've done now is we've taken, for every div record that uses a preferred form of ah, they contributor, or subject heading, we have pulled in all alternate forms of those headings and made sure that that record is findable using the alternate form.  This just went live at the beginning of calendar year.  It's been very successful so far.  None of these records have heart attack as a subject heading, even though this is a subject search. So this is the first hit.  You would have found this record in all Fields.  The general search for heart attack, not a subject heading search.  And this record is the second hit.  In fact, it doesn't contain the word attack anywhere in the record.  It still is very, very appropriate results.  So this we're hoping to be a very subtle difference that people will find the records they want, but they won't necessarily realize that we've done anything interesting to allow them to find the records that they want. And one of the things that I like about this particular integration is it's something that could be adopted without need to go set up an entire browse.  As long as the piece that is doing your indexing of divs can make the retrieval, can query using the preferred headings to retrieve the ultimate forms, then this is a kind of thing that is doable. So index authority records. There are a lot of systems that already do this.  Our first step was to index the entire authority file.  It's both name authority and subject heading authority into a single database.  We assign our own I Ds to everything, because the authority record doesn't assign identifiers to alternate forms, and in order to make a really good mapping of all the cross-references, we need things to have identifiers. If you are thinking of doing your own investing of the authority file, be aware of the undifferentiated name words.  One of the risks of reconciliation is that you will merge records for two different people accidentally.  The undifferentiated name record has done it not necessarily willingly, but deliberately.  It's known.  They are labeled.  Beware not to trust the alt forms, because you don't know which of the people they refer to. So once we've built our database, headings that appear in authority records, we index the headings that appear in the div.  These are 12 different fearies that goes into our Blacklight folder index of bibliographic headers.  How often headers appear and in what Fields.  Heading entries and database, and we create new heading entries for things that weren't already in there. Finally, we push all of this from our database in Solr.  As Jen was explaining, we don't push authority records.  We don't have anything interesting to say about as far as our own content.  And you can see, we drop about two-thirds or three-quarters of the authority records in this move just because we don't have that kind of coverage. So I've been asked why it's a good idea to go to the effort to build a separate index for browse versus trying to pull the summary data from Solr, which is something that Solr can do, but you Solr is a lot stronger when you're trying to pull -- when you're trying to pull information at the same granularity level that is indexed, so our Blacklight index is indexed at the bib level.  This is at the heading level.  And it gives us a lot of additional capability. So the only other thing I wanted to talk about filing.  It doesn't sound exciting.  Everybody knows that you need to -- well, you will experience -- if information is not normalized before the system tries to story about it, it becomes very obvious, because all the capital letters before the [indiscernible] mess everything up. What we're doing here is a little bit more complicated, because you we have decided that if two things normalize to the same sort value, they are the same thing.  So in this case, a few minor changes in punctuation, capitalization don't actually constitute a different heading in our indexing.  In this case, we had to do something a little bit different, because you we have a difference in headings that we want to be reflected in the sort -- that we don't want to be reflected in the sort, but we do want to be reflected in the file, and these are not the same headings. And finally, we have the Dutch and the East Indies are not the Dutch East Indies, and both of these headings actually do appear in our catalog.  The zero is for the sort [indiscernible] which means sort first by the main heading, then by the subdivision.  You can see in this browse everything that relates to prints, either the individual or the concept comes before Prince Albert and everything else.  And this is especially vital in this view where you would never be able to find in the title browse the prince word if they didn't store it first. And we have code references. [Applause] Thank you, Francis and Jennifer.  Next up, making new discoveries from old data, using digital scholarship to foster new research in Special Collections.  And that's my Matt Carruthers. Hello, everybody.  I'm Matt Carruthers.  I'm from the University of Michigan.  I'm going to talk to you briefly today about some of our recent efforts in the library towards utilizing digital scholarship as a way to foster new research in Special Collections.  Linked to these slides at the bottom, I also posted the link on the slide channel just in case there are heads in the way. Just a bit of background to start off.  A few months ago, a small group of us in the library were brought together to explore using digital scholarship methods and technologies to enhance user experience in Special Collections and potentially even provide new services.  In terms of a charge, that was all the detail we got.  It was kind of wide open in terms of where we can go. So we decided to start off, we would talk to some patrons, some staff, get an idea of where we were falling short in terms of discovery of our data or providing access to our data, and use that as a basis for developing the proof of concept for a new service under a couple of operating principles.  First, use existing open source technologies as soon as possible so we can cut down the need for in-house development, and second, hopefully develop service models that would require little to no need for dedicated IT support over time, so essentially, it would be something that Special Collections staff could run start to finish in-house. What we came up with, or, and then, this is still in the very early changes, proof of concept for a service to provide visualization on demand, customized to a patient's individual research question.  So we would use data that we already have as a basis for this in form of our EAD finding aides.  Combine that with open source digital scholarship to create interactive visual I said of the social networks or network graphs of creators of our archival collection.  Again, customize to what a patron wants to look at.  So all of the people, organizations, and things like our collection creators are connected to somehow. The idea behind this would be to make these connections between entities much more apparent, because often they're either buried within the finding aid or scattered amongst a lot of different resources.  We can provide these visual I said as a discovery tool to the patrons to help them further their research. So as I men, I'll take you through a little bit of the process that we've developed so far.  It all starts with the EAD files we already have.  And we start by extracting the information about the creators of those collections from the EAD files and using that to create EAC-CPF files t stands for encoded archival context for corporate bodies, persons, and families, and it's the companion standard to EAD.  EAD is the archival stuff and EAC-CPF describes the agents responsible for creating the stuff.  You can actually encode relationships between the subject of the record and all other entities and describe the nature of those relationships in a machinery format.  That's the date we're looking to take advantage of here. In terms of creating the EAC-CPT records, we have options available for helping us automate those prospects.  I don't have time to go through those in detail.  Suffice it to say, these are all services and tools that are openly available to automatically create the EAC-CPF records based off existing EAD files, and then it gives you the option to enhance those records by pulling in additional machine from various APIs, like BF and world cat. Once we have our corpus of EAC-CPF files for creators of our web collection, we run the files through a very simple style sheet that we put together to extract the relationship data from those files and put them into tab to text format that we can use to put through our visual said software 679 once we have the tab and the text files, it's just a question of actually having a reference interview with a researcher who is interested and looking at or building a custom digitalization, looking at a subset of our data, so we figure out what the research question is, work with them to decide exactly which files and what subset of their data they're interested -- or our data they're interested in looking at. So what we're pulling out in terms of the p.m.  SLT style sheet and putting in limited tabs is four bits of information: The name of the entity connected to the subject of the record be the type of entity that it represents, and we have use five basic entity types, the type of connection that exists between those two entities, and we use four connection types that were originally defined by the social networks and archival context part, and when it's available in the record, a link to annex term web resource that provides more contextual information about the entity that's being represented. So once you've got those text files, we can actually move on to the business of visualizing all of this data, and we looked at a lot of different platforms, everything from D3 to Geffy.  We decided in the end that site escape was the tool that worked best for us.  An open source desk tap program designed for visualizing complex network graphs.  So this is a green shot of what the interface actually looks like.  The visualization that you're look at here is actually one for the social network graph for all of the creators of our children's literature collections.  The cool thing about site escape is you can design a custom style file to determine the layout, color-coding options, things like that, and you can import it into any data set that you want to build a visualization for.  So you don't have to do the same thing over and over again. So if you can see there, the red notes represent the entities that were the collection creators for the collections we have locally.  All of the White Node s are other entities that we're related to.  And the connections, the edge lines are color-coded according to the connection types that we have to find. So the other cool thing about site escape is you can very easily break out subnetworks from the larger whole, so if we wanted to, for instance, have individual visualizations for the first degree network connections of all of our collection creators in here, we could actually, with two or three button clicks, break out a subnetwork.  So we can produce a lot of visualizations very quickly for the patron.  And it usually only takes a few minutes to go from selecting the text files to use to spinning up these visualization.  It's very quick for us, because all we have to do is import the data files, import the style file that we already have defined and that's pretty much it. The thing is we didn't want to rely on the desktop applications * indication as a way of sharing this with our users who requested these visualization, because they would have to be in Special Collections and use our machines or download their own, and we didn't want to make them do that. Luckily, site escape has kind of a companion platform designed to web publishing called sign up share.  And this is the point where I could link out of these, but I don't trust my luck going forward with the wifi holding together, so I'll stick to screen shots for the time being. To use share, it's hosted by UCFD.  All you have to do is export the files from your visualization have cytoscape, if you lose to create custom ones, you put them anywhere on the web that gives them a public URL.  You can use your own website.  Put the URLs into the form, click visualize, and you get a fully interactive web version of the visualization that you created in the desktop interface, complete with the guilt to generate a short URL that you can e-mail to the patron who asked this so you can look apt these visualization so you can use them any time, anywhere from the web. and this is just a screen shot to sole you how you can interact with these visualizations.  They're zoomable.  You can actually click on different parts of the visualization, drag to manually reformat, if you want to move different elements around.  There's a table available, so if you click any different portion of the visualization, you can get more textual information about what's actually being represented there.  So it's pretty malleable interface. So that's where we are right now in determining our service model.  The next step that we want to do is continuing to streamline the work flow as much as possible.  It only takes a few minutes to spin up you each one of these on average.  It depends on the size of the data set a little bit, but we want to make sure that it's as easy as possible for the staff that it will actually be at least interior running the service. Then we can move on to performing some more robust user testing, performing actual pilots with real patrons and working their feedback back into our service model.  And then we can start to use more options for using and manipulating the data.  Sign up area is built off of HTM L5 and uses Java is the library and Angular JF.  We could actually install and host our own version of that if we wanted to have permanent visualizations linked to from our Special Collections website and use the hosted version at UCSD to publish the on demand visually indications we get requests for from patrons. That's all I have.  Fee free to contact me if you have you questions or track me down.  I'll be at the conference all day today and tomorrow.  Again, slides at the bottom or on Slack.  So thanks very much for listening. [Applause] Wow.  That was so well timed, I think he finished with one second left.  Okay.  We have one more talk of the afternoon, and then we'll wrap it up.  Kim Pham is going to tell us about -- talk to us about curating my web crawl, building a multiprocessing web crawler for ethnographic research. Hi, everyone.  My name is Kim and I'm a librarian at the University of Toronto Scarborough in the digital scholarship unit.  I'm here to share our experience so far with building MediaCAT with the web crawler application made possible through a partnership between our university's Department of superior science, library and faculty member, Dr.  Alajendro PAZ and the Department of -- This began with a question posed by ala hand Joe.  If we acknowledge that current mobile I said of public opinion transgressed geographic and political boundaries, we begin to see patterns of intertextual and interdiscursive dialogue that forms these public spheres of communication.  Alajendro studies mass forms of communication such as online journalism to see how they inform these global public spheres.  But with a specific focus on tracking how you Israeli news is being produced and disseminate online and recirculated in English news or social media.  He began his research two years ago interviewing journalists in steal about contextual practices of news, but he also want to look at the actual news content that's being produced. Studying this quickly became complicated, so that was when he came to the library to see if we could help him realize his plans of gathering and analyzing large amounts of articles on the web. So our digital scholarship unit coordinator reached out to a computer science professor, Dr.  Takliovich, who thought this sounded like a good project for her software engineering course.  This is how the project began.  Last January we had students working in teams to develop personas and to gather not only Alajendro's requirements, but also the requirements of the library be, the idea that once the course was finished and the application developed to the point of maturity, we would maintain it and drop it further. So throughout that semester, we came into the course, working with computer you science students to flush out the requirements and turn them into cases.  From our perspective, we wanted an app that opened standards, something that could be modified and understood by others, and also keeping it open to provide the added benefit for the students to have code that they could publish and point to on their resum. The crawler should accomplish the objectives of Alajendro's theoretical approach to electricitying data, which would also be sufficiently generalized so it could serve a broader community of researchers and students. Anyone should be able to benefited from this, from using this app, regardless of their technical proficiency.  Ideally, you should be able to build your own personal web archive with minimal interaction with code.  You should also be presented with a variety of export options, depending on your comfort level to manage your own data sets. We also wanted to make sure that we would be able to provide sustainable support in using this it app, so thinking about a post Grant or no grant scenario.  We didn't have access to high performance clusters.  We could still support multiple users to get meaningful data out of the system.  So instead of going with the approach of amassing large amounts of data, we wanted to focus on building unique data sets that capture only what is needed. So once the course is finished, the smaller team was formed in the summer to continue working on this project, and we've been working on it ever since then, these guys.  I won't do a demo, but I will quickly run through the basic functions of this app.  So MediaCAT is written in python and uses the Jango framework as its front end, which you might know Jango was originally developed by two programmers working for the Lawrence journal world news and was designed to manage news-related data.  Jango out of the box tried user authentication and comes with administrative interface that the library used to provide user accounts to students and researchers friended From the interface, you could also stop, pause, and start the web and Twitter crawling processes. To define the scope of a crawl, user inputs a list of referring sites such as New York Times or BBC.com.  What you would do for the screen shot, so you would you end up with a list of referring sites that would be crawled and archived.  The refer sites, however, are constrained by your source sites and their concordances.  So that means that if [indiscernible] a referring site f there's anorchia source site, that page will be archived.  Otherwise, it won't. We also have -- we just have the same scope for Twitter accounts.  You can set up a list of referring Twitter accounts.  We mention source Twitter accounts in keywords as well. so MediaCAT comes with an installation script that installs its tendencies and reconfiguration files shown here that you would use to set up your directory structures so that Twitter keys and search formats and also set up your [indiscernible]. MediaCAT uses a number of existing libraries to help us tackle some of the features that we wanted to get out of [indiscernible]. There are two separate crawler functions.  One process looks at recent RSS fees and then we have another crawl process that scans multiple domains at once. In addition to looking for few words, we make the crawler more efficient by using readability, the python [indiscernible] and scans only the relevant sections of the web page, which saves us some crawl time by ignoring content like ads and side bars and menus. If there's a match, we would strike the relevant sessions, using newspapers.  A more robust extraction library that uses natural language process to go identify common article structures.  Right now, it recognizes things such as title, author, keywords, date Fields, which can be [indiscernible] searched on. At the same time, we capture text.  We also capture an archival copy using W pull to start work, as well as PDF and P and G screen shot copies, using PhantomJS, which is a headings web kit that will render a page as if it was being loaded into a browser.  This provides us with a simple work around for job to script heavy websites. A lot of work still needs to be done, and only recently we focused on doing more test crawls to check on the quality of the data.  By doing these repeated crawls, we've noticed interesting outliers and issues with duplication of the content. So for example, in a few particular web drawings, some Fields weren't being captured.  Domain and structure their sites differently, using their own fags and unique classes so the newspaper -- tags and unique classes so the newspaper API doesn't pick up on the right Fields to extract. In these situations, we get users to add to the assessed lectures from the source of the web page, and that's where they can input the Fields that they want to the capture with the appropriate field heading. Oh so our new pipeline for our crawler now is to check if there is input for [indiscernible] selectors on a domain, if the sectors return nothing, then website is canned using readability, and if there's a match be the content is then extracted with newspaper. In cases where editorial policies control how content needs to be formatted or cited, this increases the complexity of comparing data between different news sites.  We've seen situations where a source is mentioned in an article, but again, footnoted at the bottom, or sometimes they don't link to the source article at all, which as we're tracking mentions it will be be counted differently.  Sometimes we get relative URLs that refer back to the same page.  Usually when we're calling the URL, we can be used on the python library.  With more common cases like the first two links, this normalization works.  But for instance, in the second two cases, although it goes to the same page, the crawler treats them as distinct URLs and crawls them again. Economical URLs are written into meta tags and can also be inconsistent.  They don't always take into account article pagination.  So sometimes you would only capture the first page and not the entirely article.  The alternative to this would be to call the site you pages and then figure out a way to lead them out later. We haven't come across this issue yet, but we are aware of the fact that certain URLs can restrict access to content to certain users, and this will also later affect the way we would do our crawl. So as we're going through and documenting some of these issues, I just want to mention that we're trying to avoid hard coat ago lot of this logic into the system.  Instead, we'd like to make entering in these URL patterns configurable in order to maintain some flexibility in the system in the hopes that it will support the cure Asia secretary of web crawling and allow researchers able to iterate and adjust inquiries to get the data that they need. If you want to learn more, you can find the project, which is open forced and available on Github.  If anyone else is doing similar work, I'd like to hear from you.  Thanks. [Applause] Okay.  That does it for today.  Mind blown.  Lots of good stuff.  We will be back here tomorrow morning for our second keynote by Gabriel wineberg, and just wanted to mention that some of the details for the plan share are posted to the schedule on the website, the conference site now, so low indication and times and everything, and any requests, you can tweet them out or Slack them and we'll see what we can do.  That's also the game room.  It will be in the Hamilton room, and one last thing, I just wanted to thank our contributing sponsors really quickly as you're putting your stuff together.  Because they deserve it.  All of our sponsors do.  And let me just pull them up here so that I can read them off. Oh, cool.  Watch life stream.  Nice.  Okay.  Contributors, balsamic, Asavia, library help, Illinois Institute of Technology, Simmons school of library and information science, University of Oregon libraries, UNC school of information and library science, Drexel university libraries, the University of Texas at Austin school of information, and there was one last one that we didn't quite get up here.  We will get that before the end of the conference, but the user library consortium.  Okay.  Thanks a lot.  And I will see you tomorrow. Good morning (singing)  good morning, time to wake up, it's a beautiful day. Good morning, good morning, good morning to you all  -- [APPLAUSE] Welcome to the last day of Code4lib 2016. You survived thus far. So a few announcements before we get on to this morning's fes festivities. Today's duty officers in that corner right there are Chad Nelson and Whintney, if you need any help in terms of code of conduct or questions, go to them. Now a few folks might be missing some things, a few folks might have found things that are not theres. So at the registration desk there are a few things that are in the lost and found section. So if you have something that you lost and wondering if they have it, go look there. If you found something and you want to return it to its proper home, go to the registration desk as well. All right. Now I know we didn't do this yesterday because I'm usually the MC for Wednesday morning and for some reason I didn't do MC Wednesday morning, I don't know why. Let me think about that. But there annual tradition of seeing how many Code4libs people have attended in the past. We have been running for what, is it, 11 years now, 11 years, I feel old now. So this is audience participation time. This is your exercise for this morning. So you can wake up. And you don't have to raise your hand or if you can't raise your hand or unable to raise your hand, you can do any other type of signal as well. Feel free to participate if you want to. How many of you are attending your first annual Code4lib conference? Round of applause. [APPLAUSE] Well am could, we're glad to see a whole set of fresh new faces and we hope to see you again next year. Now for those of us who have had the honor or not so honor of surviving more than one Code4lib, we get to do another exercise. How many of you have attended -- keep your hand up if you have attended more than one Code4lib. Okay, so this is your second Code4lib? Third Code4lib. Fourth Code4lib. Fifth, sixth, seven, eight, nine, ten, eleven, now I want to point out there is one hand, Karen Kohms [Name?] [APPLAUSE] -- unless I missed another hand. Was there another hand? Brian? Where are you? Okay so Brian is there, okay. Well then it doesn't count then. Since he wasn't here his streak is broken. [laughter] But, for those of you who are recently newish to Code4lib or been to Code4lib for a while, how many of you have attended a Regional or local Code4lib meetup? If you are attending Code4lib for the first time or just wondering what the heck is she going on about local Code4lib, one of the great things about Code4lib is that if you are interested in getting a local Code4lib, original Code4lib meetup just to meet and chat about anything in the text, feel free to do so. A lot of people have done that. There is groups around the country, groups around the world. For example, Code4lib Japan is a very strong Regional, very strong international Code4lib group. So Kudos for them for coming back again this year. If you want to get more information to see if there is a Regional group around you Wiki has not a complete list, but it has a fairly expansive list of groups that are around the area. And if you don't see a group around the area, go ahead shoot an e-mail off to Code4lib list serve and say hey I want to start a local group, is anyone interested? So just an fyi, as you go back home and you suffer through Code4lib withdrawal, which is a real thing. You will suffer through Code4lib withdrawal. What you will probably do is get back home, get back to your office and the first thing that you'll do is load up the live stream archives and have that in the background as you assimilate back into your work and life. All right. Let's see, anything else? Before we start with our keynote. Okay. Sorry to interrupt, but this just in. The hotel has setup a shuttle to the airport that will be leaving at 10 and 40 after every hour starting at 9, ending at 5 and it is $10 a person. So that's a pretty sweet deal. I suggest you take advantage of that. Thank you and that just reminded me about hotel checkouts and hotel bag holding. Hotel checkout f you haven't talked to the hotel already to extend the time, hotel checkout is at noon, so plan accordingly. The hotel will hold your bag. So if you check out and you don't want to leave your bag at the conference area, just let them check it behind the desk. If there are no other announcements, bad jokes, puns, song requests. There is not even sarcastic free bird? Free bird. All right. Well I have the pleasure of introducing the closing keynote for Code4lib 2016, Gabriel Weinberg. He is the founder of CEO go go, doesn't track you over 3 billion searches in 2015. Previously he was the co-funder and CEO of [Name?] which was acquired in 2006. Gabriel resides in Vally Forge Pennsylvania and on Twitter at yegg. He has a BS in physics and MS in technology and policy from MIT. So please welcome our closing key note, Gabriel. [APPLAUSE] Hello. Really honored to be here and that you invited me. I pretty much turn down every speaking engagement request, so take that as you will, I really like the organization and what you're doing, but the flip side of that is I don't get out of my office much, so this is not super timed really well, never given this talk before. But I put something together, I hope I don't talk too long and then we will have time to take any questions you like. I did put one joke in here, so I guess we will see if anyone notices it. [laughter] That is not the joke. [laughter] As a, you know, as mentioned search engine that doesn't track you. What does that mean, when you search on DuckDuckGo you are completely anonymous. We throw away all your personal information and don't associate it with search results and in fact don't even connect search results together so there is no notion of search history on DuckDuckGo. So as a show of hands, I'm just super curious, who have heard of DuckDuckGo outside of this conference before? I wish it was the whole world. Who has actually tried DuckDuckGo? Who is a semiuser of DuckDuckGo? And who has it as their primary search engine? Thank you. Would love to increase those numbers and I'm going to talk to you about that. Oh, let's see here. Amanda, thank you. How do you advance? Okay. Can we do like -- yeah, thank you for helping me. So anyway, we did about 3 billion searches last year. That sounds like a lot, it is a lot, but it is not a lot when you compare it to Google. So we're very proud of where we have come over the years but we feel like we got a lot to go. We're built into Safari and FireFox, not as like the default, but as a default option. So in the US it's Google, Yahoo, Bing and us and we're in about 90 other browsers and distributions, FireFox and things like that, where we are the default for kind of organizations that care more about privacy as a focus. So we focus as a company on things that we think Google can't or won't copy easily. The two primary ones of those are privacy, which I'm going to talk about. And then open source, which I'm also going to talk about those, good for this particular audience. A little history, I started DuckDuckGo actually before this graph even existed, so in 2008. So the first two years of it it was completely flat, that was just me, by myself, coding, like every line of code, in my basement mainly. And then around 2011, where the curve starts to kick up a little more, it started to become something that people would actually switch to, at least some people would try it out, early adopters. At that pointed I realized it could be something bigger and started recruiting a team. Now we are about 40 people scattered around the world, happy to get into that if anyone is interested more on the company side. We have a work wherever, whenever policy, so we have people scattered everywhere, we hired everyone from our community, that means anyone can pop up in any country. We have someone in Japan, France, Italy, a the lot of countries we have basically one person working there. So privacy. I'm going to take you through a little bit about why people care about online privacy. The most common and annoying and emotionally resonant example everyone gets in the mainstream is retargeting ads, the ads that follow you around the Internet everywhere you This screen shot is from 2010, so you can see how long this has been going on, then it just keeps getting worse each year with the rise, which is correlated very closely with the rise of ad blockers, generally just it is annoying to be tracked around the on Internet. But the, this is really just kind of the tip of the iceberg, because what this really entails in the back end is someone or some can be making a profile about you and then using it to follow you around. So you do a search, say and then you see whatever you search for, you know, on another site that you never actually visited before, and you see the ad for it. So you have to think, if that profile exists and they use it to show me this ad, what else could they be using it for. That's where you get into more of the deeper privacy, so the second one that I usually tell people about is price discrimination. So when you land on a retailer site nowadays on the Internet, there is a descent chance something is getting tailored to you specifically, even if you have never visited that retailer before at all. And they are looking at information on your computer, you know, from ad networks or usually ad networks or sometimes kind of data brokers with ad networks to then what they buy and then they use it to often change the price on you. So you can sit next to someone else and see a different price on a retailer. Most people feel this crosses the line for their accessibility, as do I. So this is also from 2012. So it has actually been going on a while. But part of the problem is there is, which I'm going to get into, there is no regulation on this whatsoever. Not even basic transparency regulations. So we actually don't know how much of this is going on in general. It is taking investigative journalism to uncover things here and there and in the frequency of that has been increased but we literally don't know how pervasive it is. Now if it I'm normally talking to people about privacy, I would often stop with these two because they are both, it is enough for people and they are emotional resonant. But this is a different type of audience and so I want to go a bit deeper and tell you about two more details, nuance privacy that I particularly find trouble some. So the next one is called the filter bubble. Show of hands, who have heard of the filter bubble concept? It is essentially another extension of profile value. If you think about profile value, it is really them putting you into a category or set of categories, this is not my joke by the way but I like this. And so what that means on the Internet is sites are serving you things, they think that you are more likely to click on. In some context that can be good, if you are on a site you opted into and want recommendations maybe Netflix or Amazon you want that, but other context you may not. The reason you may not is because by definition if you are seeing things that are different than other people, that they think you are going to click on, that means they are filtering out things they don't think you are going to click on, that's why I call it filter bubble. They are basically putting you in a bubble what they think they know about you and filtering out what they think you won't like. In search engine context, this can be a problem, you approach searching with some expectation of objectivity, especially in certain subjects like political research. So this is from essentially the original filter bubble work that was booked, they found that, you know when people were searching Obama, more and more right wing people were seeing Fox News more, in aggregate this leads to a great partnership, people are literally seeing less opposing view points over time and just keeping themselves in kind of an idea bubble that they already agree with. Now we ran our own series of studies in the 2012 election cycle where we literally had people across the world, it was a descent size study, search the exact same things at exactly the same time, in cognito mode, we will get into that in a minute, seeing vastly different links, this exemplifies the entire thing. This is for gun control, after the Wikipedia link at the top it all went different. In fact some people didn't even see the Wikipedia at the top as the first link. In doing this study, this is kind of aside, we actually discovered something else, which when people, one of the searches we had people search was Obama and Mitt Romney, found when you search for  Obama, searches there after, turn in magic key word, so Barack Obama is a key word and then a search on gun control, you see three additional results inserted into the results that said because you searched for Obama on the first one, second two weren't labeled. Subsequent searches, correlated at all, issues, for example, you see additional Barack Obama stuff, problem with that is Mitt Romney was not a key word, for millions and millions of people that search for Obama, a bunch of extra Obama stuff in there, which was hard to notice and wasn't any for Mitt. I think this might of had a descent size impact on the election, I don't know what, can't really know. People searching for political things in a kind of mode of objectivity and kind of getting more one candidate. So that's the filter bubble. The fourth one is this notion of chilling effects which probably is very well known to this object in general in terms of free speech, but maybe less known applies to this context. So peer research has done a great series of privacy studies over the last two years trying to track people's behavioral changes to technology after finding about Corporate and government surveillance. Also just a feeling about what people think. One of the things they found is, you know, a quarter of people have started changing their habits around technology and they broke it down further into different technologies, including search engines. So the percentage for search engine was 17%. What that essentially means is 17% of people at some point in the recent past, americans at least, are not searching whatever, whenever they are. They are self-centering their searches because they feel some repercussion, either Corporate or government. And I think that's a deep chilling effect. When often times what I hear back about privacy is that it is like a 1, 2% issue. I think it is actually way more than that, some of these statistics, I'm going to share with you now basically share that, I think people think it is a 1 or 2% thing, larger thing is people are concerned and doing something about it. This kind of illustrates that. If you take a step back a little bit with some of these, you see even bigger numbers. So particular scenarios, what people kind of would be interested in and 40% of people said they were interested in a search engine that wouldn't track their activity, which is great motivation for us. But in general it is showing that if a larger percentage of people who are actually interested in this, if you take a step further back most Americans have taken some steps to map their digital identity online and this includes things like, you know, changing, making a fake name is what they asked and, you know, clearing your cookies and things like that. But most say they are unaware of really what they can do. Like the huge gap between education and this I think this plays into libraries to some degree. What people, people want to do and what people know they can do. Then finally like the further they step back um pretty much everyone, I think it is hard to get above 90%, so pretty much everyone thinks that people have lost control of their information online and I look at this as, you can look at this two ways. You can say it is kind of a glass half full or glass half empty, glass half empty is privacy is dead, get over it, which I obviously don't uphold. Glass half-full scenario is um you know, we have never really had the debate yet about where the line is for online tracking. So if you think about every other area of technology embedded in our society, so like medical, military, agricultural, financial, we have regulations around where as a society we want to place limits as to the use of that technology. We have not really done that for online tracking. And Apple are really starting to finally have that debate, which I think is excellent. Europe has gone ahead in this a bit with regulations to some degree, I'm not like proregulation person but I think this needs to have some limits at some point and that needs to be regulatory. You can argue about Europe's regulations, whether they are good or not, but that's a good argument to have because maybe we can make better ones. But in the US so far very self-regulatory. What has that gotten us, do not track setting in the browser that essentially does nothing. It is a voluntary setting that I find very misleading. Now we support trying to make it better and we have been involved in that, but right now like people go on their browser and they hit do not track and it basically literally does nothing and then the other thing that people mainly do is go incognito mode that says in bold now, it has gotten bigger. So cognito, doesn't hide browsing from your employer or Internet browser, yes, no one understands this. This is one we are going to do and try to figure out what people think of private browser mode and what it actually does, our hunch, we have a lot of anecdotal evidence to support this, everyone thinks you are anonymous on the Internet, which it doesn't mean at all. It really means this. [laughter] That was my one joke. [laughter] [APPLAUSE] So right, so what can you do about it? Well at the moment you can support organizations that are really trying to push forward real regulation here, like EFF. Then you can use or support in some way that you can, services that are trying to take privacy be design approach like DuckDuckGo or tools that you can add on to your browser like EFF has a bunch, like this everywhere. What we have learned through transitioning into the second part, is that it is not enough like we have this term called frictionless privacy. People do want privacy, but they don't want to give up that much for it, which I can understand like in the search engine context you want to get your results. So we have realized that it is great to have privacy, but we also have to have everything that you expect and we hope more. So what, what do you expect now from search engines? You expect, you know, quick information at the top. So like when you search you get some of this or something. You expect images, you expect news, you expect maps and local and so we worked over the last two years really to bring, close all these gaps so that there is no excuse for people not wanting to try this out. And we think we have gotten pretty far on that. People who have tried us out in the past should give us another try. But more to the point, we don't want to just be like me too, we want to go in our own direction and have a better search experience, you know, that Google still won't go and so our answer to that is duck duck hack, the second piece I want to talk to you about, open source platform. So when you are searching you generally want an answer, that's our main premise. Basically, we have been ahead on that, like we started doing this right from the beginning that was one of my pieces for the company back in 2008. But all the other search engines have come on board to that now, that's me something an answer above the link. So you would rather answer your search, at least partially with zero clicks, instead of digging around in the Web results. Now if you think about search there, you can divide it into lots of different categories. So you are searching for images sometimes, searching for restaurants sometimes, depending on who you are you are searching for very niche topics, if you are a database administrator that does a lot of SQL, you have a lot of SQL, if you code in Pearl, like my staff, you have a lot of Pearl queries. Each of those areas of the search has a community associated with it, generally online. And generally over the last ten years they built up a lot of structured data around that community. So be it accessible to API or database or Wiki or even off line not too queriable for the Internet, what we are trying to do is empower our users who are in those individual communities, to come in and say I want to make my own search experience better and that is where everyone who is in my community and help us connect that, those right APIs to the searches. That's kind of the vision of DuckDuck Hack, what do you get when you do that? People are interested in all sorts of things. We want the search engine to be the best for each one of these categories. So this one comes from, you know, the Wiki, you get this stuff or any kind of professional or academic pursuits. You get, you know, my SQL as I mentioned and really if you think about building out a space -- I think we are just still in the infancy of this. If you think about building out a space of the best search experience for say my SQL it is really not just this one, it is a set of instant answers. I'm going to illustrate this, this is a cheat sheet we call it, just a quick reference guide for my SQL, I have about 200 of these in the search engine right now. And then we also have like deeper information for things that would not make it into a cheat sheet but any kind of key word associated with my SQL from reference documentation gets put in this kind, then we also have a, we pulled in the database administrator stock exchange and we have a full text search of that. So you can start to see that if you approach an area from a lot of different angles you can start to cover a descent portion of say people, my SQL period. That would be our goal is to do that for each of these communities. So you also get things that you don't expect. So like someone like this, which we love, we would never made this ourselves. Like there is a site that tells us who is in space all the time and they have an API and so now you can decide in space. And we had made, for ourselves, a long time ago, a very simple color, I made it initially when you design the site you would type in like a Hex color and see, see other colors and things. Then someone in the community came in and took it like interactive and made it just way better. So it is just kind of another area where we think that the platform could go, if people are interested. So like taking in my SQL thing you could make interactive around queries analysis or something. And then finally like it runs back end code too. Someone made this QR code generat generator, Pearl, whoever codes in Pearl has Z-pan a gazillion modules, oh we can put this on search engine. You can do cool things like that too. My illustration here is just the possibilities, I wouldn't say are endless, but they're back and we want to support cool and interesting things. Okay so tying those things together. We recently over the last six months like did a reevaluation of vision mission as a company and we settled on this vision, raising standard of trust online and essentially what that means is we want to exist as a company that kind of proves to the world that a company can deliver, can compete in the major market and deliver a real alternative that also has privacy via design and that will prove to everybody that, you know, there is a standard like standard doesn't have to race to the bottom, that's not inevitable you can have a product that works up here. Our mission, which is how we're working on, you know, implementing that vision over the last two years and going forward over the next two years is to build the world's most trusted search engine. Mission statement, trust doing double duty here, both that we want to inspire trust in our users to be able to search whenever, wherever they want, so gets rid of that chilling effect I mentioned earlier, as well as reduce privacy harm of the other one. So, you know, people tracking you across the Internet essentially and putting you in a filter bubble. But it also means that you actually have to trust that you are getting results that up want on DuckDuckGo. And so you are not having any major sacrifice. So it really is serving both of those kind of methods of trust and actually internally trust is our main value internally. Which has different implications, but it leads to things like our work wherever, whenever policy, that kind of stuff. So with that, call to get involved, if anything I said interested you check out DuckDuckGo or duck duck hack and I'm happy to take questions. I'm done. [APPLAUSE] Great. So questions? We have microphone runners, so please raise your hand or make a signal that you have a question. [speaker off mic] How do you think the quality of your search compares to Google in terms of the amount of resources that you index and the results? So we, early on I decided, first I started calling the Web myself, back in 2010 before incorporated thing, then quickly realized that was a complete uphill battle and that's how Google wants you to compete with them, that's how a lot of search engines died in mid-2000. I went and looked at Yahoo and Bing, and index also had English index that was coming up. I looked at the linked variations and it wasn't that wide actually. Microsoft had done a study where they had actually taken Google's logo, put it on their results and people perceived them to be better and so from that I concluded that like the links themselves reached diminishing returns the real value was on the instant answers. So I decided to not worry so much about calling the Web and use other ACIs for that and try to add value on top via instant answers. For many years we did that, we did crawl the Web, but for a negative way, a lot of spam in 2007 and 2010, a lot of links you clicked on were bad. We crawled the lab looking for spam and aggressively moved content farms. Even around 10, 11 most search engines, Google and Bing got way better with that. Nowadays we don't mess around too much from the links, we get from Yahoo and Bing, Bing is doing a good job with that. We still work hard to do things to the query to make the resultses better, like Google over lots of time has stop treating your query exactly. They do a lot of rewriting, so we heavily weight against that. So the links will look vastly different on our search engine than will on Bing and Google, but to answer your question I think everything is there. You are not missing anything there and we're hoping to deliver this search experience part on top getting instant answers. The other way to look at this, a long answer, but it gives you some background. When you, when you look at a search engine people click kind of in a power fashion, they click on first link way more than second or third link. The instant answers, they are on top completely. So if you have a lot of instant answers they actually transform the search experience. We have about a third queries now with instant answers on top. And we want to keep increasing that with this platform. Just curious, how do you fund yourself? So we are now profitable, which is great. [laughter] [APPLAUSE] And we plan to have that be the case indefinitely. Before that he we had fundraising, before that I could self-fund it mainly for first few years. Then when we expanded the team I didn't have that amount of money. So we raised some money for that. In subsequent years as we we have grown we can support ourselves, we have ads they don't track you. There is no good reason that Google is tracking you for their search results. Most of the money on search is made based on your search intent on the query. If you type in car you get a car ad or mortgage you get a mortgage ad, all that can be done per query, even locally, the local information, without tracking search history or what not. The reason for all that tracking Google is really advertising company, they run four ad networks in the world. They run ad mob for Apps and double click and fourth one escapes me, in any case their ads are on millions of site including their properties that don't have search intent like YouTube and g mail, they need this to make good advertising across the Internet, we don't we're just a search engine. We can make money off of search ads and still throw away your information. Hi there. As a privacy advocate I'm often kind of DuckDuckGo to friends of mine, one of the selling operator is the operators which you didn't talk about today. But I have some friends like who are the kind of people who will always be logged into Google and the way they want to use is to run search but with a bang operator for Google, so they are getting encrypted results on Google. I wonder if you have done any studies about whether or not that's getting them out of Google's filter bubbles, if they are logged in into their browser whether or not that's keeping that search history from being tracked with their account. Yeah so I think it is totally tracked, in 2012 study with election we ran, everybody had complete fresh browser install, get rid of cookies and everything and the, they were still individualized based on previous history. So like I think even if you are logged out or anything seeing the browser fingerprint, I think even further. Customizing based on that. I don't think it is protecting them very much. I had a question about something user experience and privacy question. So coming from libraries quite often we get folks suggesting features oh do an Amazon thing like you like this book, here is another book. Because we don't want to track that patrons data we can't offer those things. Some folks say let's do opt-in things to sort of balance those questions, but then as anyone who has ever clicked on i tunes agreement thing we know we will opt in for anything. Give us the stuff, use my FaceBook pictures, that's fine, just give me the stuff. So how, as a company that values privacy, do you kind of balance that like what people are willing to give up versus what we should let them give us as you move forward with trying to think of features and things like that. Yeah, great question. I mean, us in particular, I think this applies generally, I think people jump to personalization too quickly, I don't think it actually in most cases yields that much additional benefit. There is a lot of just aggregation that you can do anonymously that often can get you most of the benefit there. So our first approach, and pretty much our only approach to date has been to not collect any information and work on the search problem in aggregate and we can do that to some degree because we have a lot of information, we have a lot of searches now. But I think that generally works well. Now the other flip side of that, yes the opt-in, can give a whole other speech on this, which I don't. I will try to keep it short. But you know, I think, this goes back to regulation thing, I think the privacy policies are just bad. But that doesn't mean they have to be bad. So you can imagine much more transparent privacy policies using iconography and other things to call out the bigger risk. If you are familiar with mortgage and credit card kind of legislation in the country the last ten years, we are now billing statements and mortgage application statements have to be formatted in certain ways and the risks called out big type at the top. You can imagine privacy policies having similar regulation where things like that need to be called out. So I think in that world or in a world even where the company really cares about explaining it and makes it more of a quid pro quo, I think you could have a good privacy policy and still collect good information, not great examples in the wild. Hi there. Are you focused solely on search or are you planning on in the future getting into the social media space or something else? This We thought about other areas but then, you know, realized we were kidding ourselves to do anything but search. Search itself is really, really difficult for us and so our approach now is to, just do search and try to do it well and try to encourage other companies to pick up the areas. Try to get other companies doing the other areas. Oh good runner gets to ask a question. I used to work for Northern Lights Search, got eaten by Google, I have been a DuckDuckGo user since you announced -- Thank you. Looking for a verb though which people say ask me something, well duck duck went and found -- -- [laughter] Yeah it is a challenging one, we say duck it, but it doesn't catch on. [laughter] [APPLAUSE] But I want to thank you for the search and for keeping, fighting the privacy side. Thank you. [speaker off mic] I wondered what you think, when I think about the American people and, you know, we have been hearing a lot about American people in this election season. There is a lot of ambivalence, a lot of anger, some people are antiimmigrants some people are, you know, I tend to think that they are all in favor of privacy until they think about bad guys. And, you know, they think about ISIS or mass shooters and suddenly they don't want those people's privacy to be protected, they want their own privacy to be protected. So trying to figure out what my question is in there, I guess it's, you know, how do you -- what would your response be to somebody who says well I think your services great but I don't want, you know, an ISIS person to be allowed to use it. I want them to be tracked. Yeah so I think the one interesting thing, if we delve into this further the last two studies, last May, relatively recent. They actually made, it was a scenario study. So they did, they asked very specific scenarios kind of like what you are saying, of different kind of context, would you be okay with it in this situation, this situation and this situation and what was found was very nuance answers like, you know, there were swings in different cases, but it was never like, never got to even over like 70% and so what I take-away from that is there are going to be people who don't care, would want to, you know, people to be tracked in different situations. That may even be the majority of people. But it is not 100%. It gets up to like 60%. So there is a descent size minority to kind of, at least from my perspective, feels like they are making the tradeoff there in kind of understanding the bigger picture. So I hold on to that percentage is my basic answer. I have a question. So I heard stories about things like Google results showing things that Google says are unintentionally racist or otherwise pragmatic, it used to be, I don't think this is still the case, if you did a Google map search for N-word house the White House came up. Is that something you are thinking about in your algorithms and any kind of social justice perspective you are bringing to this? Yeah, we do, it is really hard to do though in all honesty because you, you -- it is hard to find them first of all. Like they often crop up and then you get notified. It is been hard work on that, we think more of the perspective filter bubble question, trying to serve the people, more objective piece and make sure that's happening. I don't have a great answer for you though other than we do think about it. My question is about transparency of the search algorithm, like recently Twitter, I don't know if it is in effect or not, but in attempt to make more money, has, you know, started using algorithms to show you what they think you want to see in the filter bubble as the approach. And, you know, the whole trust issue with knowing who sees what on feedback or things like that, do you do you have a way for people to know how to, what the algorithm is doing in terms of how to search results are prioritized? Our approach has been first serve people the same stuff in the link. Then in the [speaker off mic] with duck duck hack, that's all open source. So you can see how that whole thing is working. Not a lot of people seem like they are mucking around in it, in the algorithm piece there. But it is there. In the link algorithm itself, like I was answering over here, we rely on a lot of other APIs so some of that is confiscated from us. So there is not a lot of reveal we can even make there. And so the places that we can reveal on this we have just been trying to make it open source. Obviously, haven't had a lot of time to look at go how can you take this data and tie it into it to add richness to that and make the library more accessible? I probably should have a better answer to that. [laughter] The answer is we would love to work with anyone who wants to make the search results better. So if people have kind of structured data they want to showcase and make accessible to a search on DuckDuckGo we would just join the community and start asking questions. We will route you to the right place and talk with you. Where are you on schema and Fedora? We as a small company have not participated in a lot of standard organizations in general, just because we haven't had the resources. In terms of like formats in particular we basically made our own, you know. Like we had duck duck hack and our we don't feel like we have too much of a roll in those places. I'm not sure if that is answering your question there. A lot of us have been investment in being able to structure our data there we want to get out of the Google rankings, but that's an entre, might be an entre, we think it is pretty weak, a lot of structure, change to API. Yeah I mean we end up like a lot of what we do is, if you look at duck duck hack it is just translation wrappers around different APIs. So yeah again like we would love to, we would do the work to make it work, you know. I love the duck duck hack thing you have, I'm curious if more of this pops up, how do you deal with priority and approval. If there could be multiple information to show up, how do you decide which one shows up and who gets the quality of what goes in there. How does that process work? The vetting process, we're trying to do more and more community empower, we have community leaders, do first crack as we are doing things code wise and design wise. We have been putting out guidelines and things like that. So for the most part that's been working pretty well. Guidelines and stuff are followed and what not. Interesting platform because our bar for showing you have stuff needs to be better than the link. So those are the kind of guidelines we put out but some people they are not us so they don't really know where that bar is. To answer your first question is we have internal metrics to basically know when something is showing if it is performing well or not based on if people are interacting with it or not, that is essentially feedback loop to decide what we should be showing at different points. The main answer is the ones we have been doing ourselves like images and maps. If you search something ambiguous anyway, like a place, you may want a map, you may want Wikipedia page. So that logic is all, is all the same kind of thing like that, what people really want to see in that frame. To answer your question, we haven't had a lot of overlap yet like and there is so much white space there like most things are not instant answers yet. Smaller communities. So hasn't been a lot of overlap, overlap has been mainly on our side. You find is hard to maintain technical talent like Google, or find mission of your company is enough to attract that talent and does a way besides monetary benefit. Yeah where so we, we hire pretty much everyone from our community. So super mission-driven. We have had only I think two people in the company ever. So it's, we have had no problem recruiting people but we don't actually go recruit people very much. We don't feel like we are in that race with the other companies. Thank you again. [APPLAUSE] All right we have a ten minute break. We are back here at 10:10. For those of you who are presenting for the rest of the morning, if you have not uploaded your slides on to the presentation computer, please do so now. See you at 10:10. (break) We are actually back two minutes late and we actually have overbooked our lightning talk session. We were supposed to have eight, but it went to ten. So before we get started just one, or actually two announcements. First, you might be wondering what this little fellow is. If you went to a particular museum the other night, this is the plushy form of the mega colon. It is going to be up for one of the giveaways later today. And if you are going to be around for the afternoon there will be hotel space for birth of a feather session, already a couple already organized, so fyi. Now let's get the show on the road and for our first lightning talk is finding a digital collection integration. Al Thanks my name is Shawn, Duke library, I'm going to talk a little bit about things we have been doing in making digital collections work more harmoniously together. I'm going to show a photo I took at the Motor Museum, you can open your eyes, just this. This is Benjamin Rush, he was a prominent physician here, also a signer, we digitalize papers and put up online through hydroblack light digital repository, UI. Like a lot of places we have done a poor job of collecting digital objects back to the content. We are trying to fix that, right. Our new UI, providing a little bit of the taste of the collection when you mouse over it. But more importantly we are now storing the archive space component ID in hydro, so we can provide a link to get you to exactly the spot in the finding aid where this might be described, could be a series, folder file, doesn't matter. If you do this before you know they are really long, lots of text, kind of cruel to jump somebody right in the middle of one, but notation device. We know we needed to change that, fix that a little bit. We didn't have luxury of doing full-on makeover like University of Florida folks showed us, we need to do like a fresh haircut, like a little bit something different on top. We created a sticky title, in addition to sticky title the corresponding series information up at the top. We put that series, you will jump up to where it begins, often helpful description there that helps things in context. Back on item level we also added links to view images, you can click that link, through the magic of the trip A image API you get a nice smooth zooming representation of the items right within the finding aid. Also as you navigate up and down through the finding aid, that series information doesn't just say where you originally landed it updates based on where you scrolled to. The magic behind that is bootstrap tool JavaScript, it is originally designed to highlight different menus, different items on a menu, sticky item, start using it in a different way. Also worth noting we don't just use bootstrapping, we are just something we can add very easily to the UI. I think it makes a big difference. So some quick take-aways on finding, we can use bootstrap without actually using bootstrap, making small differences can make a difference on video collections and getting a lot of value of using triple IS image API, this is wonderful. The more we can do to connect the dots between finding aids, digital collections the better it will be for our user. Not perfect, we still have a lot to do. Still not a very automated process for doing mapping. We are not quite sure view item, makes a lot of sense to a researcher. We can use ideas of how to do that. We have a lot of other things not just image content that are working on making this connection as well. That's all I got. Thank you. [APPLAUSE] Up next is our Code4lib limits. Hi. I'm Chad and my first Code4lib was in Seattle in 2012, it was a great time, I loved it. It was really wonderful. At that Code4lib Chan gave a really moving and personal key note that ultimately came down to being about how we as a community had screwed up. Limited registration to 250, I think 250 people that year and it was a large number of people on the waiting list, a lot of people didn't get in and Dan made it really apparent that that was a problem for the community. We shut a lot of people out, we didn't invite people in and we didn't do what we needed to do. So in 2013 code biggen again, we registered 450 people this year, but we still had quite a few people on the waiting list, felt bad about that. But frankly the bigger venues sucked. They, they were like big Corporate monster hotels where we share this room with three other conferences or share the hotel with three other conferences. They had huge rooms for like seas of vendor tables. I don't know they just didn't feel like Code4lib places. They didn't feel like intimate in anyway, we decided we didn't want to do that. We thought a lot about what it meant to have this feeling for every organizer every year that we have to grow the conference and have to grow the conference. As much as I hate finding myself contradicting Chan, I just feel like I must be wrong if I'm disagreeing with him, I want to say to every future organizer, don't feel like you have to make this conference bigger. Don't feel like you have to grow it. Because that's, the real cost to the organizers of making it bigger. Because a bigger venue also means a much more complex conference. And I don't feel like anyone should have to necessary do that. A bunch of volunteers, that have never stared down the barrel of a huge contract with a hotel before. So that kind of brings me to my second point and that second point is this conference registration is too damn cheap. And I know it is a point of pride for a lot of people in, in this community. But I'm going to tell you that it limits the risks that the organizers are willing to take. When you are looking down this huge contract and you have never organized a conference before and you don't have that experience, that looks really scary. When you add on to that, that registration doesn't even cover those costs and you are going to have to fundraise and have never done that before, that's terrifying. So you become very conservative, you don't take risks. You don't do things like offer discounted rates to library school students or people self-funding or people from diverse backgrounds that this community, that are still not well-represented at this community that we talked about. So this is my um, this is my speech to future organizers, again you are off the hook. If you want to raise that price, I think you should feel free to do it because there is a lot of organizations here that could totally afford to pay a little more so we can let other people in so we can make structural changes at the lowest levels of how we organize this conference to take those risks. So -- I hope as a community we will support them. I would just say that in the future anyone in the community that doesn't think that's a good idea, please don't yell at the organizers, yell at me. Because, you know what, I don't be organizing a conference at that time I will have emotional labor left to tell you that you are wrong [laughter] and that we can have that fight, but don't do it with them, they will have enough on. Thank you, I love Code4lib and I want to see it grow, so those are my thoughts on how we keep it. Thank you. [APPLAUSE] Next up github as a knowledge base. Okay. So I'm Heidi, I'm with NC state, that's not working, okay. Okay. So I'm going to be talking about using github for things that aren't code. So this could be documentation, instructional materials, in my case I will talk about templates for high tech wall and spaces. Across the libraries we have four high definition video walls in public areas and four high tech rooms, we invite members of the NC State community for this content for these walls and spaces. As you can see they are various shapes and sizes, so that can be a little daunting. So we specify things like dimensions, recommended font sizes and people view these walls at, from different distances. Guidelines for where bezzles falls and other general weirdness of the spaces. So for example, this room where one of our library spaces we would probably have some guidelines that say don't put important stuff at the bottom half of your presentation. Now getting people to follow those guidelines is a completely different talk that I don't have prepared. So created across several departments and saved and made available in various locations. So say if we had a patron come in who wanted a template recommended by an NCS [Name?] they talk to some of the libraries and get appropriate template from the Internet or library website or server or Google Drive or from their own desktop or another librarian and there might be situations where somebody who is sharing the same template over and over they save it on their desktop, so you never know when it is being updated. So obviously that's not ideal. What we want is a solution, we didn't want to go to the moon, but wanted something that centralized and accessible by patrons, wanted ability to collaborate and comment, version control and basically wanted everybody to have access to highest quality and most recent available template. Hopefully this would also present existence of templates created in one department unbenouncedded to everybody else. So github has obvious features that made us interested in, I will talk about specific ones for templates. You have desktop is an easy cross platform way to contribute to projects. I don't mean in the command line 95% of the time, if you are coding you are probably in the command line, it makes sense. If working with things like photo shop or PowerPoint, this fits into my process a lot better. Now github also has a way to drag and drop files right into the Repo an website, which is nice. It also displays Jpeg, photoshop files and ways to show differences between versions of images, you can see what the changes are. Also github is also being used in the library, so that made is it a using pick as well. This is a shot of the read me for the Web page. We made a table so people could see immediately what templates we had available for which spaces. So while there are many good features there are some reasons github is not a perfect solution, not the most intuitive site to use, this is a quote, co-worker, one of the more technology aware designers I know. That person wasn't alone, I was kind of confused at first too. So some things to think about. Think about the templates and organization as the process that is generating them. Part of our challenge is the templates were made across departments, if we had somebody interdepartmental sway that could of encouraged collaboration, that would of been great. How soon is soon? Ideally, you know, we would of talked about these things at the beginning, you know not three hours later, kind of got from everybody. Documentation is important, Mark down is simple. Think about user experience related to your organization whether or not it is tied to internal processes that don't make sense to anybody else. The act of gathering and organizing files like this provides great opportunity to gap analysis. Consider limitations of github, you can't put files greater than 100 megabytes in github, there are ways around that, you just have to plan for it up front. Also github will make zip file for you of entire Repo, not individual files our repository is very large, that's not great. Perfect documentation, figuring out how to make individual zip files might not be actually worth the time up front. Consult with others, if you are using github for code may be using something like MIT license, not appropriate for non-code things. Failure, something probably won't work. Thank you. [APPLAUSE] All right. Next up leveraging the public cloud for iOS. Hi. My name is [Name?] developer and I would like to talk about leveraging [speaker off mic] drive. Next generation is reduce team structure in the library. We would like to take that even further with what we use, compliment [speaker off mic] so work environment we no longer look like this. Again we're also not here yet. But we're getting closer. So we want to help get there. Do want to help some of the more forgetful students via sending them, remind them it is due tomorrow. So what do we need to do? First we need data source. For example we use the reports, we can use any data source from iOS and we need to retrieve and process the data. And lastly we need to send SOS to the student, we use a cloud communication source. [speaker off mic] URL on top. So to get down loads, we call simple API, parts results, group the results by the user and send each user estimate API, which is also really simple. Okay. So we have our cool little script and one -- we don't really want to set up a server in our local institution to do this. What do we do? We can leverage the public cloud. For this example we use this which let's you upload your code and -- infrastructure. So what do we do? [speaker off mic] Creating a zip file creating the scripts in the modules then we deploy by defining lambda function, find language and entry point and uploading the zip file, we schedule to run daily by creating this source. That's it. So to summarize, we start with a data source, we wrote our script which chooses the data using API, [speaker off mic] zipped it up, upload to lambda, schedule it to run every day. Did I forget anything? Yeah that's us. Thank you very much. [APPLAUSE] All right. Next up stupid, system JavaScript that makes you a hero and how to win friends and libraries. Okay, sorry, wasting more of your time. Okay so it this goes stand up in front of this room twice, I felt like nobody was drilling home the power of communication. I want to give this talk to drive home power of communication. I want to start off by if you aren't familiar or just a way of reintroducing you to this, which is a group all about communication. It is about meshing up Human Resources on data development side of the house, it is a community, it is a banner, a series of tangible meetings, Twitter chats and other efforts inviting immediate for increased communication between IT units and meta data units in the library world. To do some things with library data we need to be working in tandem and we need to be communicating. I would say that we need to be communicating informally. So it has been said before my answer to every problem is what is Wiki. So happy hour can be a really great opportunity to spark communication among co-workers and really a great opportunity to spark crazy ideas, so that's awesome. But Whisky is not required for communication, you can achieve communication through many needs, maybe ten minute standing meetings, over coffee, in the Hallway, whatever you have to do. Make friends with your co-workers, talk to them and make psychological safety where you can ask questions, pitch ideas, work together. Okay. What happens to get people talking informally and working together, you find out about all of the tiny things that matter. We can be bogged down with the big project. We tend to be focused on long-term goals and kind of like meetings and certain stakeholders and maybe missing the needs, tiny needs, of other people that matter in the workplace. So what happens, to illustrate, when I made friends with colleagues and pick their brains about their areas of needs, one thing I found came up again and again, it came up at my library, came up at other libraries. I wish I could search on such and such. I wish I could search by printer, by date, on this or that. I wish the search interface took full advantage of the data that we have, the data that maybe I create, the data my users need, data that exists, basically this may boil down to I literally need a search box for that thing. Stupid simple JavaScript that makes you a hero. So basically just create a search URL and reassign window location. This is such a simple solution that anyone with basic HTML skills can be empowered to create custom search box they need. So simple that you could put code generator, something like that. But we had to have the conversation about this comparatively small need in order to address it. So I want to pitch a couple of action items for the room. Whisky is great, but really communication, however that is achieved. Found out about the small things that matter to your colleagues. Then maybe set aside some time every week, every month, whatever it is, to work on those tiny projects, put that into your day. Why? Because it fosters partnership and collaboration, it fosters brainstorming and it fosters service. Be somebody's hero, be the life-saver for someone and make friends with your colleagues, please. [APPLAUSE] Next up, single sign on identity and system integration. And for reference, I forgot my lanyard, if I was wearing one it would be blue. Got it. So what I would like to talk about today is single signon and authentication. It is a little bit of boring subjects sometimes, but you got to remember your user data is library data. So we spent a lot of time worrying about what is the system record for our monographs or manuscripts, the fact that many scripts require information names, mail addresses, often gets accepted as a given without considering what the downstream effects are. Those are often lots and lots of library accounts. Even though you may have a single signon system that makes it feel like one thing, you will often find with URL account, Digital Library account, as you get more and more services you have more and more library accounts, places where your data is stored. How does that affect experience? If you have robust identity provider, maybe not so bad. If you are signing up to use special collections or other library services you do a lot of typing into a lot of forms that is often not very ergonomic. If you look at other solutions you might be familiar with, it is something that we try to deal with over and over again but they seem to be dealing with problems a little different than what we are working with in libraries. We are often working under institutions that grant identities in a concrete way but need to have the authority to mentor our own. Essentially somebody comes to the desk without ever meeting the services or any other identifying body they want to use the materials and we don't want to have to turn them away. Looking to the keyhole about how we can deal with that, think of identity as another service, something we can look up on the fly or provide to consumers if needed. The way we found best to do that was technology called open ID connect, over OF2 [Name?] I'm not sure how many people are familiar with that, it allows you to have identity model essentially claimed-based. Any user can have arbitrary number of claims or factsassociated with an account and single subject that can be passed between applications seamlessly. This means you can access scope information, get somebody's e-mail address, home address to another application, or just their name to a third. All associated with the same data source, all with the same identifier but passed in a secure way. It means a lot of open-source clients and servers already exist for providing this. And so what do users see? They get a single signon interface. In our case, we split it up into [Name?] that's what we use and what people are used to. And gives a great opportunity to revisit, I hope nobody else's looks like the one on the left. What it looks like on the back end is critical separation of authentication from one, two, three areas. Instead of having people you a authenticate directly into Apps they are into service provider and when the application goes to retrieve user information it doesn't get it direct, it asks for information from central provider, allowing us to control exactly what is provided. What that translates into, it is really need, applications can specialize, pass information between it knowing they are talking about the same thing. Downstream you can set up trust resources, one of our use cases has authorization, logs into streaming video provider, even for non-institutional users this is talking about the same person, that they have access to the video. And finally, what's most important is that open ID connect server means you have OF2 server, this means you have a secure way to grant access to APIs. This is really, really exciting it means you cannot only get this integration I'm talking about but also means you can start setting up a word where mobile Apps can integrate in the, people can signin with own credentials to access iOS, research sharing and user account. It gives you a secure way to open up in a scope way for student developers. Where are we, use local users for login, library applications can specialized more easy and you can without side providers if necessary. It might seem like talking about security with authentication, but we are trying to talk about how to open Web systems. [APPLAUSE] Next up, code art. Hi my name is Allison, from NC Libraries, here to talk about a project called code art, this is a project I took over managing last July and in second year. It started as contest for students, for display on large scale video walls in the library, which Heidi mentioned earlier. This is art created with autonomous system and can run, for example, on computer algorithms. So the library opened in 2013, four video walls built into the public spaces of library were intended to be canvass for the library to show student and faculty work. So the code art contest was created to advertise this, it was sponsored by digital systems, maker of [Name?], the competition with a substantial monetary of prize, hundreds of dollars for first and second place winners would be attracted to students, along with getting the exhibit to work in the library. Another aim of the contest was to get awareness, coding, and encourage students to learn to code who wouldn't of considered it a possibility. Making art with code, this includes processing. Last year in 2015 the contest structure required interest students write a written proposal, they competed for the final judging. The projects were developed over a few months. The outcome of last year's contest was that we had two very impressive pieces produced for video walls, created with data, code and stand-alone art. The winner was Forest, entry microcontroller to make trees that grow in a planet, so sun and moon revolve and serve as hands of a clock. The WKP visualizer visualizes, birds flying over the sky line of Raleigh. It was visualized in the building with the light flowing up and down. Taking over the project in July I set some improvement goals for 2016 including more student participation, more diverse participation in terms of students participating in terms of their identities and also program of study. More faculty involvement and mentorship of participants who might be interested in entering the contest. One challenge, and potential opportunity there, the pool of students who already make code on campus is pretty small and hard to identify. Very few courses on campus related to making art with code. The coding there in computer science program about a thousand undergraduate and graduate students, not clear how many are interested in art. How many are interested in coding, the design and using digital design tools, however. Creative, eager to help with advertising and mentoring students. Also creating new classes, involve creating coding in some fashion. The deadline is next week for the contest. Planned a series of events in maker space that allows students with no experience to get hands-on experience and make something, these creations would be eligible to submission to the contest. Interestingly, while the workshop and hackathon, no experience necessary, they drew different audiences. Perhaps people waited to enter because of the title hackathon, maybe a certain kind of competition, workshop seems more accessible. Just last week, the studies from national University of Singapore, highly competitive settings woman made weighted, qualified woman may be discouraged from competing. Due to structural courses in society, competitions may not be the best way to identify talent. Most talented may not be competing. It may be that more non-competitive programming is a key to building this on campus, we can support this year the modest gains, we have modest gains in number of woman, students of color and non-coders who participated in the program in contest more specifically, we have more work to do this. This when includes shifting focus from being just a contest to more robust and inclusive program, more opportunity for underrepresented students. I believe we can develop community for everyone that wants to learn to make art with code will feel empowered to do so. Thank you. [APPLAUSE] Next up opaquenamespace.org. Hello I'm Ryan and from Oregon state University, I'm here to talk about a joint project, kind of stemming out from from Oregon digital, so Oregon state University and University of Oregon. So opaque name space got a shoutout from Steven earlier. I want to talk about it a little bit to talk about what role it fits for us. Organ digital.org is joint hydro instance, finally created migration and as part of that process we kind of went all in and that's had a lot of strings and meta data and we wanted to have a way to have IDs for say our local vocabularies or local people, people who wouldn't be in LC names, things like that. Kind of drove this, the other reason we kind of wanted to define new predicates so we can use Oregon digital, have at least some definition and maybe others would want to use them too. So the app that powers this is called control vocabulary manager and that is available at github. There is a link on the home page. A little bit about the name, so back in, I think it was 2013 um several of us went up to UW and then coming back on the train and at the time this included Tom Johnson, Karen and we can't remember who came up with opaquenamespace, but I think it was either Tom or Karen and then Karen pushed for it. Others, others were lessen Tuesdayeded, but we went with it. Also Jeremy and Trey and myself were there and so I think Tom registered the domain and we went with it. We wanted something others could use. Just to show a couple of things real quick. So we used for Oregon digital, we used from this or creative commons, but we gathered a couple others. Again we needed URIs, so we added them, we felt at the time weren't used elsewhere. We can have own local creators, a lot of this came out of [Name?]. Some of the things listed on this vocabulary page are just predicates, we are working, others on the team are here Linda and Josh, we are working on them quite a bit. Yeah here we go. So this is our local creators and as you can see there is a whole new bunch of, a whole bunch of local URIs we created when there wasn't a known connection to another authority anywhere. And then if we do find them, whereas we kind of resolve names or entities, we can do it same as or we can do another connection or even depric ate a term. We are looking at expanding the field we have, expanding kind of vocabularies we are working to model our academic units of the University back through time and have that all in here. Because again we will need that for our repository. We might also use it, we could use it as subjects in Oregon digital or possibly future app. Another thing was building, campus buildings. I was going to show this. This page badly needs a sort and there is a ticket for that. Oh here it is. So to give each building a URI or again building often change departments or change, you know, affiliations and we want a way to reference those. This is a start and it works and gives us labels. Yeah if you want to learn more about this or have questions or want to use it, you can contact us through Oregondigital, thank you very much. [APPLAUSE] Okay up is meta data, what -- This project came out basically it is a one off project, sparked by a conversation in a social event at the library. It fits in well with the model that Allison covered in her talk. Basically the problem was about once a month the east Asian studies library gets a box of materials from the office of the Library of Congress in India. Included in this when, included with it is meta data describing the material. But to provide the material the Dahli office takes marked record for the books, prints them out, scans those pages back in, bundles the pages into a PDF file and delivers that to us. Awesome. [laughter] To which I say, what? And so of course the question was, is there something we can do with this so that we don't have to manually reach this and lookup or import or manually rekey these records. And of course my first answer was, ask them to change their workflow and you can just do this and you are done. [laughter] But apparently they are not able to do that. We asked them to, we got some response, we can't do this, I don't understand why. You have the marked records, just give them to us. So apparently it is not going to work. We have, here is a, the PDF image. It's not bad, we can work with that. So if plan B was something involving OCR. So split the PDF into images, these images to create marked records and deliver those. But after OCRing it, it is pretty good but not good enough that cataloging department wouldn't have to do extensive manual edits, which would probably be as much as work as creating in the first place. So dirty OCR would be bad records. In reading through them I discovered that every single record contained O1O field containing LCN number, like uh-huh, unique identifier. Plan C of OCR + LCC and lookup and started working on that and split the images and OCR and get the LCC in, look that one up and deliver those records with cataloging department. In the test case for the first file, it worked great. But some of the records were already in our catalogs, we had to do some duplicate removal as LCC in, really isn't unique, sometimes occurring multiple records from OCLC catalogs. The very best problem was some of the pages in the PDF, upside down. [laughter] So went at it once again, added a few more steps of split the image, look for upside down images and fix them. [laughter] OCR mode, get the LCC in, look that up. Look for duplicates, get the best unique record and deliver those. In the end pretty much a success, small tool, one-off, in one small corner of the library there is one guy who is happy. [laughter] Thank you. [APPLAUSE] [speaker off mic] [laughter] All right. Last but not least, phone. All right, hey everybody this transition will not be smooth, give me just a second here. There we go and full screen. And we're out. Hi everybody I'm James, everybody say hi James (audience saying hi James) oh that's great, I love those kinds of reactions, the first thing I'm going to show you again as I spend five minutes bragging about what we are doing in the was natural library, in our discovery layer called Pica, first thing I'm going to brag about, every family member, every friend and library user that I've explained it to is like yeah that's the way the catalog works, what cool thing did you want to show me? And so hopefully with this audience I might get a slightly more elevated response than that. We're going to find out here, I'm going to call it Folber once instead of Furber [Name?] so was problem that we have of bringing same files together. We call it record grouping, when we are really bragging amongst ourselves we just call it awesome. An example of the problem is three years ago, almost to the day, our Mayor announced city ride read of Life of Pie, we had 20 versions of editions and different bibliographic records, four separate overdrive things. We ended up with patrons with long -- on one title and available copies on another. Which, in the world of overdrive and automated purchasing, wasn't working out for us too well. So and so with, and so here we see on the, on your left the old version, you know, we have them in a crazy chaotic arrangement, a couple audio books together, versions maybe another audio book on the next page, then print the print and eBook version and it is very difficult to get the thing as quickly as you can in the format that you would prefer. Or if you are format, if you don't really care about the format, get the thing fast you can. On the right we see what a result for Life of Pie looks like today. Where you see single result, three citations, which covers all of the text analogy format. Within those, the formats themselves are grouped to show you all the books in one place, all of the large type books in another place, all of the eBooks in another place, all of the audio books in another. And a little human there, therefore if you don't really care which, which specific, who is writing the forward of the overdrive title, you click the place hold on overdrive and you end up in the queue that will get you the thing the fastest. [speaker off mic] Oh, I'm terribly sorry. I would be more than, oh -- so what I can also do is repeat the question. Which was a request to pull it up in the browser, which I'm caution about, but we will see how it goes. Oh here, I'm down to a minute. So we do Life of Pie, my favorite is to do The Hobbit because of how many crazy editions we have, if we open up eBook, we really do care which version of the eBook we are going for we can select appropriate one from this list. Is that what you are hoping to see? We can also go to the group of, what I wanted to do is now explain a little bit about how this magic works now that you've given me this overwhelming, oh my goodness that's the most amazing thing I've ever seen, hey I'm James, everybody say hi James, that's better, that's better. What was the cool thing that you wanted to show me? (sigh) And so the cool, the cool thing here is, you know, the way it comes together is through the creation of a group work ID by normalizing title and author, throwing in major class, we end up with unique ID all things from these records emerge with the same group, grouped work IT if they belong together. That's the basic, the core simple idea that helps this work in our catalog. And that's record grouping, which is awesome. So I'm from Nashville, Nashville is famous in 15 seconds for one thing, it is not country music, it is our partnership with the public school systems and their libraries. I do, it is an amazing program where we help kids get access to our entire collection delivered to their school. It is, it is this discovery layer that helps facilitate that, not only by helping kids find what they are looking for in their collections and national library collections but by also in the shared discovery layer which we [laughter] oh, which we release this year. But even we also have linked accounts that allow them to do it so they are with one log-in logging into school and doing all the patron functions they need to do to get the things they need. Yay! [APPLAUSE] Thank you. All right so you might think there is a break, there is no break, ha-ha-ha it is a lie. So we have our last few talks of the day and first up is, do we have our speakers? Okay. All right. Welcome up on stage. Free work flows and the rest will follow. Community driven AV solutions through open source work flow development by Dinah Handel and Ashley Blewer. [APPLAUSE] All right, let me drag and drop. Come back. Okay. We had it -- [speaker off mic] here you go. All right. I wanted to preface this talk by saying there is a lot of, an omage to the gift throughout this talk, if you are sensitive to flashing lights, seriously might want to, try to do whatever you need to do to be most comfortable. But now that I've scared you, hi I'm Ashley Blewer, happy to be here, I'm at New York Public Library, digital development team and I love it and we are hiring. This is Dinah Handel, national Stewardship resident at CUNY Television in New York, they will get a new one of her too, in the way that they are hiring. Dinah is going to start off by talking about open-sourcing audio-visual archiving work flow and file formats. All right. Hi everybody. So there is sort of like a few in theory like ideas that we need to layout before we keep going. So sort of assumptions between library and archive work. The first is that librarians try to create and think it is important to have clear, concise good documentation of work flows. A large part of how many librarians and archives work is to figure out how something is done, how materials are processed or digitalized, for example, and translate those steps. What we would consider a work flow into a piece of documentation. The second assumption or sort of in theory, they care about making information accessible to others, this is a fundamental principle of librarianship, it certainly becomes more complicated in practice, but we typically describe library and archive work we do in service of access. Yet often these two things don't go hand in hand with each other, so we are more concerned and rightly so with making materials we have in our collections accessible to patrons rather than sharing how we made it accessible. Occasionally we might make presentations like this one, write blog posts or journal articles, make public versions of internal work flows available on our websiteses, but overall there isn't the infrastructure or expectation for sharing the nitty-gritty details of library and archive workflows and so Ashley is going to talk a little bit about practice now. Yeah so why is there a gap between theory and practice? This is the part of the talk where everyone is like, it is not our fault. We all want to do the best thing, but some things get in the way. Time and money, a few cat slides, sitting in with the code, like with many other things play a huge role in ability to turn project dream into a project reality. Writing documentation is hard and underappreciated and so it is a real skill to be able to bridge the gap between a code base and human comprehension. It is also scary to put things out there on the Internet, a shared sense of imposter syndrome, the effort made or even size of institution. Your code might not be good enough, you might not think your code is good enough, not enough test or not enough documentation or not clean enough for the high quality of the Web. It can also be hard to deal with feedbacks, of course negative feedback is always hard. But even having to spend time answering questions or clarifying or helping other people use your code base, that takes time that is likely not allocated and dedicated to that cause, time and money. On top of all this AV is really hard compared to text or images. Especially when someone is more of a generalist. Code, format migration, difference between digital and analog and trying to answer and explain what a video is, it can be challenging. From this stand point quick successioning isn't a reality, analog video and even digital video, files can't be played back prior to migration, NPLT, kind of useless. Once they are migrated to stable digital format the file sizes are enormous and care can be made further burdensome, no generally accepted archive format, just a lot of opinions. Not one format is going to work for every institution. Small non-profit their solution is not the same as the Library of Congress' solution, for example. Historically AV has had to deal with these formats in software more so than other forms of media in libraries, this is only sort of recently caught up with needs of processing AV materials. So only just now standard video cards are sufficient for doing this of digitalized film and video, so we get in-house, we used to have to rely on vendors and companies to do for us. Dinah is going to -- So in terms of open work flows, there is sort of some ideas or definitions that I'm going to use to contextualize the rest of what we are talking about. Some of these are like for this room and this audience, like everybody is I know this already. We are just going to go over them to make sure we are all on the same page. The first is open source, what do we mean by open source? I like this definition from open source initiative that defines open source software as software that can be freely used, changed and shared in modified or unmodified form by anybody. Open access, what we need by open access is unrestricted access to information, open access is commonly used in world of academic publishing to denote a work freely available to read without description to journals or databases. I feel like the same should apply to code, people should be able to sign and download code for free. Open file formats are a third really significant component of this, as Ashley said earlier. A file format defines structure and type of data stored in a file and open file format then is the accessible published specification for the structure and storage of that data. Open file formats are usually maintained by standards organization and open file format is platform independent, meaning it can be used across software and operating systems, using this with publicly available is necessary in digital preservation, which this is crucial. In particular audio-visual file formats contain a lot of information about the file because audio-visual files are significantly larger and with proprietary audio-visual formats it makes future playback really difficult to do. So microservices, so a microservice framework, in terms of software, sort of brakes down monolithic software applications with many different processes into sort of distinct pieces. Each microservices ideally accomplishing one discrete task. And I like to think about microservices as sort of like modular code that can be combined in numerous ways depending on the desired outcomes. Now Ashley is going to talk about some goals. So earlier I talked about the working in the open, here are some of the benefits of working in the open. When thinking again about this gap between theory and practice a lot of this weighs more heavily as code based burden is just on one person or one institution or one person at one institution. It can be alleviated when a project is collaborative. As librarians I think we are familiar with vendor lock, which keeps an institution locked into one system, an institution own data sources attachment with our software, I think we should be aware in institution locks. How can we create things that can be extended out to other institutions. We heard a lot of great examples this week. Sharing vulnerability makes project stronger, knowing your project is out there will make it stronger. This will help make it better. Global impact, we're all connected so let's connect in ways that benefit all of us. Tangible, having an open work flow can make what is usually described as abstract process more tangible. To bring this around to practice, at the library on my team we strive for approach to development, maybe slightly other terms when Dinah talks about microservices later. Went the ability to switch out any one part of a project for something better better when it comes along, I know you all know what I'm talking about. We do this by having clear end points from one place to another. What better way to know your software component of overall work flow can be swapped out into the future, it can also be easily swapped into another institution code base. What are some tools that can help facilitate this process as open sourcing workflows and shout out to lightning talk earlier, I think one of the most promising terms either get from the command line or user interface, I work with graphical interface a lot github, I see it as a great tool for hosting work flow from different institutions. Sometimes I think something like github gets seen as irrelevant because they don't write code. Github doesn't have to just be a repository for code, it can contain documentary of work flows like PDF or Mark down that outlines institution work flow and different specifications. I think benefit of using github for this is version control. It allows for the public to see how and why work flow or policy has changed, which provided greater transparency to archival decision as well as archival labor. Some libraries already do this and it is really awesome to see. Another thing is it is not like a tool necessarily, but it is a space that I find really exciting, with regards to sharing knowledge about librarian work flows, two libraries, Anna and [Name?], library workflow exchange share the same dream I do, a magical database that would allow us to find out what other libraries are doing to create work flows. They started this website, options to submit work flow for institutions that don't have ability to host institutional site. The site also pulls from workflows from from websites, blogs, conferences and github and host of other places. I'm going to talk about two, two pieces of software that are examples of what we mean by open-sourcing audio-visual work flows. The first are media microservices. They are a set of open source microservice scripts that I've been working on as part of my time as national Stewardship at CUNY Television. These are written and use open source so the wear as thumbtag and labor of media, they are archiving work flow. Much of our documentation of how they work or what they do is stored in the comments within the code. As Ashley noted earlier, what's useful about implementing this approach is we aren't restricted to one work flow that's imposed by a software system. Which makes it easier to adapt as technology changes or as our local needs change. And this is especially important with audio-visual materials, also because a lot of times digital preservation software doesn't have ability to deal with these. These are developed for our institutional needs but also work as individual services, so up here right now is running a microservice make YouTube, transcodes one or multiple inputs according to specifications needed to upload to YouTube. These services are free and open source anybody can use them and configure them using our config fire to process, switching out server names or having different addresses and file make it possible for them, for other people to use them. Another example of open source audio-visual work flow is this audio-visual, [Name?] this was initially created at CUNY Television following hardware and software, it rendered this unusable. So down loadable via github and homebrew grown into a project worked on with many individuals at many different institutions. It is interesting to see who is involved in this project. And it uses the open source software again as a thumbtag and works with black magic design open source development kit which is hugely important. So this obviously doesn't involve problem of difficult to obtain hardware, it does make the process of digitalization a little bit more accessible. And going to talk about these projects. I'm going to talk about a few projects, microservices developed on the open are supplemental non-program specific projects. A project I work on coming out of area of video coalition, here is an example of what it is like, thank you. Software that helps detect processeses in digitalized analog video. Running quality control and analysis on these videos, which is great for doing inspections on video, after digitalization or after it comes back from a vendor, you are not necessarily doing this in-house. This works for a single files mostly right now but recently received a grant and funding and support from from Indiana University to allow this to work as a microservice and added database and Web server for batch level processing and analysis. Media conch is another project I work on, video file performance checking, the video files are what they say they are, lightning talked about this last year, funded by European commission, it is required to be open source if they give you that money, which is really rad. Media conch is based on this, this is running in the background, I would say the most used media microservice among information professionals, this gives you information about your files. The more importantly, it does it very quickly and even works on partial files and files in transit. It is easy to integrate this into much larger projects, this is what we intend to do with media conch as well. Not only do you get information for your files, you can note your files are healthy and happy and conforming to your institution personalized policies. An example, artifactuals, we will soon be using media conch soon to get files into digital long-term storage. These artifactual use [Name?] an open-video format going through standardization process via Internet engineering task force seller group. Coding for archiving and real-time transition. We had to work real hard to get acronym that sounded fun. But this, longevity as a recommended digital preservation file format. I'm going to switch up to talk more about less, less programming heavy programs, this similar it is a good example of small project that helps people at a lot of different institutions worked contributed by many people at many institutions, so it is a platform for sharing small scripts and so hosted on the association of moving image open source committees github page. There has been a lot of collaboration and issue section and we have even had contributions come in from a contributor. So we know we are not writing bad scripts. it is great to see so many people share such knowledge. ABAA, this is also out of area, Wiki for sharing video play back problems where people can contribute their video play backer error, they can explain or describe problems or can figure out why their videos look weird and potentially learn how to fix those. I think no, but -- [laughter] so freer work flows and the rest will follow. In conclusion, you can grow your projects beyond your own institutions. More people provide more perspectives, more contributions make projects better and we can provide each other with supportive environments. Additionally, we don't have to spend time reinventing the wheel. It opens uptime for us to do other things that are really difficult, like doing advocacy, writing the documentation or doing sort of any other task thatcomputers can't do. Furthermore which we open source our workflows we build a community of practitioners who can collaborate to make shared goals a reality. Here is some links about the stuff that we talked about, our slides are on github. Thank you to Dave and YPL, all free and open source contributors in the world. Free workloads and the rest will follow. Thanks. [APPLAUSE] Any questions. Two minutes. Any questions, we have mic runners. [speaker off mic] Thank you. [APPLAUSE] Next up is Indoor Positioning Services and location based recommendations by Jim Hahn. Okay, my name is Jim Hahn, library in undergraduate library at University of Illinois. So to understand this talk though, you have to know the undergrad library was built underground. There is a research plot of soil, it is the oldest operating soil plot in the United States. For that reason, undergraduate library could not be built above ground because it would throw shade on the corn. So students have to go through a subterrainyus to locate in a system 30 thousand undergraduate students. The service we prototype allows new students guidance to library items of interest. It promotes resources from mobile device, just gets overlooked when doing this browsing. As an added benefit we are able to show students popular circulating items in the area they are standing. The interface looks like this, this is prototype interface at left the blue dot is the location of the users device. You can see a red dot also, location of the item they searched for. The search actually happens in a different screen. This is part of a modular app Minrva, they search in catalog search and have this modular, I'm talking about upgrades to our modular, Minrva. Any location in the library, you tap the button for related items and this is where a query to popular near me algorithm is generated, popular near me results are populated in that screen to the right. If you can see this file there, we suggest using API we suggest a database that may be relevant for their location. We suggest electronic books relevant to their location that they are in. And then also a popular circulating. We do this with Beacons, a company that came out of [Name?] at the startup, these Beacons actually over proximity Beacons and we use 52 of them based above the tiles in the undergrad library. Library measures about 50 meters by 56 meters inside of the Beacon is a 2.4 gigahertz radio using bluetooth 4.0 smart, bluetooth low energy. The Estimote has best analogy for how to think about Beacons. You can think of the Beacon as a small lighthouse, instead of light it uses radio waves, instead of ships it alerts smartphones of its presence. In real world conditions though these get a range of about 40 to 50 meters. So we have some pretty good saturation, remember we use 52 of these. For the rest of the talk I'm just going to sort of unpack some of the processes that we use to integrate these Beacons into the app, including that recommendation algorithm, I'll step through that code. Also how we actually test it and improved Beacon position. And something we're still trying to nail down, I wouldn't say that it is, we made efforts at securing them, but I want to say they are completely secured, that's something we are still working on. So I want to start with step 0, which is the device locolocations are in a table that corresponds with grid system of library map. Step one, it uploads, checks Minrva to ensure it has most recent tables locations. There is an indoor SDK that could position, but we chose actually to utilize a library for inferring locations. The reason why you need that, once you have the table and you send the Beacons, you still have a bit of an emergency in that you don't really know where you are yet until you have done trilateration. So a process of determining absolute or relative locations of points by measurement of distances, using, you can use the geometry of circles, spheres or triangles. The Spheres are not perfect in any real world environment. The spheres are actually going to defact and look weird based on who is standing there, where they are in the book stacks and if it is passing through, you know, people in the library. So those are some variances. And then API, I'm going to go from at the bottom to the top. We have basic call number layout the custom table we curate over time, ranking popularity, less than Oracled database, we belong to consortium, but if you are familiar with backend Java connectors to the Oracle database to get recommendations for popularity. That's actually from the last year of circulation, not total time. So we want to keep it relevant. In order to process the recommendations we filter through several depending on location. That's based on this button, recommendations near me. We get the PatronLocation and then based off PatronLocations we get shelf ranges from custom, once we know ranges of nearby stack we get relevant both in eBooks, filter population based on range on and any eBooks that would be relevant finally we add relevant databases to patrons starting location. This gets with JSON, Android app creates JSON with location based recommendations view.  A little bit about Beacon locations, we, we had our facilities department help us out with these above the tiles so students wouldn't know they are there, so they wouldn't be able to steal them, they are above the tiles. We had to introduce different access there. We tried them in the stacks, but actually that metal defacted the signal too much. In my office there was tiles, I studied how it would work in my office first then facilities got, there is a courtyard in the middle so no Beacons are required there. We have a computer science team, what Beacons ask see, it let's us know how well the program is deferring locations and helps us test out latency, I was doing my iPhone to go through the text and see the Eskimo app, there is a testing app, you can see where you might see some sparseness based on where you place them. At the far left that represents an area with only two, trilateration is going to be difficult there, we want to see at least three. Far right shows five Beacons, this area I'm just going back a few slides here. This area is really like the bottom left component right when you walk in. It is not really saturated enough with Beacons, that kind of told us right away we need to add one or two more. I will conclude with thoughts on security, it is in the cloud. What does that mean? Our e-mail address that we used to purchase these devices, that's where we can sign in and actually set the signal strength, but then you need to sign into the app and go out in the library and send as commands to work on signal strength. Someone could maybe figure out how to access if they got your password. Another thing that we're interested in securing is the local database that tells us the XY coordinates in the library. All that anyone could do if they sign in here would be to delete all those or just add locations, but that's still something I'm really concerned about securing actually. So those are some initial efforts but I think entry point the Internet of things, technologies, some security considerations when there is a middle ware component that we don't know a lot about. We are always looking for ways to navigate that and maybe make it a bit more secure. So I'll just conclude by noting some next steps. We're going to push this update to the Minrva, to Android Play store, it will be an update to Minrva app this summer. Then we have a project page for some of our prototyping projects as well. Thank you. [APPLAUSE] We have one minute for questions. Raise hands, make signals for the mic. We got two right here. Quick question, in addition to way finding, have you looked at any looking to do heat mapping of where where users not actively using the map, but where they are locating themselves within the library now that you have this mesh network of location definers. That's a good idea, we haven't done that yet. I think that's an open question to explore more, while we still have print stacks we should look at things like that, yeah. Okay. Time is up, thank you. [APPLAUSE] All right. Next speaker is coming up. All right. So the title is a chain, I want to say a combo breaker but I will save that for later. Archive space, archive space work flow integration by Mike Shallcrass. Thank you. All right. This is my my fear, I'm grateful that Julie did preliminaries the other day, I think there is sin what Dinah and Ashley were talking about, we are the official archive of the University of Michigan, we also repository for papers and records from individuals and organizations from around the state of Michigan. In April of 2014 we received grant from Andrew foundation, archive space, archive space in end-to-end born digital archive work flow. In the development work in this projected is being done by archive systems, archive. So before I dive right into the project itself, I want to talk a little itself about why we embarked upon this. Bigger reason we had a number of home grown systems, the term we spoke, I think might denote too much -- we have been using some Microsoft word templating with some macros tied to styles to generate description. We have our local maker database where we record acquisition information and track donors and what not. We, I guess around 2012 developed a windows, in 2014 we had a reorganization where we originally we had three different processing, digital curation unit did things on their own. We wanted to bring them under one umbrella and standardized across all formats so users had more consistent experience when looking to try to access information. Then we also had kind of a shifting repository landscape since 2008 we signed memorandum of understanding with main University of Michigan library, we are separate, to use the D space repository. This isn't idea for these collections but a lot better than what we had, saving stuff on CDs or hanging off our website this is a real jump forward. Then shortly though after we received our grant we learned University of Michigan library wanted to move to hydro, this last year became a hydro partner. This has impacted our work, focusing on 0 a solution that involves D space but want to be agnostic enough so we can swap out that backend and put another repository solution in there. Finally, touch upon what other folks talked about in discussing archives here, there is, we see a real value in this intervention. Key functions and principles of archivals, the content for preservation is a big concern for us because we can't save everything we have to focus our resources on what we can. Just also documenting the context of materials, which includes the original orders, who created, what functions or activities this material is related to and original order it existed when originally created, used or maintained. That's a little bit of background. Our goals for the project are to steam line and deposit of context in a repository. So we are looking for greater automation to both increase efficiency and also reduce potential for humor error and also to empower more of our staff to deal with digital archives to move away from notion of this digital, we have a person on staff focusing on the digital curation. We want to bring more people into the fold and work with this material, the principles and the work are the same no matter what format it is. We have also, trying to establish a curation eco system. In terms of the tools that we're looking at, each platform kind of addresses a different part of the digital preservation cycle, archive space, documenting acquisition, being system of record for administrative rights, this being tool that these packages and places into storage and access portal. But we are, you know, this microservice approach, trying to employee right tool for right job and avoiding overlap between things. I think a big part of this ecosystem notion, meta data across systems so we don't have to enter the same information multiple times. We are looking to participate in larger communities of practice. We are going to move away from doing it in our own, with this grant particularly we are trying to find solutions that meet local needs an applicable to others in these communities. Part of our grant we will share all project code and the work we are doing integrated in source code of archive, and moving forward we are looking to collaborate with these larger communities to support and develop these, these tools and solutions. So I'm going to run through these key development tests in my upcoming slides and kind of consolidate into three large areas. The first is development of a new archive metaca test. Both intellectually and physically in this dashboard we place this new appraisal tab. I guess when we kind of launched the project we had Evelyn and Justin come out for a site visit, this is a great tool for injetting material, but wasn't really a chance to interact with materials to kind of understand what the content is. So that's the idea for this appraisal tab. We also realize there is a lot of really rich text meta data that we are collecting and generating as it is working, kind of working with content itself. So we were looking at ways we could try to reuse that to help get a better handle on what they are working with and document that information. So this is the current appraisal tab or recent version at least and it is actually divided into separate, on left transfer backlog that shows content you are working with. Each of these panes you can toggle to make them appear or disappear. The middle line is right here is analysis pane where we can learn more about the content on the far end this file list, file listing of all the content, starts with what we select in transfer backlog pane and archive which I will touch upon in a bit and arrangement for those not actually using archive space to kind of create Submission Information Packages that represent the intellectual arrangement of materials. Some of the stuff we can do with the appraisal tab is using some of the file format information to represent in both a table, like a report format as well as visualization, distribution of file format. So we get a field what's actually in this succession of material, what's the variety, anything particularly thorny or problematic. Also using information from the examine content which is running bulk and various scanners to pull out information and particularly interested in trying to identify content with credit card numbers or personally identify these, particularly Social Security numbers. Part of the analysis pane we are able to look at content that has, had a hit according to bulk extractor. Then actually preview that and see is this a false positive or really a Social Security number or something here. I mention a file list, if you a full path of files, as well as size and last modified date and so you can sort this, actually click on files and it will, there is a preview kind of window in the analysis pane and currently only supports things your browser can actually display. But if your browser can't display, you have an opportunity to down load it. Again gives you a chance to see what the content is and give you a feel for it. You can tag material in transfer backlog pane or file list or back extractor kind of content window. This is just to record notes as you are going through and appraising material. If there is information you want to tag that so you can keep track of it, this could be, information especially with the arrangement of it or things that need to be successioned. But these tags are persistent, but do stick around and they are saved over sessions. So next is archive, archive space, so we have archive space pane that loads and actual retrieved interest resource record in archive space, displaying display, you can create, you can add or edit meta data, add premise rights information to archival objects and also create digital objects and associate with components of these objects. This is a window showing the meta data that you can edit. Archive space has digital objects where essentially kind of a manifestation of this description. So we can create digital object components, drag and drop content on to that and finalize that arrangement and kicks off this procedure and goes on and produces package, to write this to a space. We are using the sort to deposit meta data to repository, this has involved repackaging of archive, this is going into private zip file content public one, deposited into D space, this stored service can put all this together. On deck we are looking at, you know, some additional work. Here is some like, okay, thank you. [APPLAUSE] there is archive camp at University of Michigan in August, I want to put a plug to that. Thank you. The time has come for the last presentation, no wait, describe towards a general framework for community description. This is going to be the last name I'm going to probably butcher, Paul Beaudoin. Or would you do, if you are from Quebec. Thank you. Sorry I had this loaded up here, but it is gone. Okay. Sorry about that. Yeah my name is Paul Beaudoin, I work for MPIL Labs [Name?] and this is a talk about Scribe framework for community transcription, but I mostly want to talk in general about things we can do to extract data from our digitalized assets. So anyone recognize this elephant? This is a famous elephant, I mostly, have to admit wanted to see it at full scale on these massive screens. I don't always have access to screens this large. [laughter] Just about got it right, I think. The significance of this elephant, there is significance, will be declared later in the talk. So cultural institutions, nothing new here, have this problem of having a lot of data that's stored away in forms that are difficult to analyze. I'm thinking of things like this. So at its heart there is a wealth of data here, but not expressed in discrete human terms. Our minds are really grit at looking at this bitmap and identify the phraseses that represent discrete concept and assertions, but the document as it stands doesn't lend itself to easy reading, much less aggregate analysis. Which leads me to this idea. So I want to strive to extract the semantic content from the bitmaps that we are producing out of digitalization because it is frequently not about the image. There is nothing objectively beautiful about this image, as far as I'm concerned anyway. More about we digitalize things because of the data it contains. The thing is the computational methods so far can't really compete with humans for this kind of thing. Which leads to this project that group universe collaborative in late 2014. It was an NDH funded project to solve the problem of data extraction in general. So this universe, if you are not familiar is these folks, you really should be familiar by the way. They've been building a lot of crowd sources tools to extract data from videos and photos from a gamut of things, this 19th century, shipyards, penguins, these labs have also built a number of tools to extract data, notably building expecter, what's on the menu, my own project ensemble, many others. So labs and universe came together in 2014, core experience in these sources and build general purpose community data extraction tool and we debated the name tore a long time, but somehow we still have Scribe. The primary thing about Scribe, it is configured around questions. These questions can take a number of forms, the four primary kinds of questions are these. So meta questions about the entire document you are looking at such as does this appear to be a document of type, whatever. Questions about the locations of things inside that document, like prompts like draw box around the document title or click on each penguin you see, questions about those things previously identified. Transcribe title shown or is this juvenile penguin or adult, meta questions those questions, rather questions about the answer to those questions, which amplify or reduce validity of those previously supplied answers. You can imagine this this chain of prompts forming the sort of tree that extends, you see from left to right. Where as in the beginning we have primary documents upon which a bunch of Marks are made, upon which a bunch of transcriptions are made, depending on the degree of um, of agreement about those transcriptions, potentially a number of flags that amplify or produce the strength of those assertions. So the primary thing, one doesn't need to configure a schema, one configures questions. Let's particularly look at this in particular. So this is a project around mortgage records from City Savings Bank in late 19th early 20th century. Been running since November, we have a dedicated, about 5000 contributors who have submitted about half a million individual classifications. And these classifications general term from if anything transcribing text, loading on one potentially transcriptions or flagging individual documents. The dedication looks something like this. So the core of our contributors are submitting between 10 and 100 classifications, more dedicated people about 800 are submitting over 100 and the cream of the crop, the 2%, at the very top, submitting more than five classifications about 100. In one case one person submitted 32, 621 classifications over the course of this four month project. Let's look how that plays out in terms of single chain of classifications, just to make how this works a bit more real. One day user was presented with this and asked to Mark the amount loaned. Which they did, in addition to marking a bunch of other things. Subsequently a different contributor encountered that users Mark and typed what they thought. Within 20 minutes, I know this from the log, three distinct users encountered the same Mark and the exact same text, in particular, immediately uploaded that to consensus data and we know that this particular record, or at least we're pretty confident this record represents $200 thousand dollar loan. So also another thing to note is that Scribe does have this optional facility for mapping the answers to these questions to a custom schema. This is a late edition, not yet mastered but is something we are working on. It allows you to define custom schema in, with custom fields, which can be associated with primitive types like dates and dimensions. We see it applied here to a not perfectly competent transcription, because I think that's probably because of the year. It is not like 1880 or 1870, but any case we have two distinct transcriptions, pretty good competence it is 1880. Because we are mapping to a date and doing the proper reduction of that to, to represent it as a full date, we can run queries like this. So here I'm filtering on records between 1885 and March 15th, 1890 arbitrarily. Similarly, because we're interpreting the free text transcription of the amount loaned field I showed earlier, we can make, so making the common substitutions and stripping out non-numeric, we can run queries like this to respect highest payout mortgages in 400, $200 thousand range which is a lot of money in the late 19th century. So this gets back to the elephant finally. In perusing this list of names the other night, I noticed a name and folks familiar with the big players in late 19th century Manhattan will recognize this name. I recognize it because of coloring contest I appreciated in when I was eight years old [laughter] and that I won. [laughter] [APPLAUSE] If you take one thing away, I'm really good at coloring. It terms out PT Barnham, great show on Earth, the great national show united and author of the Art of Getting, of Money Getting, published in if 1880 in which he had a whole chapter cautioning young people against dangers of borrowing money, took out a loan for $200 from the Savings Bank, within about a year and a half of that he purchased Jumbo. Not saying a direct relationship between this loan and Jumbo, necessarily. I'm not saying Immigrant City Saving Bank, but corrections -- so I encourage you to check out Immigrant City, Scibe, if for nothing else than conceptual approach. Moreover just encourage us to seek ways to extract this kind of data, primary source Wiki data radicals. Thank you very much. [APPLAUSE] Now it is the last presentation. There you go. Edward Corrado. Yes. Yes. Issues to consider before pushing out an open source project. Having, being the last one up here I think, it is one of my duties to start off by thanking everyone that put this conference together. I really appreciate all the work the local people did, all the volunteers. They put on an excellent conference. Can we get a round of applause. Okay. So I'm here to talk about issues to consider before pushing out open source project, which in itself a project. We went to a few of us went to a conference, international preservation conference in November and we attended a workshop called rolls and responsibilities for sustaining open source platforms and tools. As part of that workshop there was a breakout session where smaller groups got together and decided to work on a project, some sort of interest. I ended up in a group that was interested in things to consider before open sourcing a project. I have some code at the University of Alabama where I work now, that we've been asked to share across campus, not necessarily the world, but when we started thinking what does this mean if we need to share this code. How much time are we going to have to worry about support what, are the licensing issues, what types of things. I'm going to share to people on campus, why not share to a larger community. We still have a lot of these issues. That's why I was involved. But the checklist was nice, it was a really good start, but due to limited time it wasn't really substantive, didn't have a lot of substance, didn't have a lot of context. So three of us with really different backgrounds actually decided that we wanted to move this forward and decide to write a report, I don't know what to call it yet. We maybe it is an article, I don't know. But we're working on something that we think is going to be useful for the community for people to share about this. And the real reason why I'm up here is to tell you to give feedback and make you aware of this project you are working on. Who are we? Associate Dean library technology at Alabama. Carl Wilson, who is from the UK, he is in open preservation foundation, currently kind of the lead on Jove and, he knows a lot about code and trying to grow projects. Brett who is on Twitter right now, director salary communications, also has a law degree so he was able to talk a lot about the legal aspect. Later on we included Yier Hueng, the open preservation foundation, a really good diverse background, got some Europeans involved. Got some people from the United States, we are trying to make this kind of international. So the reports still a Work in Progress, I wish we were, had a complete final thing but actually will never have a complete final thing. Things change, we hope to, becomes a continuing growing document, a living document, if you will. But it is actually at the point where it can be shared. It has a lot of format issues, there is a couple places not still in it. I know, I haven't actually seen the current version because I know that Carl Wilson was working on it this morning trying to finish up a few spots. So it is kind of a Work in Progress from that stand point. So just to give you a really brief overview of some of the things we're talking about, before you start review current landscape, is there other software there? Do we really need 21stwidget that does same thing or better off working with other people? Why do you want to release open source, do you really want to release open source? You can share code and share programs without it being open source. Terrier is a great example of a great project, not open source but a lot of people use it and it is a wonderful project. What are some of the legal considerations? Copyright, remember international audience, copyright and software license codes quite different and actually a lot of people probably don't really know what their country is. They think a lot, maybe in the United States we have a good object but I know, in preparing this paper we discussed that in Germany the laws are quite different and a lot of people are developing code in Germany don't actually realize their laws are different in what they can contribute. Make sure you plan, manage, have a road map for your project so you can help ensure stainability, make sure you test. How do you really build your project? We had great talk the other day by Sebastian about how we did this library platform from that box to actually something people can use. You know how you do that? You do that by building commune. Community doesn't just happen. You have to work at building community. You have to communicate. You have to have a mission statement, you have to make features and requirements clear. What development status. How many people have looked for open source project and found a page and wondered if this project is actually still live or not? [laughter] Exactly. Be clear about that and be honest about it. Let people know how, where you are, what you are doing with it. And, you know, we are living in kind of agile world now, make code available as quickly as possible. Make sure you have a read me there, you have some instructions on how to actually install the software and things like that. These are all things I know I have come across in some projects I haven't had. You need to develop user community. I have on the screen one of our heros from Wednesday talking about how to present, how to participate in open source project without being a voter. Documentation is important, by growing the user community you can have people contribute back to your, your project, maybe not necessarily by giving code, but by giving documentation. Help provide support on mailing lists. But you have to really grow that community. You have to answer basic questions about your software on a mailing list. You have to be visible, you know, even if they seem stupid questions to you they are not stupid to the people that want to use your code. You really need to try to develop that community. But there is two types of communities. There is the end user and a software developer. A lot of times the software developer is, if we only think about software development community. Maybe a lot of that is nature of what we're normally developing, but this is a community. Your community will vary greatly depending on the project. Think of an open source Web browser, like FireFox. That community is not a lot of developers percentage wise, there is a lot of developers in that community, but percentage line there isn't versus some project that's really made like a library that's made to be included in some other bigger project, that's going to be almost all developers that are going to be the direct users. You need to know there is, these different communities and you need to figure out what the Governance model is, how code is going to be contributed, how you are going to set priorities, who sets the priorities and how are they set. And again you need to use standard relevance to your community, think about text transfer identification clauses and I know a lot of us don't necessarily like to speak with lawyers, but you need to. If you really want to grow a long-term successful project you need to make sure that you have these issues, especially if you are doing it all on your own, maybe it is less important. But if you are working for a University or for a business or a non-profit make sure you get that done. And, you know, make sure that you have a way to track bugs, feature requests, make sure you have people actually confirm and another thing we heard about today back on Wednesday. And so really what's the future of what we're working on? We want feedback, we want to know what we missed, we want to know what needs clarification. We really want contributions, we think will be real useful right now a Google doc about 30 pages, misleading, a lot of it is things we crossed out based on earlier outlines and everything. So you can see that and we really hope this is useful to the community. We want your feedback and if you can participate and let us know, you know, let us know what we are doing, what we are missing wrong and let us know it is useful. There is a bitli URL, this is kind of unmanageable, if you go there and you can see what we have and let us know, make comments. It is an open document, people can edit it and thanks for listening. [APPLAUSE] oi Just one quick comment, having worked on one of the bigger open source projects, your community page. I think when you start with that you need to start with where you are at and where you plan to be in the next few months rather than where you plan to go. Because certainly it will change, all those things, except maybe the legal, will change over time. And of course choosing the license, that will may kill you down the road. So you have to be very careful. Yes, really no, I agree with that point but really know what your license is and actually what it means and what it requires you to do. I found out there is a lot of code, the license is supposed to be, in the header of the code, it is not. So people that are developing software don't even follow the license themselves, so make sure what license you choose and why and what it means. Thank you again. And I think that's a very appropriate presentation to end on because we learned a lot of cool stuff. We're going to bring that back to our institutions and organizatio we don't have a community that will help sustain what we build. So thank you. [APPLAUSE] you all survived, yet anothe Congratulations, to reward you for your persistence we have giveaways. Now if someone can come up here and show me where this is so we can giveaway stuff. Great so we have four prizes left. We have three oxygen licenses and, you know, that one thing that I showed a little bit earlier, that could be you as an airplane pillow. So let's go and there we go, 2016 is not responsive. Nice soothing music.  Breathe in. Keith Gilbertson, are you here. [APPLAUSE] If you want to go to the table over there, claim your prize. Round number two.  Jason Kevin, are you here? Gone. No, no? No. One more time. I feel like we need to do interpretive dance. Right, that would be cool.  Allison [Name?] -- no? Okay. This is going to be tough at this point. You know what, we will get in touch with them even though they weren't here. And we have one more then. We have one more. I think all of them are licenses, yep. One more. Arcadia. Okay. So Arcadia is our last one. [APPLAUSE] All right. So we have reached the actual, actual end but we have a little bit more for announcements. So as was on indicated earlier this morning, this conference is really, really expensive. So we couldn't have done it and kept it at a low cost without all our sponsors. So again I want to give a big hand to all our sponsors. [APPLAUSE] In particular by those of you who were not able to attend or for those of you who made use of the transcription service, we want to thank Temple University so much for providing that service. [APPLAUSE] And speaking of live streaming, you may have noticed this big panel table right straight ahead, those folks right there have been sitting there all, making sure that people who could not attend were able to see the live stream and able to record these videos for whenever, whenever the Internet, you know, decide to go away. So I want to thank the streaming team for all the hard work, thank you, thank you, thank  you. [APPLAUSE] And the Code4lib conference is very, very unforgiving, especially for a core group of folks. And that is the local planning committee. So for 2016 I want to say local planning committee, either stand up, get up here, get recognizize, get up here, come on. [APPLAUSE] You have now joined the rank of the few, the proud, the poor, poor bastards that had to organize this damn thing. [laughter] Outside of local planning committee, they could not of pulled this off without the multitude of volunteers who gave their time and energy to putting on the conference either by organizing scholarships or programs, preconferences, social activities, childcare, streaming, a whole slew of different work. Including, the whatever committee, you know, just grabbing them and saying hey this needs to be done. So if you volunteered please stand up so we can say thank you. [APPLAUSE] All right. For those of you who have some time, maybe stay for a little bit longer. We do have some hotel space available for birds of feather, one particular one I want to point out is the Fedora birth of a feather, it will be in the flower room and it will start at 1:00 pm. Otherwise hotel shuttle will be on the 10 and the 40 of the hour until 5 pm, $10 to the airport. Enjoy the weather. I'm happy to see you all again. And I wish you safe travels and see you in 2017. Thank you. [APPLAUSE]");
    var sentences = rm.generateSentences(1);
    $('#content').css("opacity",1)
      $('#content').text(sentences.join(" "));
      $('#button').append(
        $('<button>').click(function(){
          var sentences = rm.generateSentences(1);
          $('#content').text(sentences.join(" "));
        }).text("Moar")
      )


  },500)
    };
  </script>
  <header id="header">Overheard at Code4Lib 2016</header>
  <blockquote id="content"></blockquote>
  <div class="button-container" id="button"></div>
  <footer id="foot">Markov n-gram chains created from the <a href="http://2016.code4lib.org">code4lib 2016</a> transcripts. <a href="http://thisismattmiller.github.io/overheard-at-c4l2016">Repo</a></footer>
 </body>
<html>